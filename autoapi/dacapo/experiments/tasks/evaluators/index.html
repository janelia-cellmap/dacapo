

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dacapo.experiments.tasks.evaluators &mdash; DaCapo  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/custom.css" />

  
      <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores" href="binary_segmentation_evaluation_scores/index.html" />
    <link rel="prev" title="dacapo.experiments.tasks.dummy_task_config" href="../dummy_task_config/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            DaCapo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">DaCapo API:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notebooks/minimal_tutorial.html">Minimal Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../data.html">Data Formatting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../unet_architectures.html">UNet Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorial.html">Tutorial: A Simple Experiment in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../docker.html">Docker Configuration for JupyterHub-Dacapo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../aws.html">AWS EC2 Setup Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cosem_starter.html">Fine-Tune Cosem Starter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../roadmap.html">Road Map</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../index.html">dacapo</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../index.html#submodules">Submodules</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../../apply/index.html">dacapo.apply</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../blockwise/index.html">dacapo.blockwise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../compute_context/index.html">dacapo.compute_context</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../index.html">dacapo.experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../ext/index.html">dacapo.ext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gp/index.html">dacapo.gp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../options/index.html">dacapo.options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../plot/index.html">dacapo.plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../predict/index.html">dacapo.predict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../predict_local/index.html">dacapo.predict_local</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../store/index.html">dacapo.store</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tmp/index.html">dacapo.tmp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../train/index.html">dacapo.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../utils/index.html">dacapo.utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../validate/index.html">dacapo.validate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#classes">Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#functions">Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cli.html">CLI</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">DaCapo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">API Reference</a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">dacapo</a></li>
          <li class="breadcrumb-item"><a href="../../index.html">dacapo.experiments</a></li>
          <li class="breadcrumb-item"><a href="../index.html">dacapo.experiments.tasks</a></li>
      <li class="breadcrumb-item active">dacapo.experiments.tasks.evaluators</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/autoapi/dacapo/experiments/tasks/evaluators/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-dacapo.experiments.tasks.evaluators">
<span id="dacapo-experiments-tasks-evaluators"></span><h1>dacapo.experiments.tasks.evaluators<a class="headerlink" href="#module-dacapo.experiments.tasks.evaluators" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="binary_segmentation_evaluation_scores/index.html">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="binary_segmentation_evaluator/index.html">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator</a></li>
<li class="toctree-l1"><a class="reference internal" href="dummy_evaluation_scores/index.html">dacapo.experiments.tasks.evaluators.dummy_evaluation_scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="dummy_evaluator/index.html">dacapo.experiments.tasks.evaluators.dummy_evaluator</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation_scores/index.html">dacapo.experiments.tasks.evaluators.evaluation_scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluator/index.html">dacapo.experiments.tasks.evaluators.evaluator</a></li>
<li class="toctree-l1"><a class="reference internal" href="instance_evaluation_scores/index.html">dacapo.experiments.tasks.evaluators.instance_evaluation_scores</a></li>
<li class="toctree-l1"><a class="reference internal" href="instance_evaluator/index.html">dacapo.experiments.tasks.evaluators.instance_evaluator</a></li>
</ul>
</div>
</section>
<section id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores" title="dacapo.experiments.tasks.evaluators.DummyEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DummyEvaluationScores</span></code></a></p></td>
<td><p>The evaluation scores for the dummy task. The scores include the frizz level and blipp score. A higher frizz level indicates more frizz, while a higher blipp score indicates better performance.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.DummyEvaluator" title="dacapo.experiments.tasks.evaluators.DummyEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DummyEvaluator</span></code></a></p></td>
<td><p>A class representing a dummy evaluator. This evaluator is used for testing purposes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.EvaluationScores" title="dacapo.experiments.tasks.evaluators.EvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EvaluationScores</span></code></a></p></td>
<td><p>Base class for evaluation scores. This class is used to store the evaluation scores for a task.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.Evaluator" title="dacapo.experiments.tasks.evaluators.Evaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Evaluator</span></code></a></p></td>
<td><p>Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiChannelBinarySegmentationEvaluationScores</span></code></a></p></td>
<td><p>Class representing evaluation scores for multi-channel binary segmentation tasks.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BinarySegmentationEvaluationScores</span></code></a></p></td>
<td><p>Class representing evaluation scores for binary segmentation tasks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator" title="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BinarySegmentationEvaluator</span></code></a></p></td>
<td><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores" title="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceEvaluationScores</span></code></a></p></td>
<td><p>The evaluation scores for the instance segmentation task. The scores include the variation of information (VOI) split, VOI merge, and VOI.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluator" title="dacapo.experiments.tasks.evaluators.InstanceEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">InstanceEvaluator</span></code></a></p></td>
<td><p>A class representing an evaluator for instance segmentation tasks.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="package-contents">
<h2>Package Contents<a class="headerlink" href="#package-contents" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">DummyEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>The evaluation scores for the dummy task. The scores include the frizz level and blipp score. A higher frizz level indicates more frizz, while a higher blipp score indicates better performance.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.frizz_level">
<span class="sig-name descname"><span class="pre">frizz_level</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.frizz_level" title="Link to this definition"></a></dt>
<dd><p>float
the frizz level</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.blipp_score">
<span class="sig-name descname"><span class="pre">blipp_score</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.blipp_score" title="Link to this definition"></a></dt>
<dd><p>float
the blipp score</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.higher_is_better">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.store_best">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The DummyEvaluationScores class is used to store the evaluation scores for the dummy task. The class also provides methods to determine whether higher is better for a given criterion, the bounds for a given criterion, and whether to store the best score for a given criterion.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluationScores.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['frizz_level',</span> <span class="pre">'blipp_score']</span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd><p>The evaluation criteria.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[&quot;criterion1&quot;, &quot;criterion2&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">frizz_level</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id0" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">blipp_score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id1" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id2">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id2" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether higher is better for this criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">DummyEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;frizz_level&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether higher is better for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id3">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id3" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple[Union[int, float, None], Union[int, float, None]]</dt><dd><p>the bounds for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">DummyEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;frizz_level&quot;</span><span class="p">)</span>
<span class="go">(0.0, 1.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the bounds for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id4">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id4" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether to store the best score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">DummyEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;frizz_level&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether to store the best score for the given criterion.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">DummyEvaluator</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluator" title="Link to this definition"></a></dt>
<dd><p>A class representing a dummy evaluator. This evaluator is used for testing purposes.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluator.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluator.criteria" title="Link to this definition"></a></dt>
<dd><p>List[str]
the evaluation criteria</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation dataset.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.DummyEvaluator.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.DummyEvaluator.score" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The DummyEvaluator class is used to evaluate the performance of a dummy task.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="id5">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['frizz_level',</span> <span class="pre">'blipp_score']</span></em><a class="headerlink" href="#id5" title="Link to this definition"></a></dt>
<dd><p>A list of all criteria for which a model might be “best”. i.e. your
criteria might be “precision”, “recall”, and “jaccard”. It is unlikely
that the best iteration/post processing parameters will be the same
for all 3 of these criteria</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id6">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id6" title="Link to this definition"></a></dt>
<dd><p>Evaluate the given output array and dataset and returns the scores based on predefined criteria.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – The output array to be evaluated.</p></li>
<li><p><strong>evaluation_dataset</strong> – The dataset to be used for evaluation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An object of DummyEvaluationScores class, with the evaluation scores.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>DummyEvaluationScore</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – if the output array identifier is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_evaluator</span> <span class="o">=</span> <span class="n">DummyEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="s2">&quot;output_array&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="s2">&quot;evaluation_dataset&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_dataset</span><span class="p">)</span>
<span class="go">DummyEvaluationScores(frizz_level=0.0, blipp_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to evaluate the output array against the evaluation dataset.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id7">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="dummy_evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.dummy_evaluation_scores.DummyEvaluationScores" title="dacapo.experiments.tasks.evaluators.dummy_evaluation_scores.DummyEvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.dummy_evaluation_scores.DummyEvaluationScores</span></a></em><a class="headerlink" href="#id7" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An object of DummyEvaluationScores class, with the evaluation scores.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.DummyEvaluationScores" title="dacapo.experiments.tasks.evaluators.DummyEvaluationScores">DummyEvaluationScores</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_evaluator</span> <span class="o">=</span> <span class="n">DummyEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dummy_evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">DummyEvaluationScores(frizz_level=0.0, blipp_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.EvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">EvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.EvaluationScores" title="Link to this definition"></a></dt>
<dd><p>Base class for evaluation scores. This class is used to store the evaluation scores for a task.
The scores include the evaluation criteria. The class also provides methods to determine whether higher is better for a given criterion,
the bounds for a given criterion, and whether to store the best score for a given criterion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.EvaluationScores.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.EvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd><p>List[str]
the evaluation criteria</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.EvaluationScores.higher_is_better">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.EvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.EvaluationScores.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.EvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.EvaluationScores.store_best">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.EvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The EvaluationScores class is used to store the evaluation scores for a task. All evaluation scores should inherit from this class.</p>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="id8">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id8" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>The evaluation criteria.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[&quot;criterion1&quot;, &quot;criterion2&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id9">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id9" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Wether or not higher is better for this criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether higher is better for this criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether higher is better for a given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id10">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id10" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>The bounds for this criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple[Union[int, float, None], Union[int, float, None]]</dt><dd><p>the bounds for this criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">(0, 1)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the bounds for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id11">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id11" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>Whether or not to save the best validation block and model
weights for this criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether to store the best score for this criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion1&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return whether to store the best score for the given criterion.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">Evaluator</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator" title="Link to this definition"></a></dt>
<dd><p>Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.</p>
<p>An evaluator takes a post-processor’s output and compares it against
ground-truth. It then returns a set of scores that can be used to
determine the quality of the post-processor’s output.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.best_scores">
<span class="sig-name descname"><span class="pre">best_scores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.best_scores" title="Link to this definition"></a></dt>
<dd><p>Dict[OutputIdentifier, BestScore]
the best scores for each dataset/post-processing parameter/criterion combination</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Compare and evaluate the output array against the evaluation array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.is_best">
<span class="sig-name descname"><span class="pre">is_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.is_best" title="Link to this definition"></a></dt>
<dd><p>Check if the provided score is the best for this dataset/parameter/criterion combo.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.get_overall_best">
<span class="sig-name descname"><span class="pre">get_overall_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.get_overall_best" title="Link to this definition"></a></dt>
<dd><p>Return the best score for the given dataset and criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.get_overall_best_parameters">
<span class="sig-name descname"><span class="pre">get_overall_best_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.get_overall_best_parameters" title="Link to this definition"></a></dt>
<dd><p>Return the best parameters for the given dataset and criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.compare">
<span class="sig-name descname"><span class="pre">compare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.compare" title="Link to this definition"></a></dt>
<dd><p>Compare two scores for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.set_best">
<span class="sig-name descname"><span class="pre">set_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">validation_scores</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.set_best" title="Link to this definition"></a></dt>
<dd><p>Find the best iteration for each dataset/post_processing_parameter/criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.higher_is_better">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.bounds" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.store_best">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.store_best" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Evaluator class is used to compare and evaluate the output array against the evaluation array.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="id12">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dacapo.store.local_array_store.LocalArrayIdentifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">funlib.persistence.Array</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></span></span><a class="headerlink" href="#id12" title="Link to this definition"></a></dt>
<dd><p>Compares and evaluates the output array against the evaluation array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – LocalArrayIdentifier
The identifier of the output array.</p></li>
<li><p><strong>evaluation_array</strong> – Array
The evaluation array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>EvaluationScores</dt><dd><p>The evaluation scores.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="n">LocalArrayIdentifier</span><span class="p">(</span><span class="s2">&quot;output_array&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_array</span> <span class="o">=</span> <span class="n">Array</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_array</span><span class="p">)</span>
<span class="go">EvaluationScores()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to compare and evaluate the output array against the evaluation array.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id13">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">best_scores</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">OutputIdentifier</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../../utils/view/index.html#dacapo.utils.view.BestScore" title="dacapo.utils.view.BestScore"><span class="pre">BestScore</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id13" title="Link to this definition"></a></dt>
<dd><p>The best scores for each dataset/post-processing parameter/criterion combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Dict[OutputIdentifier, BestScore]</dt><dd><p>the best scores for each dataset/post-processing parameter/criterion combination</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AttributeError</strong> – if the best scores are not set</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">best_scores</span>
<span class="go">{}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best scores for each dataset/post-processing parameter/criterion combination.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id14">
<span class="sig-name descname"><span class="pre">is_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../post_processors/index.html#dacapo.experiments.tasks.post_processors.PostProcessorParameters" title="dacapo.experiments.tasks.post_processors.PostProcessorParameters"><span class="pre">dacapo.experiments.tasks.post_processors.PostProcessorParameters</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id14" title="Link to this definition"></a></dt>
<dd><p>Check if the provided score is the best for this dataset/parameter/criterion combo.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>parameter</strong> – PostProcessorParameters
the post-processor parameters</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
<li><p><strong>score</strong> – EvaluationScores
the evaluation scores</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether the provided score is the best for this dataset/parameter/criterion combo</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameter</span> <span class="o">=</span> <span class="n">PostProcessorParameters</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">is_best</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to check if the provided score is the best for this dataset/parameter/criterion combo.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id15">
<span class="sig-name descname"><span class="pre">get_overall_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id15" title="Link to this definition"></a></dt>
<dd><p>Return the best score for the given dataset and criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Optional[float]</dt><dd><p>the best score for the given dataset and criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">get_overall_best</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best score for the given dataset and criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id16">
<span class="sig-name descname"><span class="pre">get_overall_best_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id16" title="Link to this definition"></a></dt>
<dd><p>Return the best parameters for the given dataset and criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Optional[PostProcessorParameters]</dt><dd><p>the best parameters for the given dataset and criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">get_overall_best_parameters</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best parameters for the given dataset and criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id17">
<span class="sig-name descname"><span class="pre">compare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id17" title="Link to this definition"></a></dt>
<dd><p>Compare two scores for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>score_1</strong> – float
the first score</p></li>
<li><p><strong>score_2</strong> – float
the second score</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether the first score is better than the second score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score_1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score_2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">score_1</span><span class="p">,</span> <span class="n">score_2</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to compare two scores for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id18">
<span class="sig-name descname"><span class="pre">set_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">validation_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../validation_scores/index.html#dacapo.experiments.validation_scores.ValidationScores" title="dacapo.experiments.validation_scores.ValidationScores"><span class="pre">dacapo.experiments.validation_scores.ValidationScores</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a></span></span><a class="headerlink" href="#id18" title="Link to this definition"></a></dt>
<dd><p>Find the best iteration for each dataset/post_processing_parameter/criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>validation_scores</strong> – ValidationScores
the validation scores</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">validation_scores</span> <span class="o">=</span> <span class="n">ValidationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">set_best</span><span class="p">(</span><span class="n">validation_scores</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to find the best iteration for each dataset/post_processing_parameter/criterion.
Typically, this function is called after the validation scores have been computed.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.criteria">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.criteria" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>A list of all criteria for which a model might be “best”. i.e. your
criteria might be “precision”, “recall”, and “jaccard”. It is unlikely
that the best iteration/post processing parameters will be the same
for all 3 of these criteria</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id19">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id19" title="Link to this definition"></a></dt>
<dd><p>Wether or not higher is better for this criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether higher is better for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether higher is better for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id20">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id20" title="Link to this definition"></a></dt>
<dd><p>The bounds for this criterion</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple[Union[int, float, None], Union[int, float, None]]</dt><dd><p>the bounds for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">(0, 1)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the bounds for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id21">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id21" title="Link to this definition"></a></dt>
<dd><p>The bounds for this criterion</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether to store the best score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return whether to store the best score for the given criterion.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.Evaluator.score">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.Evaluator.score" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>The evaluation scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>EvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">EvaluationScores()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">MultiChannelBinarySegmentationEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>Class representing evaluation scores for multi-channel binary segmentation tasks.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.channel_scores">
<span class="sig-name descname"><span class="pre">channel_scores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.channel_scores" title="Link to this definition"></a></dt>
<dd><p>The list of channel scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Tuple[str, <a class="reference internal" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores">BinarySegmentationEvaluationScores</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">higher_is_better(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Determines whether a higher value is better for a given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">store_best(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Whether or not to store the best weights/validation blocks for this criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">bounds(criterion</span></span></dt>
<dd><p>str) -&gt; Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.</p>
</dd></dl>

<p class="rubric">Notes</p>
<p>The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="id22">
<span class="sig-name descname"><span class="pre">channel_scores</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores"><span class="pre">BinarySegmentationEvaluationScores</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id22" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.criteria">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd><p>Returns a list of all criteria for all channels.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The list of criteria.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">channel_scores</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="n">BinarySegmentationEvaluationScores</span><span class="p">()),</span> <span class="p">(</span><span class="s2">&quot;channel2&quot;</span><span class="p">,</span> <span class="n">BinarySegmentationEvaluationScores</span><span class="p">())]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="p">(</span><span class="n">channel_scores</span><span class="p">)</span><span class="o">.</span><span class="n">criteria</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns a list of all criteria for all channels. The criteria are stored as attributes of the class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.higher_is_better">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Determines whether a higher value is better for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if a higher value is better, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;channel1__f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.store_best">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Determines whether or not to store the best weights/validation blocks for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if the best weights/validation blocks should be stored, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;channel1__f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.bounds">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.MultiChannelBinarySegmentationEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The lower and upper bounds for the criterion.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[Union[int, float, None], Union[int, float, None]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">(0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;channel1__hausdorff&quot;</span><span class="p">)</span>
<span class="go">(0, nan)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">BinarySegmentationEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>Class representing evaluation scores for binary segmentation tasks.</p>
<p>The metrics include:
- Dice coefficient: 2 * <a href="#id59"><span class="problematic" id="id60">|A ∩ B|</span></a> / <a href="#id61"><span class="problematic" id="id62">|A|</span></a> + <a href="#id63"><span class="problematic" id="id64">|B|</span></a> ; where A and B are the binary segmentations
- Jaccard coefficient: <a href="#id65"><span class="problematic" id="id66">|A ∩ B|</span></a> / <a href="#id67"><span class="problematic" id="id68">|A ∪ B|</span></a> ; where A and B are the binary segmentations
- Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
- False negative rate: <a href="#id69"><span class="problematic" id="id70">|A - B|</span></a> / <a href="#id71"><span class="problematic" id="id72">|A|</span></a> ; where A and B are the binary segmentations
- False positive rate: <a href="#id73"><span class="problematic" id="id74">|B - A|</span></a> / <a href="#id75"><span class="problematic" id="id76">|B|</span></a> ; where A and B are the binary segmentations
- False discovery rate: <a href="#id77"><span class="problematic" id="id78">|B - A|</span></a> / <a href="#id79"><span class="problematic" id="id80">|A|</span></a> ; where A and B are the binary segmentations
- VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
- Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
- Mean false negative distance: mean distance of false negatives
- Mean false positive distance: mean distance of false positives
- Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
- Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
- Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
- Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
- Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
- F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
- Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
- Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
- F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.dice">
<span class="sig-name descname"><span class="pre">dice</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.dice" title="Link to this definition"></a></dt>
<dd><p>The Dice coefficient.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.jaccard">
<span class="sig-name descname"><span class="pre">jaccard</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.jaccard" title="Link to this definition"></a></dt>
<dd><p>The Jaccard index.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.hausdorff">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.hausdorff" title="Link to this definition"></a></dt>
<dd><p>The Hausdorff distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_negative_rate">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_negative_rate" title="Link to this definition"></a></dt>
<dd><p>The false negative rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_negative_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_negative_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The false negative rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_positive_rate">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_positive_rate" title="Link to this definition"></a></dt>
<dd><p>The false positive rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_discovery_rate">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_discovery_rate" title="Link to this definition"></a></dt>
<dd><p>The false discovery rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_positive_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.false_positive_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The false positive rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.voi">
<span class="sig-name descname"><span class="pre">voi</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.voi" title="Link to this definition"></a></dt>
<dd><p>The variation of information.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_distance">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_negative_distance">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_negative_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false negative distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_positive_distance">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_positive_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false positive distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_negative_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_negative_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false negative distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_positive_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.mean_false_positive_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false positive distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.precision_with_tolerance">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.precision_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The precision with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.recall_with_tolerance">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.recall_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The recall with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.f1_score_with_tolerance">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.f1_score_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The F1 score with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.precision" title="Link to this definition"></a></dt>
<dd><p>The precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.recall">
<span class="sig-name descname"><span class="pre">recall</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.recall" title="Link to this definition"></a></dt>
<dd><p>The recall.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.f1_score">
<span class="sig-name descname"><span class="pre">f1_score</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.f1_score" title="Link to this definition"></a></dt>
<dd><p>The F1 score.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">store_best(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Whether or not to store the best weights/validation blocks for this criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">higher_is_better(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Determines whether a higher value is better for a given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">bounds(criterion</span></span></dt>
<dd><p>str) -&gt; Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.</p>
</dd></dl>

<p class="rubric">Notes</p>
<p>The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="id23">
<span class="sig-name descname"><span class="pre">dice</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id23" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id24">
<span class="sig-name descname"><span class="pre">jaccard</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id24" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id25">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id25" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id26">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id26" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id27">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id27" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id28">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id28" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id29">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id29" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id30">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id30" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id31">
<span class="sig-name descname"><span class="pre">voi</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id31" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id32">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id32" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id33">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id33" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id34">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id34" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id35">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id35" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id36">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id36" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id37">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id37" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id38">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id38" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id39">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id39" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id40">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id40" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id41">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id41" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id42">
<span class="sig-name descname"><span class="pre">recall</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id42" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id43">
<span class="sig-name descname"><span class="pre">f1_score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id43" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['dice',</span> <span class="pre">'jaccard',</span> <span class="pre">'hausdorff',</span> <span class="pre">'false_negative_rate',</span> <span class="pre">'false_negative_rate_with_tolerance',...</span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd><p>The evaluation criteria.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[&quot;criterion1&quot;, &quot;criterion2&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.store_best">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Determines whether or not to store the best weights/validation blocks for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if the best weights/validation blocks should be stored, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.higher_is_better">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Determines whether a higher value is better for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if a higher value is better, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.bounds">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The lower and upper bounds for the criterion.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[Union[int, float, None], Union[int, float, None]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">(0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;hausdorff&quot;</span><span class="p">)</span>
<span class="go">(0, nan)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">BinarySegmentationEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">clip_distance</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol_distance</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator" title="Link to this definition"></a></dt>
<dd><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:
- Dice coefficient: 2 * <a href="#id81"><span class="problematic" id="id82">|A ∩ B|</span></a> / <a href="#id83"><span class="problematic" id="id84">|A|</span></a> + <a href="#id85"><span class="problematic" id="id86">|B|</span></a> ; where A and B are the binary segmentations
- Jaccard coefficient: <a href="#id87"><span class="problematic" id="id88">|A ∩ B|</span></a> / <a href="#id89"><span class="problematic" id="id90">|A ∪ B|</span></a> ; where A and B are the binary segmentations
- Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
- False negative rate: <a href="#id91"><span class="problematic" id="id92">|A - B|</span></a> / <a href="#id93"><span class="problematic" id="id94">|A|</span></a> ; where A and B are the binary segmentations
- False positive rate: <a href="#id95"><span class="problematic" id="id96">|B - A|</span></a> / <a href="#id97"><span class="problematic" id="id98">|B|</span></a> ; where A and B are the binary segmentations
- False discovery rate: <a href="#id99"><span class="problematic" id="id100">|B - A|</span></a> / <a href="#id101"><span class="problematic" id="id102">|A|</span></a> ; where A and B are the binary segmentations
- VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
- Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
- Mean false negative distance: mean distance of false negatives
- Mean false positive distance: mean distance of false positives
- Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
- Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
- Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
- Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
- Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
- F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
- Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
- Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
- F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.clip_distance">
<span class="sig-name descname"><span class="pre">clip_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.clip_distance" title="Link to this definition"></a></dt>
<dd><p>float
the clip distance</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.tol_distance">
<span class="sig-name descname"><span class="pre">tol_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.tol_distance" title="Link to this definition"></a></dt>
<dd><p>float
the tolerance distance</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.channels">
<span class="sig-name descname"><span class="pre">channels</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.channels" title="Link to this definition"></a></dt>
<dd><p>List[str]
the channels</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.criteria" title="Link to this definition"></a></dt>
<dd><p>List[str]
the evaluation criteria</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.BinarySegmentationEvaluator.score" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The BinarySegmentationEvaluator class is used to evaluate the performance of a binary segmentation task.
The class provides methods to evaluate the output array against the evaluation array and return the evaluation scores.
All evaluation scores should inherit from this class.</p>
<p>Clip distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a false positive.
Tolerance distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a true positive.
Channels are the channels of the binary segmentation.
Criteria are the evaluation criteria.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="id44">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['jaccard',</span> <span class="pre">'voi']</span></em><a class="headerlink" href="#id44" title="Link to this definition"></a></dt>
<dd><p>A list of all criteria for which a model might be “best”. i.e. your
criteria might be “precision”, “recall”, and “jaccard”. It is unlikely
that the best iteration/post processing parameters will be the same
for all 3 of these criteria</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id45">
<span class="sig-name descname"><span class="pre">clip_distance</span></span><a class="headerlink" href="#id45" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id46">
<span class="sig-name descname"><span class="pre">tol_distance</span></span><a class="headerlink" href="#id46" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id47">
<span class="sig-name descname"><span class="pre">channels</span></span><a class="headerlink" href="#id47" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id48">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id48" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – str
the identifier of the output array</p></li>
<li><p><strong>evaluation_array</strong> – Zarr Array
the evaluation array</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the output array identifier is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">(</span><span class="n">clip_distance</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tol_distance</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="s2">&quot;channel2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="s2">&quot;output_array&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_array</span> <span class="o">=</span> <span class="n">open_from_identifier</span><span class="p">(</span><span class="s2">&quot;evaluation_array&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_array</span><span class="p">)</span>
<span class="go">BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to evaluate the output array against the evaluation array.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id49">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><a class="headerlink" href="#id49" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">(</span><span class="n">clip_distance</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tol_distance</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="s2">&quot;channel2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">InstanceEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>The evaluation scores for the instance segmentation task. The scores include the variation of information (VOI) split, VOI merge, and VOI.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi_split">
<span class="sig-name descname"><span class="pre">voi_split</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi_split" title="Link to this definition"></a></dt>
<dd><p>float
the variation of information (VOI) split</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi_merge">
<span class="sig-name descname"><span class="pre">voi_merge</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi_merge" title="Link to this definition"></a></dt>
<dd><p>float
the variation of information (VOI) merge</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi">
<span class="sig-name descname"><span class="pre">voi</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.voi" title="Link to this definition"></a></dt>
<dd><p>float
the variation of information (VOI)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.higher_is_better">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.store_best">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The InstanceEvaluationScores class is used to store the evaluation scores for the instance segmentation task.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['voi_split',</span> <span class="pre">'voi_merge',</span> <span class="pre">'voi']</span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd><p>The evaluation criteria.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_scores</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[&quot;criterion1&quot;, &quot;criterion2&quot;]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id50">
<span class="sig-name descname"><span class="pre">voi_split</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id50" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id51">
<span class="sig-name descname"><span class="pre">voi_merge</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id51" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id52">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">voi</span></span><a class="headerlink" href="#id52" title="Link to this definition"></a></dt>
<dd><p>Return the average of the VOI split and VOI merge.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>float</dt><dd><p>the average of the VOI split and VOI merge</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluation_scores</span> <span class="o">=</span> <span class="n">InstanceEvaluationScores</span><span class="p">(</span><span class="n">voi_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">voi_merge</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluation_scores</span><span class="o">.</span><span class="n">voi</span>
<span class="go">0.15</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to calculate the average of the VOI split and VOI merge.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id53">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id53" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether higher is better for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">InstanceEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;voi_split&quot;</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether higher is better for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id54">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id54" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple[Union[int, float, None], Union[int, float, None]]</dt><dd><p>the bounds for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">InstanceEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;voi_split&quot;</span><span class="p">)</span>
<span class="go">(0, 1)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the bounds for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id55">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id55" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the evaluation criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether to store the best score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">InstanceEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;voi_split&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether to store the best score for the given criterion.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.</span></span><span class="sig-name descname"><span class="pre">InstanceEvaluator</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluator" title="Link to this definition"></a></dt>
<dd><p>A class representing an evaluator for instance segmentation tasks.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluator.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluator.criteria" title="Link to this definition"></a></dt>
<dd><p>List[str]
the evaluation criteria</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.InstanceEvaluator.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.InstanceEvaluator.score" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The InstanceEvaluator class is used to evaluate the performance of an instance segmentation task.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="id56">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['voi_merge',</span> <span class="pre">'voi_split',</span> <span class="pre">'voi']</span></em><a class="headerlink" href="#id56" title="Link to this definition"></a></dt>
<dd><p>A list of all criteria for which a model might be “best”. i.e. your
criteria might be “precision”, “recall”, and “jaccard”. It is unlikely
that the best iteration/post processing parameters will be the same
for all 3 of these criteria</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id57">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id57" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – str
the identifier of the output array</p></li>
<li><p><strong>evaluation_array</strong> – Zarr Array
the evaluation array</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>InstanceEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the output array identifier is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluator</span> <span class="o">=</span> <span class="n">InstanceEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="s2">&quot;output_array&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_array</span> <span class="o">=</span> <span class="n">open_from_identifier</span><span class="p">(</span><span class="s2">&quot;evaluation_array&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_array</span><span class="p">)</span>
<span class="go">InstanceEvaluationScores(voi_merge=0.0, voi_split=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to evaluate the output array against the evaluation array.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id58">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="instance_evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores" title="dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores</span></a></em><a class="headerlink" href="#id58" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>InstanceEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluator</span> <span class="o">=</span> <span class="n">InstanceEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">instance_evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">InstanceEvaluationScores(voi_merge=0.0, voi_split=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../dummy_task_config/index.html" class="btn btn-neutral float-left" title="dacapo.experiments.tasks.dummy_task_config" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="binary_segmentation_evaluation_scores/index.html" class="btn btn-neutral float-right" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, William Patton, Jeff Rhoades, Marwan Zouinkhi,  David Ackerman, Caroline Malin-Mayor, Jan Funke.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>