<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator &mdash; DaCapo  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/custom.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
    <link rel="next" title="dacapo.experiments.tasks.evaluators.dummy_evaluation_scores" href="../dummy_evaluation_scores/index.html" />
    <link rel="prev" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores" href="../binary_segmentation_evaluation_scores/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            DaCapo
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">DaCapo API:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorial.html">Tutorial: A Simple Experiment in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../docker.html">Docker Configuration for JupyterHub-Dacapo</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../../index.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../../index.html">dacapo</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../../index.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../../../../blockwise/index.html">dacapo.blockwise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../compute_context/index.html">dacapo.compute_context</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../../../index.html">dacapo.experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ext/index.html">dacapo.ext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../gp/index.html">dacapo.gp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../store/index.html">dacapo.store</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../utils/index.html">dacapo.utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../index.html#submodules">Submodules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../index.html#classes">Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../index.html#functions">Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../index.html#package-contents">Package Contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../cli.html">CLI</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">DaCapo</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../../index.html">API Reference</a></li>
          <li class="breadcrumb-item"><a href="../../../../index.html">dacapo</a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">dacapo.experiments</a></li>
          <li class="breadcrumb-item"><a href="../../index.html">dacapo.experiments.tasks</a></li>
          <li class="breadcrumb-item"><a href="../index.html">dacapo.experiments.tasks.evaluators</a></li>
      <li class="breadcrumb-item active">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/autoapi/dacapo/experiments/tasks/evaluators/binary_segmentation_evaluator/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator">
<span id="dacapo-experiments-tasks-evaluators-binary-segmentation-evaluator"></span><h1>dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator<a class="headerlink" href="#module-dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator" title="Link to this heading"></a></h1>
<section id="attributes">
<h2>Attributes<a class="headerlink" href="#attributes" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.logger" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BG" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BG"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BG</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</section>
<section id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Evaluator</span></code></a></p></td>
<td><p>Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BinarySegmentationEvaluationScores</span></code></a></p></td>
<td><p>Class representing evaluation scores for binary segmentation tasks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiChannelBinarySegmentationEvaluationScores</span></code></a></p></td>
<td><p>Class representing evaluation scores for multi-channel binary segmentation tasks.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ZarrArray</span></code></a></p></td>
<td><p>This is a zarr array.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BinarySegmentationEvaluator</span></code></a></p></td>
<td><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ArrayEvaluator</span></code></a></p></td>
<td><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CremiEvaluator</span></code></a></p></td>
<td><p>Evaluate the performance of a binary segmentation task using the CREMI score.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Link to this heading"></a></h2>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.voi" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.voi"><code class="xref py py-obj docutils literal notranslate"><span class="pre">voi</span></code></a>(reconstruction, groundtruth[, ...])</p></td>
<td><p>Return the conditional entropies of the variation of information metric. [1]</p></td>
</tr>
</tbody>
</table>
</section>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Link to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.voi">
<span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">voi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reconstruction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groundtruth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_reconstruction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_groundtruth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[0]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.voi" title="Link to this definition"></a></dt>
<dd><p>Return the conditional entropies of the variation of information metric. [1]</p>
<p>Let X be a reconstruction, and Y a ground truth labelling. The variation of
information between the two is the sum of two conditional entropies:</p>
<blockquote>
<div><p>VI(X, Y) = H(X|Y) + H(Y|X).</p>
</div></blockquote>
<p>The first one, H(X|Y), is a measure of oversegmentation, the second one,
H(Y|X), a measure of undersegmentation. These measures are referred to as
the variation of information split or merge error, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seg</strong> (<em>np.ndarray</em><em>, </em><em>int type</em><em>, </em><em>arbitrary shape</em>) – A candidate segmentation.</p></li>
<li><p><strong>gt</strong> (np.ndarray, int type, same shape as <cite>seg</cite>) – The ground truth segmentation.</p></li>
<li><p><strong>ignore_seg</strong> (<em>list</em><em> of </em><em>int</em><em>, </em><em>optional</em>) – Any points having a label in this list are ignored in the evaluation.
By default, only the label 0 in the ground truth will be ignored.</p></li>
<li><p><strong>ignore_gt</strong> (<em>list</em><em> of </em><em>int</em><em>, </em><em>optional</em>) – Any points having a label in this list are ignored in the evaluation.
By default, only the label 0 in the ground truth will be ignored.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>(split, merge)</strong> – The variation of information split and merge error, i.e., H(X|Y) and H(Y|X)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>reconstruction</cite> and <cite>groundtruth</cite> have different shapes.</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>[1] Meila, M. (2007). Comparing clusterings - an information based
distance. Journal of Multivariate Analysis 98, 873-895.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">Evaluator</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator" title="Link to this definition"></a></dt>
<dd><p>Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.</p>
<p>An evaluator takes a post-processor’s output and compares it against
ground-truth. It then returns a set of scores that can be used to
determine the quality of the post-processor’s output.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.best_scores">
<span class="sig-name descname"><span class="pre">best_scores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.best_scores" title="Link to this definition"></a></dt>
<dd><p>Dict[OutputIdentifier, BestScore]
the best scores for each dataset/post-processing parameter/criterion combination</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Compare and evaluate the output array against the evaluation array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.is_best">
<span class="sig-name descname"><span class="pre">is_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.is_best" title="Link to this definition"></a></dt>
<dd><p>Check if the provided score is the best for this dataset/parameter/criterion combo.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.get_overall_best">
<span class="sig-name descname"><span class="pre">get_overall_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.get_overall_best" title="Link to this definition"></a></dt>
<dd><p>Return the best score for the given dataset and criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.get_overall_best_parameters">
<span class="sig-name descname"><span class="pre">get_overall_best_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.get_overall_best_parameters" title="Link to this definition"></a></dt>
<dd><p>Return the best parameters for the given dataset and criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.compare">
<span class="sig-name descname"><span class="pre">compare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.compare" title="Link to this definition"></a></dt>
<dd><p>Compare two scores for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.set_best">
<span class="sig-name descname"><span class="pre">set_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">validation_scores</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.set_best" title="Link to this definition"></a></dt>
<dd><p>Find the best iteration for each dataset/post_processing_parameter/criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.higher_is_better">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Return whether higher is better for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.bounds">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.bounds" title="Link to this definition"></a></dt>
<dd><p>Return the bounds for the given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.store_best">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.store_best" title="Link to this definition"></a></dt>
<dd><p>Return whether to store the best score for the given criterion.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Evaluator class is used to compare and evaluate the output array against the evaluation array.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="id0">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../../store/local_array_store/index.html#dacapo.store.local_array_store.LocalArrayIdentifier" title="dacapo.store.local_array_store.LocalArrayIdentifier"><span class="pre">dacapo.store.local_array_store.LocalArrayIdentifier</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../datasplits/datasets/arrays/index.html#dacapo.experiments.datasplits.datasets.arrays.Array" title="dacapo.experiments.datasplits.datasets.arrays.Array"><span class="pre">dacapo.experiments.datasplits.datasets.arrays.Array</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></span></span><a class="headerlink" href="#id0" title="Link to this definition"></a></dt>
<dd><p>Compares and evaluates the output array against the evaluation array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – LocalArrayIdentifier
The identifier of the output array.</p></li>
<li><p><strong>evaluation_array</strong> – Array
The evaluation array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>EvaluationScores</dt><dd><p>The evaluation scores.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="n">LocalArrayIdentifier</span><span class="p">(</span><span class="s2">&quot;output_array&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_array</span> <span class="o">=</span> <span class="n">Array</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_array</span><span class="p">)</span>
<span class="go">EvaluationScores()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to compare and evaluate the output array against the evaluation array.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id1">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">best_scores</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">OutputIdentifier</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="../../../../utils/view/index.html#dacapo.utils.view.BestScore" title="dacapo.utils.view.BestScore"><span class="pre">BestScore</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id1" title="Link to this definition"></a></dt>
<dd><p>The best scores for each dataset/post-processing parameter/criterion combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>Dict[OutputIdentifier, BestScore]</dt><dd><p>the best scores for each dataset/post-processing parameter/criterion combination</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>AttributeError</strong> – if the best scores are not set</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">best_scores</span>
<span class="go">{}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best scores for each dataset/post-processing parameter/criterion combination.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id2">
<span class="sig-name descname"><span class="pre">is_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../post_processors/index.html#dacapo.experiments.tasks.post_processors.PostProcessorParameters" title="dacapo.experiments.tasks.post_processors.PostProcessorParameters"><span class="pre">dacapo.experiments.tasks.post_processors.PostProcessorParameters</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id2" title="Link to this definition"></a></dt>
<dd><p>Check if the provided score is the best for this dataset/parameter/criterion combo.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>parameter</strong> – PostProcessorParameters
the post-processor parameters</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
<li><p><strong>score</strong> – EvaluationScores
the evaluation scores</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether the provided score is the best for this dataset/parameter/criterion combo</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameter</span> <span class="o">=</span> <span class="n">PostProcessorParameters</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span> <span class="o">=</span> <span class="n">EvaluationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">is_best</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">parameter</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to check if the provided score is the best for this dataset/parameter/criterion combo.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">get_overall_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id3" title="Link to this definition"></a></dt>
<dd><p>Return the best score for the given dataset and criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Optional[float]</dt><dd><p>the best score for the given dataset and criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">get_overall_best</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best score for the given dataset and criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id4">
<span class="sig-name descname"><span class="pre">get_overall_best_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../datasplits/datasets/index.html#dacapo.experiments.datasplits.datasets.Dataset" title="dacapo.experiments.datasplits.datasets.Dataset"><span class="pre">dacapo.experiments.datasplits.datasets.Dataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id4" title="Link to this definition"></a></dt>
<dd><p>Return the best parameters for the given dataset and criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – Dataset
the dataset</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Optional[PostProcessorParameters]</dt><dd><p>the best parameters for the given dataset and criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">get_overall_best_parameters</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the best parameters for the given dataset and criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id5">
<span class="sig-name descname"><span class="pre">compare</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score_2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id5" title="Link to this definition"></a></dt>
<dd><p>Compare two scores for the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>score_1</strong> – float
the first score</p></li>
<li><p><strong>score_2</strong> – float
the second score</p></li>
<li><p><strong>criterion</strong> – str
the criterion</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether the first score is better than the second score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score_1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score_2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">score_1</span><span class="p">,</span> <span class="n">score_2</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to compare two scores for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id6">
<span class="sig-name descname"><span class="pre">set_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">validation_scores</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="../../../validation_scores/index.html#dacapo.experiments.validation_scores.ValidationScores" title="dacapo.experiments.validation_scores.ValidationScores"><span class="pre">dacapo.experiments.validation_scores.ValidationScores</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a></span></span><a class="headerlink" href="#id6" title="Link to this definition"></a></dt>
<dd><p>Find the best iteration for each dataset/post_processing_parameter/criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>validation_scores</strong> – ValidationScores
the validation scores</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">validation_scores</span> <span class="o">=</span> <span class="n">ValidationScores</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">set_best</span><span class="p">(</span><span class="n">validation_scores</span><span class="p">)</span>
<span class="go">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to find the best iteration for each dataset/post_processing_parameter/criterion.
Typically, this function is called after the validation scores have been computed.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.criteria">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.criteria" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>A list of all criteria for which a model might be “best”. i.e. your
criteria might be “precision”, “recall”, and “jaccard”. It is unlikely
that the best iteration/post processing parameters will be the same
for all 3 of these criteria</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>List[str]</dt><dd><p>the evaluation criteria</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">criteria</span>
<span class="go">[]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation criteria.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id7">
<span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id7" title="Link to this definition"></a></dt>
<dd><p>Wether or not higher is better for this criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether higher is better for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to determine whether higher is better for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id8">
<span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#id8" title="Link to this definition"></a></dt>
<dd><p>The bounds for this criterion</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Tuple[Union[int, float, None], Union[int, float, None]]</dt><dd><p>the bounds for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">(0, 1)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the bounds for the given criterion.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id9">
<span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#id9" title="Link to this definition"></a></dt>
<dd><p>The bounds for this criterion</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> – str
the criterion</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>bool</dt><dd><p>whether to store the best score for the given criterion</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="s2">&quot;criterion&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="n">criterion</span><span class="p">)</span>
<span class="go">False</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return whether to store the best score for the given criterion.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.score">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="../evaluation_scores/index.html#dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores" title="dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores"><span class="pre">dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores</span></a></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator.score" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Abstractmethod<span class="colon">:</span></dt>
<dd class="field-odd"><p></p></dd>
</dl>
<p>The evaluation scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>EvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">EvaluationScores()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">BinarySegmentationEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>Class representing evaluation scores for binary segmentation tasks.</p>
<p>The metrics include:
- Dice coefficient: 2 * <a href="#id85"><span class="problematic" id="id86">|A ∩ B|</span></a> / <a href="#id87"><span class="problematic" id="id88">|A|</span></a> + <a href="#id89"><span class="problematic" id="id90">|B|</span></a> ; where A and B are the binary segmentations
- Jaccard coefficient: <a href="#id91"><span class="problematic" id="id92">|A ∩ B|</span></a> / <a href="#id93"><span class="problematic" id="id94">|A ∪ B|</span></a> ; where A and B are the binary segmentations
- Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
- False negative rate: <a href="#id95"><span class="problematic" id="id96">|A - B|</span></a> / <a href="#id97"><span class="problematic" id="id98">|A|</span></a> ; where A and B are the binary segmentations
- False positive rate: <a href="#id99"><span class="problematic" id="id100">|B - A|</span></a> / <a href="#id101"><span class="problematic" id="id102">|B|</span></a> ; where A and B are the binary segmentations
- False discovery rate: <a href="#id103"><span class="problematic" id="id104">|B - A|</span></a> / <a href="#id105"><span class="problematic" id="id106">|A|</span></a> ; where A and B are the binary segmentations
- VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
- Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
- Mean false negative distance: mean distance of false negatives
- Mean false positive distance: mean distance of false positives
- Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
- Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
- Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
- Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
- Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
- F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
- Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
- Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
- F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.dice">
<span class="sig-name descname"><span class="pre">dice</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.dice" title="Link to this definition"></a></dt>
<dd><p>The Dice coefficient.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.jaccard">
<span class="sig-name descname"><span class="pre">jaccard</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.jaccard" title="Link to this definition"></a></dt>
<dd><p>The Jaccard index.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.hausdorff">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.hausdorff" title="Link to this definition"></a></dt>
<dd><p>The Hausdorff distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_negative_rate">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_negative_rate" title="Link to this definition"></a></dt>
<dd><p>The false negative rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_negative_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_negative_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The false negative rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_positive_rate">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_positive_rate" title="Link to this definition"></a></dt>
<dd><p>The false positive rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_discovery_rate">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_discovery_rate" title="Link to this definition"></a></dt>
<dd><p>The false discovery rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_positive_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.false_positive_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The false positive rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.voi">
<span class="sig-name descname"><span class="pre">voi</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.voi" title="Link to this definition"></a></dt>
<dd><p>The variation of information.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_distance">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_negative_distance">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_negative_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false negative distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_positive_distance">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_positive_distance" title="Link to this definition"></a></dt>
<dd><p>The mean false positive distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_negative_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_negative_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false negative distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_positive_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.mean_false_positive_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>The mean false positive distance clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.precision_with_tolerance">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.precision_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The precision with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.recall_with_tolerance">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.recall_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The recall with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.f1_score_with_tolerance">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.f1_score_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>The F1 score with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.precision" title="Link to this definition"></a></dt>
<dd><p>The precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.recall">
<span class="sig-name descname"><span class="pre">recall</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.recall" title="Link to this definition"></a></dt>
<dd><p>The recall.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.f1_score">
<span class="sig-name descname"><span class="pre">f1_score</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.f1_score" title="Link to this definition"></a></dt>
<dd><p>The F1 score.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">store_best(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Whether or not to store the best weights/validation blocks for this criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">higher_is_better(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Determines whether a higher value is better for a given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">bounds(criterion</span></span></dt>
<dd><p>str) -&gt; Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.</p>
</dd></dl>

<p class="rubric">Notes</p>
<p>The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="id10">
<span class="sig-name descname"><span class="pre">dice</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id10" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id11">
<span class="sig-name descname"><span class="pre">jaccard</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id11" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id12">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id12" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id13">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id13" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id14">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id14" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id15">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id15" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id16">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id16" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id17">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id17" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id18">
<span class="sig-name descname"><span class="pre">voi</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id18" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id19">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id19" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id20">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id20" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id21">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id21" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id22">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id22" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id23">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id23" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id24">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id24" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id25">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id25" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id26">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id26" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id27">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id27" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id28">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id28" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id29">
<span class="sig-name descname"><span class="pre">recall</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id29" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id30">
<span class="sig-name descname"><span class="pre">f1_score</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#id30" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['dice',</span> <span class="pre">'jaccard',</span> <span class="pre">'hausdorff',</span> <span class="pre">'false_negative_rate',</span> <span class="pre">'false_negative_rate_with_tolerance',...</span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.store_best">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Determines whether or not to store the best weights/validation blocks for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if the best weights/validation blocks should be stored, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.higher_is_better">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Determines whether a higher value is better for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if a higher value is better, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.bounds">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The lower and upper bounds for the criterion.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[Union[int, float, None], Union[int, float, None]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;dice&quot;</span><span class="p">)</span>
<span class="go">(0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;hausdorff&quot;</span><span class="p">)</span>
<span class="go">(0, nan)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">MultiChannelBinarySegmentationEvaluationScores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores" title="Link to this definition"></a></dt>
<dd><p>Class representing evaluation scores for multi-channel binary segmentation tasks.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.channel_scores">
<span class="sig-name descname"><span class="pre">channel_scores</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.channel_scores" title="Link to this definition"></a></dt>
<dd><p>The list of channel scores.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[Tuple[str, <a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores">BinarySegmentationEvaluationScores</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">higher_is_better(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Determines whether a higher value is better for a given criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">store_best(criterion</span></span></dt>
<dd><p>str) -&gt; bool: Whether or not to store the best weights/validation blocks for this criterion.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">bounds(criterion</span></span></dt>
<dd><p>str) -&gt; Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.</p>
</dd></dl>

<p class="rubric">Notes</p>
<p>The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="id31">
<span class="sig-name descname"><span class="pre">channel_scores</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores"><span class="pre">BinarySegmentationEvaluationScores</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id31" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.criteria">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.criteria" title="Link to this definition"></a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Returns</span> <span class="pre">a</span> <span class="pre">list</span> <span class="pre">of</span> <span class="pre">all</span> <span class="pre">criteria</span> <span class="pre">for</span> <span class="pre">all</span> <span class="pre">channels.</span></span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The list of criteria.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>List[str]</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">channel_scores</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="n">BinarySegmentationEvaluationScores</span><span class="p">()),</span> <span class="p">(</span><span class="s2">&quot;channel2&quot;</span><span class="p">,</span> <span class="n">BinarySegmentationEvaluationScores</span><span class="p">())]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="p">(</span><span class="n">channel_scores</span><span class="p">)</span><span class="o">.</span><span class="n">criteria</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns a list of all criteria for all channels. The criteria are stored as attributes of the class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.higher_is_better">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">higher_is_better</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.higher_is_better" title="Link to this definition"></a></dt>
<dd><p>Determines whether a higher value is better for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if a higher value is better, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">higher_is_better</span><span class="p">(</span><span class="s2">&quot;channel1__f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.store_best">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">store_best</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.store_best" title="Link to this definition"></a></dt>
<dd><p>Determines whether or not to store the best weights/validation blocks for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>True if the best weights/validation blocks should be stored, False otherwise.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">store_best</span><span class="p">(</span><span class="s2">&quot;channel1__f1_score&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.bounds">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bounds</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores.bounds" title="Link to this definition"></a></dt>
<dd><p>Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>criterion</strong> (<em>str</em>) – The evaluation criterion.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The lower and upper bounds for the criterion.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[Union[int, float, None], Union[int, float, None]]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the criterion is not recognized.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;channel1__dice&quot;</span><span class="p">)</span>
<span class="go">(0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">MultiChannelBinarySegmentationEvaluationScores</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">&quot;channel1__hausdorff&quot;</span><span class="p">)</span>
<span class="go">(0, nan)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">ZarrArray</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray" title="Link to this definition"></a></dt>
<dd><p>This is a zarr array.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.name">
<span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.name" title="Link to this definition"></a></dt>
<dd><p>The name of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.file_name">
<span class="sig-name descname"><span class="pre">file_name</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.file_name" title="Link to this definition"></a></dt>
<dd><p>The file name of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Path</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dataset">
<span class="sig-name descname"><span class="pre">dataset</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dataset" title="Link to this definition"></a></dt>
<dd><p>The dataset name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._axes">
<span class="sig-name descname"><span class="pre">_axes</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._axes" title="Link to this definition"></a></dt>
<dd><p>The axes of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[List[str]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.snap_to_grid">
<span class="sig-name descname"><span class="pre">snap_to_grid</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.snap_to_grid" title="Link to this definition"></a></dt>
<dd><p>The snap to grid.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[Coordinate]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes the array type ‘raw’ and name for the DummyDataset instance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__str__">
<span class="sig-name descname"><span class="pre">__str__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__str__" title="Link to this definition"></a></dt>
<dd><p>Returns the string representation of the ZarrArray.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__repr__">
<span class="sig-name descname"><span class="pre">__repr__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__repr__" title="Link to this definition"></a></dt>
<dd><p>Returns the string representation of the ZarrArray.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.attrs">
<span class="sig-name descname"><span class="pre">attrs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.attrs" title="Link to this definition"></a></dt>
<dd><p>Returns the attributes of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.axes">
<span class="sig-name descname"><span class="pre">axes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.axes" title="Link to this definition"></a></dt>
<dd><p>Returns the axes of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dims">
<span class="sig-name descname"><span class="pre">dims</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dims" title="Link to this definition"></a></dt>
<dd><p>Returns the dimensions of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._daisy_array">
<span class="sig-name descname"><span class="pre">_daisy_array</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._daisy_array" title="Link to this definition"></a></dt>
<dd><p>Returns the daisy array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.voxel_size">
<span class="sig-name descname"><span class="pre">voxel_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.voxel_size" title="Link to this definition"></a></dt>
<dd><p>Returns the voxel size of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.roi">
<span class="sig-name descname"><span class="pre">roi</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.roi" title="Link to this definition"></a></dt>
<dd><p>Returns the region of interest of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.writable">
<span class="sig-name descname"><span class="pre">writable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.writable" title="Link to this definition"></a></dt>
<dd><p>Returns the boolean value of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dtype">
<span class="sig-name descname"><span class="pre">dtype</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.dtype" title="Link to this definition"></a></dt>
<dd><p>Returns the data type of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.num_channels">
<span class="sig-name descname"><span class="pre">num_channels</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.num_channels" title="Link to this definition"></a></dt>
<dd><p>Returns the number of channels of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.spatial_axes">
<span class="sig-name descname"><span class="pre">spatial_axes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.spatial_axes" title="Link to this definition"></a></dt>
<dd><p>Returns the spatial axes of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.data">
<span class="sig-name descname"><span class="pre">data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.data" title="Link to this definition"></a></dt>
<dd><p>Returns the data of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__getitem__">
<span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">roi</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__getitem__" title="Link to this definition"></a></dt>
<dd><p>Returns the data of the array for the given region of interest.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__setitem__">
<span class="sig-name descname"><span class="pre">__setitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">roi</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.__setitem__" title="Link to this definition"></a></dt>
<dd><p>Sets the data of the array for the given region of interest.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.create_from_array_identifier">
<span class="sig-name descname"><span class="pre">create_from_array_identifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roi</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">voxel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.create_from_array_identifier" title="Link to this definition"></a></dt>
<dd><p>Creates a new ZarrArray given an array identifier.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.open_from_array_identifier">
<span class="sig-name descname"><span class="pre">open_from_array_identifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.open_from_array_identifier" title="Link to this definition"></a></dt>
<dd><p>Opens a new ZarrArray given an array identifier.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._can_neuroglance">
<span class="sig-name descname"><span class="pre">_can_neuroglance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._can_neuroglance" title="Link to this definition"></a></dt>
<dd><p>Returns the boolean value of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._neuroglancer_source">
<span class="sig-name descname"><span class="pre">_neuroglancer_source</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._neuroglancer_source" title="Link to this definition"></a></dt>
<dd><p>Returns the neuroglancer source of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._neuroglancer_layer">
<span class="sig-name descname"><span class="pre">_neuroglancer_layer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._neuroglancer_layer" title="Link to this definition"></a></dt>
<dd><p>Returns the neuroglancer layer of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._transform_matrix">
<span class="sig-name descname"><span class="pre">_transform_matrix</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._transform_matrix" title="Link to this definition"></a></dt>
<dd><p>Returns the transform matrix of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._output_dimensions">
<span class="sig-name descname"><span class="pre">_output_dimensions</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._output_dimensions" title="Link to this definition"></a></dt>
<dd><p>Returns the output dimensions of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._source_name">
<span class="sig-name descname"><span class="pre">_source_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray._source_name" title="Link to this definition"></a></dt>
<dd><p>Returns the source name of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.add_metadata">
<span class="sig-name descname"><span class="pre">add_metadata</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metadata</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.add_metadata" title="Link to this definition"></a></dt>
<dd><p>Adds metadata to the array.</p>
</dd></dl>

<p class="rubric">Notes</p>
<p>This class is used to create a zarr array.</p>
<dl class="py property">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.mode">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mode</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray.mode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id32">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">attrs</span></span><a class="headerlink" href="#id32" title="Link to this definition"></a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Returns</span> <span class="pre">the</span> <span class="pre">attributes</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">array.</span></span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>attrs</strong> (<em>Any</em>) – The attributes of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The attributes of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">attrs</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the attributes of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id33">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">axes</span></span><a class="headerlink" href="#id33" title="Link to this definition"></a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Returns</span> <span class="pre">the</span> <span class="pre">axes</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">array.</span></span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axes</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – The axes of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The axes of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[str]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">axes</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the axes of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id34">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dims</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#id34" title="Link to this definition"></a></dt>
<dd><p>Returns the dimensions of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dims</strong> (<em>int</em>) – The dimensions of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The dimensions of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dims</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the dimensions of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id35">
<span class="sig-name descname"><span class="pre">voxel_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">funlib.geometry.Coordinate</span></span></span><a class="headerlink" href="#id35" title="Link to this definition"></a></dt>
<dd><p>Returns the voxel size of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>voxel_size</strong> (<em>Coordinate</em>) – The voxel size.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The voxel size of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Coordinate</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">voxel_size</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the voxel size of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id36">
<span class="sig-name descname"><span class="pre">roi</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">funlib.geometry.Roi</span></span></span><a class="headerlink" href="#id36" title="Link to this definition"></a></dt>
<dd><p>Returns the region of interest of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>roi</strong> (<em>Roi</em>) – The region of interest.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The region of interest of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Roi</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">roi</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the region of interest of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id37">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">writable</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#id37" title="Link to this definition"></a></dt>
<dd><p>Returns the boolean value of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>writable</strong> (<em>bool</em>) – The boolean value of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The boolean value of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>bool</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">writable</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the boolean value of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id38">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#id38" title="Link to this definition"></a></dt>
<dd><p>Returns the data type of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dtype</strong> (<em>Any</em>) – The data type of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The data type of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dtype</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the data type of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id39">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">num_channels</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a></em><a class="headerlink" href="#id39" title="Link to this definition"></a></dt>
<dd><p>Returns the number of channels of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>num_channels</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The number of channels of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of channels of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optional[int]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">num_channels</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the number of channels of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id40">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">spatial_axes</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#id40" title="Link to this definition"></a></dt>
<dd><p>Returns the spatial axes of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>spatial_axes</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – The spatial axes of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The spatial axes of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>List[str]</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">spatial_axes</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the spatial axes of the array.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id41">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#id41" title="Link to this definition"></a></dt>
<dd><p>Returns the data of the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data</strong> (<em>Any</em>) – The data of the array.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The data of the array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to return the data of the array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id42">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">create_from_array_identifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roi</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">voxel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'a'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id42" title="Link to this definition"></a></dt>
<dd><p>Create a new ZarrArray given an array identifier. It is assumed that
this array_identifier points to a dataset that does not yet exist.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>array_identifier</strong> (<em>ArrayIdentifier</em>) – The array identifier.</p></li>
<li><p><strong>axes</strong> (<em>List</em><em>[</em><em>str</em><em>]</em>) – The axes of the array.</p></li>
<li><p><strong>roi</strong> (<em>Roi</em>) – The region of interest.</p></li>
<li><p><strong>num_channels</strong> (<em>int</em>) – The number of channels.</p></li>
<li><p><strong>voxel_size</strong> (<em>Coordinate</em>) – The voxel size.</p></li>
<li><p><strong>dtype</strong> (<em>Any</em>) – The data type.</p></li>
<li><p><strong>write_size</strong> (<em>Optional</em><em>[</em><em>Coordinate</em><em>]</em>) – The write size.</p></li>
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the array.</p></li>
<li><p><strong>overwrite</strong> (<em>bool</em>) – The boolean value to overwrite the array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The ZarrArray.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray">ZarrArray</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">create_from_array_identifier</span><span class="p">(</span><span class="n">array_identifier</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="n">roi</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">voxel_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">write_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to create a new ZarrArray given an array identifier.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id43">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">open_from_array_identifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id43" title="Link to this definition"></a></dt>
<dd><p>Opens a new ZarrArray given an array identifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>array_identifier</strong> (<em>ArrayIdentifier</em>) – The array identifier.</p></li>
<li><p><strong>name</strong> (<em>str</em>) – The name of the array.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The ZarrArray.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray">ZarrArray</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">open_from_array_identifier</span><span class="p">(</span><span class="n">array_identifier</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to open a new ZarrArray given an array identifier.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id44">
<span class="sig-name descname"><span class="pre">add_metadata</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metadata</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="../../index.html#dacapo.experiments.tasks.OneHotTaskConfig.None" title="dacapo.experiments.tasks.OneHotTaskConfig.None"><span class="pre">None</span></a></span></span><a class="headerlink" href="#id44" title="Link to this definition"></a></dt>
<dd><p>Adds metadata to the array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>metadata</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – The metadata to add to the array.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add_metadata</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>This method is used to add metadata to the array.</p>
</dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.logger">
<span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.logger" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BG">
<span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">BG</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BG" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">BinarySegmentationEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">clip_distance</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol_distance</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator" title="Link to this definition"></a></dt>
<dd><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:
- Dice coefficient: 2 * <a href="#id107"><span class="problematic" id="id108">|A ∩ B|</span></a> / <a href="#id109"><span class="problematic" id="id110">|A|</span></a> + <a href="#id111"><span class="problematic" id="id112">|B|</span></a> ; where A and B are the binary segmentations
- Jaccard coefficient: <a href="#id113"><span class="problematic" id="id114">|A ∩ B|</span></a> / <a href="#id115"><span class="problematic" id="id116">|A ∪ B|</span></a> ; where A and B are the binary segmentations
- Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
- False negative rate: <a href="#id117"><span class="problematic" id="id118">|A - B|</span></a> / <a href="#id119"><span class="problematic" id="id120">|A|</span></a> ; where A and B are the binary segmentations
- False positive rate: <a href="#id121"><span class="problematic" id="id122">|B - A|</span></a> / <a href="#id123"><span class="problematic" id="id124">|B|</span></a> ; where A and B are the binary segmentations
- False discovery rate: <a href="#id125"><span class="problematic" id="id126">|B - A|</span></a> / <a href="#id127"><span class="problematic" id="id128">|A|</span></a> ; where A and B are the binary segmentations
- VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
- Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
- Mean false negative distance: mean distance of false negatives
- Mean false positive distance: mean distance of false positives
- Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
- Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
- Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
- Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
- Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
- F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
- Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
- Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
- F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.clip_distance">
<span class="sig-name descname"><span class="pre">clip_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.clip_distance" title="Link to this definition"></a></dt>
<dd><p>float
the clip distance</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.tol_distance">
<span class="sig-name descname"><span class="pre">tol_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.tol_distance" title="Link to this definition"></a></dt>
<dd><p>float
the tolerance distance</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.channels">
<span class="sig-name descname"><span class="pre">channels</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.channels" title="Link to this definition"></a></dt>
<dd><p>List[str]
the channels</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.criteria">
<span class="sig-name descname"><span class="pre">criteria</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.criteria" title="Link to this definition"></a></dt>
<dd><p>List[str]
the evaluation criteria</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator.score" title="Link to this definition"></a></dt>
<dd><p>Return the evaluation scores.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The BinarySegmentationEvaluator class is used to evaluate the performance of a binary segmentation task.
The class provides methods to evaluate the output array against the evaluation array and return the evaluation scores.
All evaluation scores should inherit from this class.</p>
<p>Clip distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a false positive.
Tolerance distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a true positive.
Channels are the channels of the binary segmentation.
Criteria are the evaluation criteria.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="id45">
<span class="sig-name descname"><span class="pre">criteria</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['jaccard',</span> <span class="pre">'voi']</span></em><a class="headerlink" href="#id45" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id46">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_array_identifier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">evaluation_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id46" title="Link to this definition"></a></dt>
<dd><p>Evaluate the output array against the evaluation array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output_array_identifier</strong> – str
the identifier of the output array</p></li>
<li><p><strong>evaluation_array</strong> – ZarrArray
the evaluation array</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the output array identifier is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">(</span><span class="n">clip_distance</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tol_distance</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="s2">&quot;channel2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_array_identifier</span> <span class="o">=</span> <span class="s2">&quot;output_array&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluation_array</span> <span class="o">=</span> <span class="n">ZarrArray</span><span class="o">.</span><span class="n">open_from_array_identifier</span><span class="p">(</span><span class="s2">&quot;evaluation_array&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">output_array_identifier</span><span class="p">,</span> <span class="n">evaluation_array</span><span class="p">)</span>
<span class="go">BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to evaluate the output array against the evaluation array.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id47">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score</span></span><a class="headerlink" href="#id47" title="Link to this definition"></a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">Return</span> <span class="pre">the</span> <span class="pre">evaluation</span> <span class="pre">scores.</span></span></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores</dt><dd><p>the evaluation scores</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – if the function is not implemented</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">(</span><span class="n">clip_distance</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">tol_distance</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;channel1&quot;</span><span class="p">,</span> <span class="s2">&quot;channel2&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">binary_segmentation_evaluator</span><span class="o">.</span><span class="n">score</span>
<span class="go">BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the evaluation scores.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">ArrayEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth_binary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_binary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truth_empty</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_empty</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resolution</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator" title="Link to this definition"></a></dt>
<dd><p>Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:
- Dice coefficient: 2 * <a href="#id129"><span class="problematic" id="id130">|A ∩ B|</span></a> / <a href="#id131"><span class="problematic" id="id132">|A|</span></a> + <a href="#id133"><span class="problematic" id="id134">|B|</span></a> ; where A and B are the binary segmentations
- Jaccard coefficient: <a href="#id135"><span class="problematic" id="id136">|A ∩ B|</span></a> / <a href="#id137"><span class="problematic" id="id138">|A ∪ B|</span></a> ; where A and B are the binary segmentations
- Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
- False negative rate: <a href="#id139"><span class="problematic" id="id140">|A - B|</span></a> / <a href="#id141"><span class="problematic" id="id142">|A|</span></a> ; where A and B are the binary segmentations
- False positive rate: <a href="#id143"><span class="problematic" id="id144">|B - A|</span></a> / <a href="#id145"><span class="problematic" id="id146">|B|</span></a> ; where A and B are the binary segmentations
- False discovery rate: <a href="#id147"><span class="problematic" id="id148">|B - A|</span></a> / <a href="#id149"><span class="problematic" id="id150">|A|</span></a> ; where A and B are the binary segmentations
- VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
- Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
- Mean false negative distance: mean distance of false negatives
- Mean false positive distance: mean distance of false positives
- Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
- Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
- Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
- Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
- Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
- F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
- Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
- Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
- F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth">
<span class="sig-name descname"><span class="pre">truth</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth" title="Link to this definition"></a></dt>
<dd><p>np.ndarray
the truth binary segmentation</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test">
<span class="sig-name descname"><span class="pre">test</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test" title="Link to this definition"></a></dt>
<dd><p>np.ndarray
the test binary segmentation</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth_empty">
<span class="sig-name descname"><span class="pre">truth_empty</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth_empty" title="Link to this definition"></a></dt>
<dd><p>bool
whether the truth binary segmentation is empty</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test_empty">
<span class="sig-name descname"><span class="pre">test_empty</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test_empty" title="Link to this definition"></a></dt>
<dd><p>bool
whether the test binary segmentation is empty</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.cremieval">
<span class="sig-name descname"><span class="pre">cremieval</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.cremieval" title="Link to this definition"></a></dt>
<dd><p>CremiEvaluator
the cremi evaluator</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.resolution">
<span class="sig-name descname"><span class="pre">resolution</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.resolution" title="Link to this definition"></a></dt>
<dd><p>Tuple[float, float, float]
the resolution</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.dice">
<span class="sig-name descname"><span class="pre">dice</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.dice" title="Link to this definition"></a></dt>
<dd><p>Return the Dice coefficient.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.jaccard">
<span class="sig-name descname"><span class="pre">jaccard</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.jaccard" title="Link to this definition"></a></dt>
<dd><p>Return the Jaccard coefficient.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.hausdorff">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.hausdorff" title="Link to this definition"></a></dt>
<dd><p>Return the Hausdorff distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_negative_rate">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_negative_rate" title="Link to this definition"></a></dt>
<dd><p>Return the false negative rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_positive_rate">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_positive_rate" title="Link to this definition"></a></dt>
<dd><p>Return the false positive rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_discovery_rate">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_discovery_rate" title="Link to this definition"></a></dt>
<dd><p>Return the false discovery rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.precision" title="Link to this definition"></a></dt>
<dd><p>Return the precision.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.recall">
<span class="sig-name descname"><span class="pre">recall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.recall" title="Link to this definition"></a></dt>
<dd><p>Return the recall.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.f1_score">
<span class="sig-name descname"><span class="pre">f1_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.f1_score" title="Link to this definition"></a></dt>
<dd><p>Return the F1 score.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.voi">
<span class="sig-name descname"><span class="pre">voi</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.voi" title="Link to this definition"></a></dt>
<dd><p>Return the VOI.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_distance">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_negative_distance">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_negative_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false negative distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_positive_distance">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_positive_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false positive distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false distance clipped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_negative_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_negative_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false negative distance clipped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_positive_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.mean_false_positive_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false positive distance clipped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_positive_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_positive_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false positive rate with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_negative_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.false_negative_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false negative rate with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.precision_with_tolerance">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.precision_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the precision with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.recall_with_tolerance">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.recall_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the recall with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.f1_score_with_tolerance">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.f1_score_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the F1 score with tolerance.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ArrayEvaluator class is used to evaluate the performance of a binary segmentation task.
The class provides methods to evaluate the truth binary segmentation against the test binary segmentation.
All evaluation scores should inherit from this class.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth_itk">
<span class="sig-name descname"><span class="pre">truth_itk</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.truth_itk" title="Link to this definition"></a></dt>
<dd><p>A SimpleITK image of the truth binary segmentation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>sitk.Image</dt><dd><p>the truth binary segmentation as a SimpleITK image</p>
</dd>
</dl>
</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">truth_itk</span>
<span class="go">&lt;SimpleITK.SimpleITK.Image; proxy of &lt;Swig Object of type &#39;std::vector&lt; itk::simple::Image &gt;::value_type *&#39; at 0x7f8b1c0b3f30&gt; &gt;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the truth binary segmentation as a SimpleITK image.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test_itk">
<span class="sig-name descname"><span class="pre">test_itk</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.test_itk" title="Link to this definition"></a></dt>
<dd><p>A SimpleITK image of the test binary segmentation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
<li><p><strong>resolution</strong> – Tuple[float, float, float]
the resolution</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>sitk.Image</dt><dd><p>the test binary segmentation as a SimpleITK image</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the test binary segmentation is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">test_itk</span>
<span class="go">&lt;SimpleITK.SimpleITK.Image; proxy of &lt;Swig Object of type &#39;std::vector&lt; itk::simple::Image &gt;::value_type *&#39; at 0x7f8b1c0b3f30&gt; &gt;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the test binary segmentation as a SimpleITK image.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.overlap_measures_filter">
<span class="sig-name descname"><span class="pre">overlap_measures_filter</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator.overlap_measures_filter" title="Link to this definition"></a></dt>
<dd><p>A SimpleITK filter to compute overlap measures.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth_itk</strong> – sitk.Image
the truth binary segmentation as a SimpleITK image</p></li>
<li><p><strong>test_itk</strong> – sitk.Image
the test binary segmentation as a SimpleITK image</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>sitk.LabelOverlapMeasuresImageFilter</dt><dd><p>the overlap measures filter</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the truth binary segmentation or the test binary segmentation is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">overlap_measures_filter</span>
<span class="go">&lt;SimpleITK.SimpleITK.LabelOverlapMeasuresImageFilter; proxy of &lt;Swig Object of type &#39;itk::simple::LabelOverlapMeasuresImageFilter *&#39; at 0x7f8b1c0b3f30&gt; &gt;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the overlap measures filter.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id48">
<span class="sig-name descname"><span class="pre">dice</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id48" title="Link to this definition"></a></dt>
<dd><p>The Dice coefficient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth_itk</strong> – sitk.Image
the truth binary segmentation as a SimpleITK image</p></li>
<li><p><strong>test_itk</strong> – sitk.Image
the test binary segmentation as a SimpleITK image</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>float</dt><dd><p>the Dice coefficient</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the truth binary segmentation or the test binary segmentation is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">dice</span><span class="p">()</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the Dice coefficient.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id49">
<span class="sig-name descname"><span class="pre">jaccard</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id49" title="Link to this definition"></a></dt>
<dd><p>The Jaccard coefficient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth_itk</strong> – sitk.Image
the truth binary segmentation as a SimpleITK image</p></li>
<li><p><strong>test_itk</strong> – sitk.Image
the test binary segmentation as a SimpleITK image</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>float</dt><dd><p>the Jaccard coefficient</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the truth binary segmentation or the test binary segmentation is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">jaccard</span><span class="p">()</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the Jaccard coefficient.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id50">
<span class="sig-name descname"><span class="pre">hausdorff</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id50" title="Link to this definition"></a></dt>
<dd><p>The Hausdorff distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None</strong></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>the Hausdorff distance</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>None</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">hausdorff</span><span class="p">()</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the Hausdorff distance between the truth binary segmentation and the test binary segmentation.</p>
<p>If either the truth or test binary segmentation is empty, the function returns 0.
Otherwise, it calculates the Hausdorff distance using the HausdorffDistanceImageFilter from the SimpleITK library.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id51">
<span class="sig-name descname"><span class="pre">false_negative_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id51" title="Link to this definition"></a></dt>
<dd><p>The false negative rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><dl class="simple">
<dt>float</dt><dd><p>the false negative rate</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>if the truth binary segmentation or the test binary segmentation is not valid</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ValueError</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">false_negative_rate</span><span class="p">()</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the false negative rate.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id52">
<span class="sig-name descname"><span class="pre">false_positive_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id52" title="Link to this definition"></a></dt>
<dd><p>The false positive rate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth_itk</strong> – sitk.Image
the truth binary segmentation as a SimpleITK image</p></li>
<li><p><strong>test_itk</strong> – sitk.Image
the test binary segmentation as a SimpleITK image</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>float</dt><dd><p>the false positive rate</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – if the truth binary segmentation or the test binary segmentation is not valid</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span> <span class="o">=</span> <span class="n">ArrayEvaluator</span><span class="p">(</span><span class="n">truth_binary</span><span class="p">,</span> <span class="n">test_binary</span><span class="p">,</span> <span class="n">truth_empty</span><span class="p">,</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">metric_params</span><span class="p">,</span> <span class="n">resolution</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">array_evaluator</span><span class="o">.</span><span class="n">false_positive_rate</span><span class="p">()</span>
<span class="go">0.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function is used to return the false positive rate.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id53">
<span class="sig-name descname"><span class="pre">false_discovery_rate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id53" title="Link to this definition"></a></dt>
<dd><p>Calculate the false discovery rate (FDR) for the binary segmentation evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The false discovery rate.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_discovery_rate</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The false discovery rate is a measure of the proportion of false positives among the predicted positive samples.
It is calculated as the ratio of false positives to the sum of true positives and false positives.
If either the ground truth or the predicted segmentation is empty, the FDR is set to NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id54">
<span class="sig-name descname"><span class="pre">precision</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id54" title="Link to this definition"></a></dt>
<dd><p>Calculate the precision of the binary segmentation evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The precision value.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None.</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">precision</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Precision is a measure of the accuracy of the positive predictions made by the model.
It is calculated as the ratio of true positives to the total number of positive predictions.
If either the ground truth or the predicted values are empty, the precision value will be NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id55">
<span class="sig-name descname"><span class="pre">recall</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id55" title="Link to this definition"></a></dt>
<dd><p>Calculate the recall metric for binary segmentation evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recall value.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">recall</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall is a measure of the ability of a binary classifier to identify all positive samples.
It is calculated as the ratio of true positives to the total number of actual positives.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id56">
<span class="sig-name descname"><span class="pre">f1_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id56" title="Link to this definition"></a></dt>
<dd><p>Calculate the F1 score for binary segmentation evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The F1 score value.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>None.</strong> – </p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">f1_score</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The F1 score is the harmonic mean of precision and recall.
It is a measure of the balance between precision and recall, providing a single metric to evaluate the model’s performance.</p>
<p>If either the ground truth or the predicted values are empty, the F1 score will be NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id57">
<span class="sig-name descname"><span class="pre">voi</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id57" title="Link to this definition"></a></dt>
<dd><p>Calculate the Variation of Information (VOI) for binary segmentation evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The VOI value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">voi</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The VOI is a measure of the similarity between two segmentations.
It combines the split and merge errors into a single measure of segmentation quality.
If either the ground truth or the predicted values are empty, the VOI will be NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id58">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id58" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false distance between the ground truth and the test results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_distance</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>This method returns np.nan if either the ground truth or the test results are empty.</p></li>
<li><p>The mean false distance is a measure of the average distance between the false positive pixels in the test results and the nearest true positive pixels in the ground truth.</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id59">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id59" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false negative distance between the ground truth and the test results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false negative distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_negative_distance</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method returns np.nan if either the ground truth or the test results are empty.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id60">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id60" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false positive distance.</p>
<p>This method calculates the mean false positive distance between the ground truth and the test results.
If either the ground truth or the test results are empty, the method returns NaN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false positive distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_positive_distance</span><span class="p">()</span>
<span class="go">0.5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mean false positive distance is a measure of the average distance between false positive pixels in the
test results and the corresponding ground truth pixels. It is commonly used to evaluate the performance of
binary segmentation algorithms.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id61">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id61" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false distance (clipped) between the ground truth and the test results.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false distance (clipped) value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_distance_clipped</span><span class="p">()</span>
<span class="go">0.123</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method returns np.nan if either the ground truth or the test results are empty.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id62">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id62" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false negative distance, with clipping.</p>
<p>This method calculates the mean false negative distance between the ground truth and the test results.
The distance is clipped to avoid extreme values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false negative distance with clipping.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_negative_distance_clipped</span><span class="p">()</span>
<span class="go">0.123</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The mean false negative distance is a measure of the average distance between the false negative pixels in the ground truth and the test results.</p></li>
<li><p>Clipping the distance helps to avoid extreme values that may skew the overall evaluation.</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id63">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id63" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false positive distance, with clipping.</p>
<p>This method calculates the mean false positive distance between the ground truth and the test results,
taking into account any clipping that may have been applied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false positive distance with clipping.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_positive_distance_clipped</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The mean false positive distance is a measure of the average distance between false positive pixels
in the test results and the corresponding ground truth pixels.</p></li>
<li><p>If either the ground truth or the test results are empty, the method returns NaN.</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id64">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id64" title="Link to this definition"></a></dt>
<dd><p>Calculate the false positive rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The false positive rate with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_rate_with_tolerance</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the false positive rate with tolerance by comparing the truth and test data.
If either the truth or test data is empty, it returns NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id65">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id65" title="Link to this definition"></a></dt>
<dd><p>Calculate the false negative rate with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The false negative rate with tolerance as a floating-point number.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_rate_with_tolerance</span><span class="p">()</span>
<span class="go">0.25</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the false negative rate with tolerance, which is a measure of the proportion of false negatives in a binary segmentation evaluation. If either the ground truth or the test data is empty, the method returns NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id66">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id66" title="Link to this definition"></a></dt>
<dd><p>Calculate the precision with tolerance.</p>
<p>This method calculates the precision with tolerance by comparing the truth and test data.
Precision is the ratio of true positives to the sum of true positives and false positives.
Tolerance is a distance threshold within which two pixels are considered to be a match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The precision with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">precision_with_tolerance</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Precision is a measure of the accuracy of the positive predictions.</p></li>
<li><p>If either the ground truth or the test data is empty, the method returns NaN.</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id67">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id67" title="Link to this definition"></a></dt>
<dd><p>Calculate the recall with tolerance for the binary segmentation evaluator.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The recall with tolerance value.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">recall_with_tolerance</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the recall with tolerance, which is a measure of how well the binary segmentation evaluator performs. It returns the recall with tolerance value as a float. If either the truth or test data is empty, it returns NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id68">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id68" title="Link to this definition"></a></dt>
<dd><p>Calculate the F1 score with tolerance.</p>
<p>This method calculates the F1 score with tolerance between the ground truth and the test results.
If either the ground truth or the test results are empty, the function returns NaN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p></li>
<li><p><strong>test</strong> – np.ndarray
the test binary segmentation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The F1 score with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">f1_score_with_tolerance</span><span class="p">()</span>
<span class="go">0.85</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The F1 score is a measure of a test’s accuracy. It considers both the precision and recall of the test to compute the score.
The tolerance parameter allows for a certain degree of variation between the ground truth and the test results.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.</span></span><span class="sig-name descname"><span class="pre">CremiEvaluator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">truth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1,</span> <span class="pre">1,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol_distance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator" title="Link to this definition"></a></dt>
<dd><p>Evaluate the performance of a binary segmentation task using the CREMI score.
The CREMI score is a measure of the similarity between two binary segmentations.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth">
<span class="sig-name descname"><span class="pre">truth</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth" title="Link to this definition"></a></dt>
<dd><p>np.ndarray
the truth binary segmentation</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test">
<span class="sig-name descname"><span class="pre">test</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test" title="Link to this definition"></a></dt>
<dd><p>np.ndarray
the test binary segmentation</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.sampling">
<span class="sig-name descname"><span class="pre">sampling</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.sampling" title="Link to this definition"></a></dt>
<dd><p>Tuple[float, float, float]
the sampling resolution</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.clip_distance">
<span class="sig-name descname"><span class="pre">clip_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.clip_distance" title="Link to this definition"></a></dt>
<dd><p>float
the maximum distance to clip</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.tol_distance">
<span class="sig-name descname"><span class="pre">tol_distance</span></span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.tol_distance" title="Link to this definition"></a></dt>
<dd><p>float
the tolerance distance</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positive_distances">
<span class="sig-name descname"><span class="pre">false_positive_distances</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positive_distances" title="Link to this definition"></a></dt>
<dd><p>Return the false positive distances.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positives_with_tolerance">
<span class="sig-name descname"><span class="pre">false_positives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positives_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false positives with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positive_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_positive_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false positive rate with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negatives_with_tolerance">
<span class="sig-name descname"><span class="pre">false_negatives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negatives_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false negatives with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negative_rate_with_tolerance">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negative_rate_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the false negative rate with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.true_positives_with_tolerance">
<span class="sig-name descname"><span class="pre">true_positives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.true_positives_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the true positives with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.precision_with_tolerance">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.precision_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the precision with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.recall_with_tolerance">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.recall_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the recall with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.f1_score_with_tolerance">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.f1_score_with_tolerance" title="Link to this definition"></a></dt>
<dd><p>Return the F1 score with tolerance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_positive_distances_clipped">
<span class="sig-name descname"><span class="pre">mean_false_positive_distances_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_positive_distances_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false positive distances clipped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_negative_distances_clipped">
<span class="sig-name descname"><span class="pre">mean_false_negative_distances_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_negative_distances_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false negative distances clipped.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_positive_distance">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_positive_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false positive distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negative_distances">
<span class="sig-name descname"><span class="pre">false_negative_distances</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.false_negative_distances" title="Link to this definition"></a></dt>
<dd><p>Return the false negative distances.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_negative_distance">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_negative_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false negative distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_distance">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_distance" title="Link to this definition"></a></dt>
<dd><p>Return the mean false distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_distance_clipped">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.mean_false_distance_clipped" title="Link to this definition"></a></dt>
<dd><p>Return the mean false distance clipped.</p>
</dd></dl>

<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The CremiEvaluator class is used to evaluate the performance of a binary segmentation task using the CREMI score.</p></li>
<li><p>True and test binary segmentations are compared to calculate various evaluation metrics.</p></li>
<li><p>The class provides methods to evaluate the performance of the binary segmentation task.</p></li>
<li><p>Toleration distance is used to determine the tolerance level for the evaluation.</p></li>
<li><p>Clip distance is used to clip the distance values to avoid extreme values.</p></li>
<li><p>All evaluation scores should inherit from this class.</p></li>
</ul>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test_mask">
<span class="sig-name descname"><span class="pre">test_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test_mask" title="Link to this definition"></a></dt>
<dd><p>Generate a binary mask for the test data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>test</strong> – np.ndarray
the test binary segmentation</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A binary mask indicating the regions of interest in the test data.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>test_mask (ndarray)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">test_mask</span><span class="p">()</span>
<span class="go">array([[False,  True, False],</span>
<span class="go">        [ True,  True,  True],</span>
<span class="go">        [False,  True, False]])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method assumes that the background class is represented by the constant <cite>BG</cite>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth_mask">
<span class="sig-name descname"><span class="pre">truth_mask</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth_mask" title="Link to this definition"></a></dt>
<dd><p>Returns a binary mask indicating the truth values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>truth</strong> – np.ndarray
the truth binary segmentation</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A binary mask where True indicates the truth values and False indicates other values.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>truth_mask (ndarray)</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">truth_mask</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="go">[[ True  True False]</span>
<span class="go">    [False  True False]</span>
<span class="go">    [ True False False]]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The truth mask is computed by comparing the truth values with a predefined background value (BG).</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test_edt">
<span class="sig-name descname"><span class="pre">test_edt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.test_edt" title="Link to this definition"></a></dt>
<dd><p>Calculate the Euclidean Distance Transform (EDT) of the test mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.test_mask</strong> (<em>ndarray</em>) – The binary test mask.</p></li>
<li><p><strong>self.sampling</strong> (<em>float</em><em> or </em><em>sequence</em><em> of </em><em>floats</em>) – The pixel spacing or sampling along each dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The Euclidean Distance Transform of the test mask.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p># Example 1:
test_mask = np.array([[0, 0, 1],</p>
<blockquote>
<div><p>[1, 1, 1],
[0, 0, 0]])</p>
</div></blockquote>
<p>sampling = 1.0
result = test_edt(test_mask, sampling)
# Output: array([[1.        , 1.        , 0.        ],
#                [0.        , 0.        , 0.        ],
#                [1.        , 1.        , 1.41421356]])</p>
<p># Example 2:
test_mask = np.array([[0, 1, 0],</p>
<blockquote>
<div><p>[1, 0, 1],
[0, 1, 0]])</p>
</div></blockquote>
<p>sampling = 0.5
result = test_edt(test_mask, sampling)
# Output: array([[0.5       , 0.        , 0.5       ],
#                [0.        , 0.70710678, 0.        ],
#                [0.5       , 0.        , 0.5       ]])</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Euclidean Distance Transform (EDT) calculates the distance from each pixel in the binary mask to the nearest boundary pixel. It is commonly used in image processing and computer vision tasks, such as edge detection and shape analysis.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth_edt">
<span class="sig-name descname"><span class="pre">truth_edt</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator.truth_edt" title="Link to this definition"></a></dt>
<dd><p>Calculate the Euclidean Distance Transform (EDT) of the ground truth mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.truth_mask</strong> (<em>ndarray</em>) – The binary ground truth mask.</p></li>
<li><p><strong>self.sampling</strong> (<em>float</em><em> or </em><em>sequence</em><em> of </em><em>floats</em>) – The pixel spacing or sampling along each dimension.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The Euclidean Distance Transform of the ground truth mask.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edt</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">truth_edt</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Euclidean Distance Transform (EDT) calculates the distance from each pixel in the binary mask to the nearest boundary pixel. It is commonly used in image processing and computer vision tasks.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id69">
<span class="sig-name descname"><span class="pre">false_positive_distances</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id69" title="Link to this definition"></a></dt>
<dd><p>Calculate the distances of false positive pixels from the ground truth segmentation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.test_mask</strong> (<em>ndarray</em>) – The binary test mask.</p></li>
<li><p><strong>self.truth_edt</strong> (<em>ndarray</em>) – The Euclidean Distance Transform of the ground truth segmentation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An array containing the distances of false positive pixels from the ground truth segmentation.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_distances</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
<span class="go">[1.2, 0.8, 2.5, 1.0]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method assumes that the ground truth segmentation and the test mask have been initialized.
The ground truth segmentation is stored in the <cite>truth_edt</cite> attribute, and the test mask is obtained by inverting the <cite>test_mask</cite> attribute.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id70">
<span class="sig-name descname"><span class="pre">false_positives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id70" title="Link to this definition"></a></dt>
<dd><p>Calculate the number of false positives with a given tolerance distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_positive_distances</strong> (<em>ndarray</em>) – The distances of false positive pixels from the ground truth segmentation.</p></li>
<li><p><strong>self.tol_distance</strong> (<em>float</em>) – The tolerance distance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of false positives with a distance greater than the tolerance distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">tol_distance</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">false_positives</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">false_positives_with_tolerance</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">false_positives</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <cite>false_positive_distances</cite> attribute should be initialized before calling this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id71">
<span class="sig-name descname"><span class="pre">false_positive_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id71" title="Link to this definition"></a></dt>
<dd><p>Calculate the false positive rate with tolerance.</p>
<p>This method calculates the false positive rate by dividing the number of false positives with tolerance
by the number of condition negatives.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_positives_with_tolerance</strong> (<em>int</em>) – The number of false positives with tolerance.</p></li>
<li><p><strong>self.truth_mask</strong> (<em>ndarray</em>) – The binary ground truth mask.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The false positive rate with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positives_with_tolerance</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">truth_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_rate_with_tolerance</span><span class="p">()</span>
<span class="go">0.5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The false positive rate with tolerance is a measure of the proportion of false positive predictions
with respect to the total number of condition negatives. It is commonly used in binary segmentation tasks.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id72">
<span class="sig-name descname"><span class="pre">false_negatives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id72" title="Link to this definition"></a></dt>
<dd><p>Calculate the number of false negatives with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_negative_distances</strong> (<em>ndarray</em>) – The distances of false negative pixels from the ground truth segmentation.</p></li>
<li><p><strong>self.tol_distance</strong> (<em>float</em>) – The tolerance distance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of false negatives with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">tol_distance</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">false_negatives</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">false_negatives_with_tolerance</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">false_negatives</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>False negatives are cases where the model incorrectly predicts the absence of a positive class.
The tolerance distance is used to determine whether a false negative is within an acceptable range.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id73">
<span class="sig-name descname"><span class="pre">false_negative_rate_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id73" title="Link to this definition"></a></dt>
<dd><p>Calculate the false negative rate with tolerance.</p>
<p>This method calculates the false negative rate by dividing the number of false negatives
with tolerance by the number of condition positives.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_negatives_with_tolerance</strong> (<em>int</em>) – The number of false negatives with tolerance.</p></li>
<li><p><strong>self.false_negative_distances</strong> (<em>ndarray</em>) – The distances of false negative pixels from the ground truth segmentation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The false negative rate with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negatives_with_tolerance</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_rate_with_tolerance</span><span class="p">()</span>
<span class="go">0.6666666666666666</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The false negative rate with tolerance is a measure of the proportion of condition positives
that are incorrectly classified as negatives, considering a certain tolerance level.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id74">
<span class="sig-name descname"><span class="pre">true_positives_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id74" title="Link to this definition"></a></dt>
<dd><p>Calculate the number of true positives with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.test_mask</strong> (<em>ndarray</em>) – The test binary segmentation mask.</p></li>
<li><p><strong>self.truth_mask</strong> (<em>ndarray</em>) – The ground truth binary segmentation mask.</p></li>
<li><p><strong>self.false_negatives_with_tolerance</strong> (<em>int</em>) – The number of false negatives with tolerance.</p></li>
<li><p><strong>self.false_positives_with_tolerance</strong> (<em>int</em>) – The number of false positives with tolerance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The number of true positives with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">test_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">truth_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negatives_with_tolerance</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positives_with_tolerance</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">true_positives</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">true_positives_with_tolerance</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">true_positives</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>True positives are cases where the model correctly predicts the presence of a positive class.
The tolerance distance is used to determine whether a true positive is within an acceptable range.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id75">
<span class="sig-name descname"><span class="pre">precision_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id75" title="Link to this definition"></a></dt>
<dd><p>Calculate the precision with tolerance.</p>
<p>This method calculates the precision with tolerance by dividing the number of true positives
with tolerance by the sum of true positives with tolerance and false positives with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.true_positives_with_tolerance</strong> (<em>int</em>) – The number of true positives with tolerance.</p></li>
<li><p><strong>self.false_positives_with_tolerance</strong> (<em>int</em>) – The number of false positives with tolerance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The precision with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ZeroDivisionError</strong> – If the sum of true positives with tolerance and false positives with tolerance is zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">true_positives_with_tolerance</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positives_with_tolerance</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">precision_with_tolerance</span><span class="p">()</span>
<span class="go">0.6666666666666666</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The precision with tolerance is a measure of the proportion of true positives with tolerance
out of the total number of predicted positives with tolerance.
It indicates how well the binary segmentation evaluator performs in terms of correctly identifying positive samples.
If the sum of true positives with tolerance and false positives with tolerance is zero, the precision with tolerance is undefined and a ZeroDivisionError is raised.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id76">
<span class="sig-name descname"><span class="pre">recall_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id76" title="Link to this definition"></a></dt>
<dd><p>A measure of the ability of a binary classifier to identify all positive samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.true_positives_with_tolerance</strong> (<em>int</em>) – The number of true positives with tolerance.</p></li>
<li><p><strong>self.false_negatives_with_tolerance</strong> (<em>int</em>) – The number of false negatives with tolerance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The recall with tolerance value.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ZeroDivisionError</strong> – If the sum of true positives with tolerance and false negatives with tolerance is zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">recall_with_tolerance</span><span class="p">()</span>
<span class="go">0.75</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the recall with tolerance, which is a measure of how well the binary segmentation evaluator performs. It returns the recall with tolerance value as a float. If either the truth or test data is empty, it returns NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id77">
<span class="sig-name descname"><span class="pre">f1_score_with_tolerance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id77" title="Link to this definition"></a></dt>
<dd><p>Calculate the F1 score with tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.recall_with_tolerance</strong> (<em>float</em>) – The recall with tolerance value.</p></li>
<li><p><strong>self.precision_with_tolerance</strong> (<em>float</em>) – The precision with tolerance value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The F1 score with tolerance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ZeroDivisionError</strong> – If both the recall with tolerance and precision with tolerance are zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">recall_with_tolerance</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">precision_with_tolerance</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">f1_score_with_tolerance</span><span class="p">()</span>
<span class="go">0.8571428571428571</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The F1 score is a measure of a test’s accuracy. It considers both the precision and recall of the test to compute the score.
The F1 score with tolerance is calculated using the formula:
F1 = 2 * (recall_with_tolerance * precision_with_tolerance) / (recall_with_tolerance + precision_with_tolerance)
If both recall_with_tolerance and precision_with_tolerance are 0, the F1 score with tolerance will be NaN.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id78">
<span class="sig-name descname"><span class="pre">mean_false_positive_distances_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id78" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean of the false positive distances, clipped to a maximum distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_positive_distances</strong> (<em>ndarray</em>) – The distances of false positive pixels from the ground truth segmentation.</p></li>
<li><p><strong>self.clip_distance</strong> (<em>float</em>) – The maximum distance to clip.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean of the false positive distances, clipped to a maximum distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the clip distance is not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">clip_distance</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_positive_distances_clipped</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the mean of the false positive distances, where the distances are clipped to a maximum distance. The <cite>false_positive_distances</cite> attribute should be set before calling this method. The <cite>clip_distance</cite> attribute determines the maximum distance to which the distances are clipped.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id79">
<span class="sig-name descname"><span class="pre">mean_false_negative_distances_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id79" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean of the false negative distances, clipped to a maximum distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.false_negative_distances</strong> (<em>ndarray</em>) – The distances of false negative pixels from the ground truth segmentation.</p></li>
<li><p><strong>self.clip_distance</strong> (<em>float</em>) – The maximum distance to clip.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean of the false negative distances, clipped to a maximum distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the clip distance is not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">clip_distance</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_negative_distances_clipped</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method calculates the mean of the false negative distances, where the distances are clipped to a maximum distance. The <cite>false_negative_distances</cite> attribute should be set before calling this method. The <cite>clip_distance</cite> attribute determines the maximum distance to which the distances are clipped.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id80">
<span class="sig-name descname"><span class="pre">mean_false_positive_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id80" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false positive distance.</p>
<p>This method calculates the mean distance between the false positive points and the ground truth points.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self.false_positive_distances</strong> (<em>ndarray</em>) – The distances of false positive pixels from the ground truth mask.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false positive distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the false positive distances are not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_positive_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_positive_distance</span><span class="p">()</span>
<span class="go">2.2333333333333334</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The false positive distances should be set before calling this method using the <cite>false_positive_distances</cite> attribute.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id81">
<span class="sig-name descname"><span class="pre">false_negative_distances</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id81" title="Link to this definition"></a></dt>
<dd><p>Calculate the distances of false negative pixels from the ground truth mask.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self.truth_mask</strong> (<em>ndarray</em>) – The binary ground truth mask.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>An array containing the distances of false negative pixels.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the truth mask is not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_distances</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
<span class="go">[0.5, 1.0, 1.5, 2.0]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method assumes that the ground truth mask and the test mask have already been set.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id82">
<span class="sig-name descname"><span class="pre">mean_false_negative_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id82" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false negative distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self.false_negative_distances</strong> (<em>ndarray</em>) – The distances of false negative pixels from the ground truth mask.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The mean false negative distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the false negative distances are not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">false_negative_distances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_negative_distance</span><span class="p">()</span>
<span class="go">2.2333333333333334</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mean false negative distance is calculated as the average of all false negative distances.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id83">
<span class="sig-name descname"><span class="pre">mean_false_distance</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id83" title="Link to this definition"></a></dt>
<dd><p>Calculate the mean false distance.</p>
<p>This method calculates the mean false distance by taking the average of the mean false positive distance
and the mean false negative distance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.mean_false_positive_distance</strong> (<em>float</em>) – The mean false positive distance.</p></li>
<li><p><strong>self.mean_false_negative_distance</strong> (<em>float</em>) – The mean false negative distance.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The calculated mean false distance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the mean false positive distance or the mean false negative distance is not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_distance</span><span class="p">()</span>
<span class="go">5.0</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mean false distance is a metric used to evaluate the performance of a binary segmentation model.
It provides a measure of the average distance between false positive and false negative predictions.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id84">
<span class="sig-name descname"><span class="pre">mean_false_distance_clipped</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#id84" title="Link to this definition"></a></dt>
<dd><p>Calculates the mean false distance clipped.</p>
<p>This method calculates the mean false distance clipped by taking the average of the mean false positive distances
clipped and the mean false negative distances clipped.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self.mean_false_positive_distances_clipped</strong> (<em>float</em>) – The mean false positive distances clipped.</p></li>
<li><p><strong>self.mean_false_negative_distances_clipped</strong> (<em>float</em>) – The mean false negative distances clipped.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The calculated mean false distance clipped.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – If the mean false positive distances clipped or the mean false negative distances clipped are not set.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinarySegmentationEvaluator</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">evaluator</span><span class="o">.</span><span class="n">mean_false_distance_clipped</span><span class="p">()</span>
<span class="go">2.5</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mean false distance clipped is calculated as 0.5 * (mean_false_positive_distances_clipped +
mean_false_negative_distances_clipped).</p>
</div>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../binary_segmentation_evaluation_scores/index.html" class="btn btn-neutral float-left" title="dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../dummy_evaluation_scores/index.html" class="btn btn-neutral float-right" title="dacapo.experiments.tasks.evaluators.dummy_evaluation_scores" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Caroline Malin-Mayor, Jeff Rhoades, Marwan Zouinkhi, William Patton, David Ackerman, Jan Funke.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>