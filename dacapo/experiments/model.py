from dacapo.experiments.architectures.architecture import ArchitectureConfig

from funlib.geometry import Coordinate

import torch

from typing import Tuple


class Model(torch.nn.Module):
    """
    A trainable DaCapo model. Consists of an ``Architecture`` and a
    prediction head. Models are generated by ``Predictor``s.

    May include an optional eval_activation that is only executed when the model
    is in eval mode. This is particularly useful if you want to train with something
    like BCELossWithLogits, since you want to avoid applying softmax while training,
    but apply it during evaluation.

    Attributes:
        architecture (Architecture): The architecture of the model.
        prediction_head (torch.nn.Module): The prediction head of the model.
        chain (torch.nn.Sequential): The architecture followed by the prediction head.
        num_in_channels (int): The number of input channels.
        input_shape (Coordinate): The shape of the input tensor.
        eval_input_shape (Coordinate): The shape of the input tensor during evaluation.
        num_out_channels (int): The number of output channels.
        output_shape (Coordinate): The shape of the output
        eval_activation (torch.nn.Module | None): The activation function to apply during evaluation.
    Methods:
        forward(x: torch.Tensor) -> torch.Tensor:
            Forward pass of the model.
        compute_output_shape(input_shape: Coordinate) -> Tuple[int, Coordinate]:
            Compute the spatial shape of this model, when fed a tensor of the given spatial shape as input.
        scale(voxel_size: Coordinate) -> Coordinate:
            Scale the model by the given voxel size.
    Note:
        The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.
    """

    num_out_channels: int
    num_in_channels: int

    def __init__(
        self,
        architecture: ArchitectureConfig,
        prediction_head: torch.nn.Module,
        eval_activation: torch.nn.Module | None = None,
    ):
        """
        Initializes a Model object.

        Args:
            architecture (Architecture): The architecture of the model.
            prediction_head (torch.nn.Module): The prediction head of the model.
            eval_activation (torch.nn.Module | None): The activation function to apply during evaluation.
        Raises:
            AssertionError: If the architecture is not an instance of Architecture.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model
            Model object
            >>> model.architecture
            Architecture object
            >>> model.prediction_head
            Prediction head object
            >>> model.chain
            Sequential object
            >>> model.num_in_channels
            1
            >>> model.input_shape
            Coordinate(1, 1, 1)
            >>> model.eval_input_shape
            Coordinate(1, 1, 1)
            >>> model.num_out_channels
            1
            >>> model.output_shape
            Coordinate(1, 1, 1)
            >>> model.eval_activation
            None
        Note:
            The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions. Update the weight initialization to use Kaiming.
            The eval_activation is only applied during evaluation. This is particularly useful if you want to train with something like BCELossWithLogits, since you want to avoid applying softmax while training, but apply it during evaluation.
            To Do: Put this somewhere better, there might be conv layers that aren't follwed by relus.
        """
        super().__init__()

        self.architecture = architecture
        self.prediction_head = prediction_head
        self.chain = torch.nn.Sequential(architecture.module(), prediction_head)
        self.num_in_channels = architecture.num_in_channels

        self.input_shape = architecture.input_shape
        self.eval_input_shape = self.input_shape + architecture.eval_shape_increase
        self.num_out_channels, self.output_shape = self.compute_output_shape(
            self.input_shape
        )
        self.eval_activation = eval_activation

        # UPDATE WEIGHT INITIALIZATION TO USE KAIMING
        # TODO: put this somewhere better, there might be
        # conv layers that aren't follwed by relus?
        for _name, layer in self.named_modules():
            if isinstance(layer, torch.nn.modules.conv._ConvNd):
                torch.nn.init.kaiming_normal_(layer.weight, nonlinearity="relu")

    def get_device(self) -> torch.device:
        """
        Get the device of the model.

        Returns:
            torch.device: The device of the model.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model.get_device()
            torch.device('cuda:0')
        Note:
            This method is useful for checking the device of the model.
        """
        return next(self.parameters()).device

    def forward(self, x):
        """
        Forward pass of the model.

        Args:
            x (torch.Tensor): The input tensor.
        Returns:
            torch.Tensor: The output tensor.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model.forward(x)
            torch.Tensor
        Note:
            The eval_activation is only applied during evaluation. This is particularly useful if you want to train with something like BCELossWithLogits, since you want to avoid applying softmax while training, but apply it during evaluation.

        """
        result = self.chain(x)
        if not self.training and self.eval_activation is not None:
            result = self.eval_activation(result)
        return result

    def compute_output_shape(self, input_shape: Coordinate) -> Tuple[int, Coordinate]:
        """
        Compute the spatial shape (i.e., not accounting for channels and
        batch dimensions) of this model, when fed a tensor of the given spatial
        shape as input.

        Args:
            input_shape (Coordinate): The shape of the input tensor.
        Returns:
            Tuple[int, Coordinate]: The number of output channels and the spatial shape of the output.
        Raises:
            AssertionError: If the input_shape is not a Coordinate.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model.compute_output_shape(input_shape)
            (1, Coordinate(1, 1, 1))
        Note:
            The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.
        """

        return self.__get_output_shape(input_shape, self.num_in_channels)

    def __get_output_shape(
        self, input_shape: Coordinate, in_channels: int
    ) -> Tuple[int, Coordinate]:
        """
        Compute the spatial shape (i.e., not accounting for channels and
        batch dimensions) of this model, when fed a tensor of the given spatial
        shape as input.

        Args:
            input_shape (Coordinate): The shape of the input tensor.
            in_channels (int): The number of input channels.
        Returns:
            Tuple[int, Coordinate]: The number of output channels and the spatial shape of the output.
        Raises:
            AssertionError: If the input_shape is not a Coordinate.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model.__get_output_shape(input_shape, in_channels)
            (1, Coordinate(1, 1, 1))
        Note:
            The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.

        """
        dummy_data = torch.zeros((1, in_channels) + input_shape, device=self.get_device())
        with torch.no_grad():
            out = self.forward(dummy_data)
        return out.shape[1], Coordinate(out.shape[2:])

    def scale(self, voxel_size: Coordinate) -> Coordinate:
        """
        Scale the model by the given voxel size.

        Args:
            voxel_size (Coordinate): The voxel size to scale the model by.
        Returns:
            Coordinate: The scaled model.
        Raises:
            AssertionError: If the voxel_size is not a Coordinate.
        Examples:
            >>> model = Model(architecture, prediction_head)
            >>> model.scale(voxel_size)
            Coordinate(1, 1, 1)
        Note:
            The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.
        """
        return self.architecture.scale(voxel_size)
