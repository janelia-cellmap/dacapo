dacapo.store.file_stats_store
=============================

.. py:module:: dacapo.store.file_stats_store


Attributes
----------

.. autoapisummary::

   dacapo.store.file_stats_store.converter
   dacapo.store.file_stats_store.logger


Classes
-------

.. autoapisummary::

   dacapo.store.file_stats_store.StatsStore
   dacapo.store.file_stats_store.TrainingStats
   dacapo.store.file_stats_store.TrainingIterationStats
   dacapo.store.file_stats_store.ValidationScores
   dacapo.store.file_stats_store.ValidationIterationScores
   dacapo.store.file_stats_store.FileStatsStore


Module Contents
---------------

.. py:class:: StatsStore



   Base class for statistics stores.

   .. method:: store_training_stats(run_name, training_stats)

      Store the training stats of a given run.

   .. method:: retrieve_training_stats(run_name)

      Retrieve the training stats for a given run.

   .. method:: store_validation_iteration_scores(run_name, validation_scores)

      Store the validation iteration scores of a given run.

   .. method:: retrieve_validation_iteration_scores(run_name)

      Retrieve the validation iteration scores for a given run.

   .. method:: delete_training_stats(run_name)

      Delete the training stats associated with a specific run.
      


   .. py:method:: store_training_stats(run_name: str, training_stats: dacapo.experiments.training_stats.TrainingStats)
      :abstractmethod:


      Store training stats of a given run.

      :param run_name: The name of the run.
      :type run_name: str
      :param training_stats: The training stats to store.
      :type training_stats: TrainingStats

      :raises ValueError: If the training stats are already stored.

      .. rubric:: Examples

      >>> store = StatsStore()
      >>> run_name = 'run_0'
      >>> training_stats = TrainingStats()
      >>> store.store_training_stats(run_name, training_stats)



   .. py:method:: retrieve_training_stats(run_name: str) -> dacapo.experiments.training_stats.TrainingStats
      :abstractmethod:


      Retrieve the training stats for a given run.

      :param run_name: The name of the run.
      :type run_name: str

      :returns: The training stats for the given run.
      :rtype: TrainingStats

      :raises ValueError: If the training stats are not available.

      .. rubric:: Examples

      >>> store = StatsStore()
      >>> run_name = 'run_0'
      >>> store.retrieve_training_stats(run_name)



   .. py:method:: store_validation_iteration_scores(run_name: str, validation_scores: dacapo.experiments.validation_scores.ValidationScores)
      :abstractmethod:


      Store the validation iteration scores of a given run.

      :param run_name: The name of the run.
      :type run_name: str
      :param validation_scores: The validation scores to store.
      :type validation_scores: ValidationScores

      :raises ValueError: If the validation iteration scores are already stored.

      .. rubric:: Examples

      >>> store = StatsStore()
      >>> run_name = 'run_0'
      >>> validation_scores = ValidationScores()
      >>> store.store_validation_iteration_scores(run_name, validation_scores)



   .. py:method:: retrieve_validation_iteration_scores(run_name: str) -> List[dacapo.experiments.validation_scores.ValidationIterationScores]
      :abstractmethod:


      Retrieve the validation iteration scores for a given run.

      :param run_name: The name of the run.
      :type run_name: str

      :returns: The validation iteration scores for the given run.
      :rtype: List[ValidationIterationScores]

      :raises ValueError: If the validation iteration scores are not available.

      .. rubric:: Examples

      >>> store = StatsStore()
      >>> run_name = 'run_0'
      >>> store.retrieve_validation_iteration_scores(run_name)



   .. py:method:: delete_training_stats(run_name: str) -> None
      :abstractmethod:


      Deletes the training statistics for a given run.

      :param run_name: The name of the run.
      :type run_name: str

      :raises ValueError: If the training stats are not available.

      .. rubric:: Example

      >>> store = StatsStore()
      >>> run_name = 'run_0'
      >>> store.delete_training_stats(run_name)



.. py:data:: converter

.. py:class:: TrainingStats

   A class used to represent Training Statistics. It contains a list of training
   iteration statistics. It also provides methods to add new iteration stats,
   delete stats after a specified iteration, get the number of iterations trained
   for, and convert the stats to a xarray data array.

   .. attribute:: iteration_stats

      List[TrainingIterationStats]
      an ordered list of training stats.

   .. method:: add_iteration_stats(iteration_stats

      TrainingIterationStats) -> None:
      Add a new set of iterations stats to the existing list of iteration
      stats.

   .. method:: delete_after(iteration

      int) -> None:
      Deletes training stats after a specified iteration number.

   .. method:: trained_until() -> int

      
      Gets the number of iterations that the model has been trained for.

   .. method:: to_xarray() -> xr.DataArray

      
      Converts the iteration statistics to a xarray data array.

   .. note::

      The iteration stats list is structured as follows:
      - The outer list contains the stats for each iteration.
      - The inner list contains the stats for each training iteration.


   .. py:attribute:: iteration_stats
      :type:  List[dacapo.experiments.training_iteration_stats.TrainingIterationStats]


   .. py:method:: add_iteration_stats(iteration_stats: dacapo.experiments.training_iteration_stats.TrainingIterationStats) -> None

      Add a new iteration stats to the current iteration stats.

      :param iteration_stats: a new iteration stats object.
      :type iteration_stats: TrainingIterationStats

      :raises assert: if the new iteration stats do not follow the order of existing iteration stats.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.iteration_stats
      [TrainingIterationStats(iteration=0, loss=0.1),
       TrainingIterationStats(iteration=1, loss=0.2),
       TrainingIterationStats(iteration=2, loss=0.3)]

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: delete_after(iteration: int) -> None

      Deletes training stats after a specified iteration.

      :param iteration: the iteration after which the stats are to be deleted.
      :type iteration: int

      :raises assert: if the iteration number is less than the maximum iteration number.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.delete_after(1)
      >>> training_stats.iteration_stats
      [TrainingIterationStats(iteration=0, loss=0.1)]

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: trained_until() -> int

      The number of iterations trained for (the maximum iteration plus one).
      Returns zero if no iterations trained yet.

      :returns: number of iterations that the model has been trained for.
      :rtype: int

      :raises assert: if the iteration stats list is empty.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.trained_until()
      3

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: to_xarray() -> xarray.DataArray

      Converts the iteration stats to a data array format easily manipulatable.

      :returns: xarray DataArray of iteration losses.
      :rtype: xr.DataArray

      :raises assert: if the iteration stats list is empty.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.to_xarray()
      <xarray.DataArray (iterations: 3)>
      array([0.1, 0.2, 0.3])
      Coordinates:
        * iterations  (iterations) int64 0 1 2

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



.. py:class:: TrainingIterationStats

   A class to represent the training iteration statistics. It contains the loss and time taken for each iteration.

   .. attribute:: iteration

      The iteration that produced these stats.

      :type: int

   .. attribute:: loss

      The loss value of this iteration.

      :type: float

   .. attribute:: time

      The time it took to process this iteration.

      :type: float

   .. note::

      The iteration stats list is structured as follows:
      - The outer list contains the stats for each iteration.
      - The inner list contains the stats for each training iteration.


   .. py:attribute:: iteration
      :type:  int


   .. py:attribute:: loss
      :type:  float


   .. py:attribute:: time
      :type:  float


.. py:class:: ValidationScores

   Class representing the validation scores for a set of parameters and datasets.

   .. attribute:: parameters

      The list of parameters that are being evaluated.

      :type: List[PostProcessorParameters]

   .. attribute:: datasets

      The datasets that will be evaluated at each iteration.

      :type: List[Dataset]

   .. attribute:: evaluation_scores

      The scores that are collected on each iteration per
      `PostProcessorParameters` and `Dataset`.

      :type: EvaluationScores

   .. attribute:: scores

      A list of evaluation scores and their associated
      post-processing parameters.

      :type: List[ValidationIterationScores]

   .. method:: subscores(iteration_scores)

      Create a new ValidationScores object with a subset of the iteration scores.

   .. method:: add_iteration_scores(iteration_scores)

      Add iteration scores to the list of scores.

   .. method:: delete_after(iteration)

      Delete scores after a specified iteration.

   .. method:: validated_until()

      Get the number of iterations validated for (the maximum iteration plus one).

   .. method:: compare(existing_iteration_scores)

      Compare iteration stats provided from elsewhere to scores we have saved locally.

   .. method:: criteria()

      Get the list of evaluation criteria.

   .. method:: parameter_names()

      Get the list of parameter names.

   .. method:: to_xarray()

      Convert the validation scores to an xarray DataArray.

   .. method:: get_best(data, dim)

      Compute the Best scores along dimension "dim" per criterion.

   .. rubric:: Notes

   The `scores` attribute is a list of `ValidationIterationScores` objects, each of which
   contains the scores for a single iteration.


   .. py:attribute:: parameters
      :type:  List[dacapo.experiments.tasks.post_processors.PostProcessorParameters]


   .. py:attribute:: datasets
      :type:  List[dacapo.experiments.datasplits.datasets.Dataset]


   .. py:attribute:: evaluation_scores
      :type:  dacapo.experiments.tasks.evaluators.EvaluationScores


   .. py:attribute:: scores
      :type:  List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]


   .. py:method:: subscores(iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> ValidationScores

      Create a new ValidationScores object with a subset of the iteration scores.

      :param iteration_scores: The iteration scores to include in the new ValidationScores object.

      :returns: A new ValidationScores object with the specified iteration scores.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.subscores([validation_scores.scores[0]])

      .. note::

         This method is used to create a new ValidationScores object with a subset of the
         iteration scores. This is useful when you want to create a new ValidationScores object
         that only contains the scores up to a certain iteration.



   .. py:method:: add_iteration_scores(iteration_scores: dacapo.experiments.validation_iteration_scores.ValidationIterationScores) -> None

      Add iteration scores to the list of scores.

      :param iteration_scores: The iteration scores to add.

      :raises ValueError: If the iteration scores are already in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.add_iteration_scores(validation_scores.scores[0])

      .. note::

         This method is used to add iteration scores to the list of scores. This is useful when
         you want to add scores for a new iteration to the ValidationScores object.



   .. py:method:: delete_after(iteration: int) -> None

      Delete scores after a specified iteration.

      :param iteration: The iteration after which to delete the scores.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.delete_after(0)

      .. note::

         This method is used to delete scores after a specified iteration. This is useful when
         you want to delete scores after a certain iteration.



   .. py:method:: validated_until() -> int

      Get the number of iterations validated for (the maximum iteration plus one).

      :returns: The number of iterations validated for.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.validated_until()

      .. note::

         This method is used to get the number of iterations validated for (the maximum iteration
         plus one). This is useful when you want to know how many iterations have been validated.



   .. py:method:: compare(existing_iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> Tuple[bool, int]

      Compare iteration stats provided from elsewhere to scores we have saved locally.
      Local scores take priority. If local scores are at a lower iteration than the
      existing ones, delete the existing ones and replace with local.
      If local iteration > existing iteration, just update existing scores with the last
      overhanging local scores.

      :param existing_iteration_scores: The existing iteration scores to compare with.

      :returns: A tuple indicating whether the local scores should replace the existing ones
                and the existing iteration number.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.compare([validation_scores.scores[0]])

      .. note::

         This method is used to compare iteration stats provided from elsewhere to scores we have
         saved locally. Local scores take priority. If local scores are at a lower iteration than
         the existing ones, delete the existing ones and replace with local. If local iteration >
         existing iteration, just update existing scores with the last overhanging local scores.



   .. py:property:: criteria
      :type: List[str]

      Get the list of evaluation criteria.

      :returns: The list of evaluation criteria.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.criteria

      .. note::

         This property is used to get the list of evaluation criteria. This is useful when you
         want to know what criteria are being used to evaluate the scores.


   .. py:property:: parameter_names
      :type: List[str]

      Get the list of parameter names.

      :returns: The list of parameter names.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.parameter_names

      .. note::

         This property is used to get the list of parameter names. This is useful when you want
         to know what parameters are being used to evaluate the scores.


   .. py:method:: to_xarray() -> xarray.DataArray

      Convert the validation scores to an xarray DataArray.

      :returns: An xarray DataArray representing the validation scores.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.to_xarray()

      .. note::

         This method is used to convert the validation scores to an xarray DataArray. This is
         useful when you want to work with the validation scores as an xarray DataArray.



   .. py:method:: get_best(data: xarray.DataArray, dim: str) -> Tuple[xarray.DataArray, xarray.DataArray]

      Compute the Best scores along dimension "dim" per criterion.
      Returns both the index associated with the best value, and the
      best value in two separate arrays.

      :param data: The data array to compute the best scores from.
      :param dim: The dimension along which to compute the best scores.

      :returns: A tuple containing the index associated with the best value and the best value
                in two separate arrays.

      :raises ValueError: If the criteria are not in the data array.

      .. rubric:: Examples

      >>> validation_scores.get_best(data, "iterations")

      .. note::

         This method is used to compute the Best scores along dimension "dim" per criterion. It
         returns both the index associated with the best value and the best value in two separate
         arrays. This is useful when you want to know the best scores for a given data array.
         Fix: The method is currently not able to handle the case where the criteria are not in the data array.
         To fix this, we need to add a check to see if the criteria are in the data array and raise an error if they are not.



.. py:class:: ValidationIterationScores

   A class used to represent the validation iteration scores in an organized structure.

   .. attribute:: iteration

      The iteration associated with these validation scores.

      :type: int

   .. attribute:: scores

      A list of scores per dataset, post processor

      :type: List[List[List[float]]]

   .. attribute:: parameters, and evaluation criterion.

      

   .. note::

      The scores list is structured as follows:
      - The outer list contains the scores for each dataset.
      - The middle list contains the scores for each post processor parameter.
      - The inner list contains the scores for each evaluation criterion.


   .. py:attribute:: iteration
      :type:  int


   .. py:attribute:: scores
      :type:  List[List[List[float]]]


.. py:data:: logger

.. py:class:: FileStatsStore(path)



   A File based store for run statistics. Used to store and retrieve training statistics and validation scores.

   The store is organized as follows:
   - A directory for training statistics, with a subdirectory for each run. Each run directory contains a pickled list of TrainingIterationStats objects.
   - A directory for validation scores, with a subdirectory for each run. Each run directory contains a pickled list of ValidationIterationScores objects.

   Attributes:
   - path: The root directory for the store.
   - training_stats: The directory for training statistics.
   - validation_scores: The directory for validation scores.

   Methods:
   - store_training_stats(run_name, stats): Store the training statistics for a run.
   - retrieve_training_stats(run_name): Retrieve the training statistics for a run.
   - store_validation_iteration_scores(run_name, scores): Store the validation scores for a run.
   - retrieve_validation_iteration_scores(run_name): Retrieve the validation scores for a run.
   - delete_training_stats(run_name): Delete the training statistics for a run.

   Note: The store does not support concurrent access. It is intended for use in single-threaded applications.




   .. py:method:: store_training_stats(run_name, stats)

      Stores the training statistics for a specific run.

      :param run_name: The name of the run.
      :type run_name: str
      :param stats: The training statistics to be stored.
      :type stats: Stats

      :raises ValueError: If the run name is invalid.

      .. rubric:: Examples

      >>> store.store_training_stats("run1", stats)

      .. rubric:: Notes

      - If the training statistics for the given run already exist in the database, the method will compare the
        existing statistics with the new statistics and update or overwrite them accordingly.
      - If the new statistics go further than the existing statistics, the method will update the statistics from
        the last stored iteration.
      - If the new statistics are behind the existing statistics, the method will overwrite the existing statistics.



   .. py:method:: retrieve_training_stats(run_name)

      Retrieve the training statistics for a specific run.

      :param run_name: The name of the run for which to retrieve the training statistics.
      :type run_name: str

      :returns: A dictionary containing the training statistics for the specified run.
      :rtype: dict



   .. py:method:: store_validation_iteration_scores(run_name, scores)

      Stores the validation scores for a specific run.

      :param run_name: The name of the run.
      :type run_name: str
      :param scores: The validation scores to be stored.
      :type scores: Scores

      :raises ValueError: If the run name is invalid.

      .. rubric:: Examples

      >>> store.store_validation_iteration_scores("run1", scores)

      .. rubric:: Notes

      - If the validation scores for the given run already exist in the database, the method will compare the
        existing scores with the new scores and update or overwrite them accordingly.
      - If the new scores go further than the existing scores, the method will update the scores from
        the last stored iteration.
      - If the new scores are behind the existing scores, the method will overwrite the existing scores.



   .. py:method:: retrieve_validation_iteration_scores(run_name)

      Retrieve the validation iteration scores for a given run.

      :param run_name: The name of the run for which to retrieve the validation iteration scores.
      :type run_name: str

      :returns: A list of validation iteration scores.
      :rtype: list

      :raises ValueError: If the run name is invalid.

      .. rubric:: Examples

      >>> store.retrieve_validation_iteration_scores("run1")



   .. py:method:: delete_training_stats(run_name: str) -> None

      Deletes the training stats for a specific run.

      :param run_name: The name of the run for which to delete the training stats.
      :type run_name: str

      :raises ValueError: If the run name is invalid.

      .. rubric:: Examples

      >>> store.delete_training_stats("run1")



