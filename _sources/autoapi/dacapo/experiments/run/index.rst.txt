dacapo.experiments.run
======================

.. py:module:: dacapo.experiments.run


Classes
-------

.. autoapisummary::

   dacapo.experiments.run.DataSplit
   dacapo.experiments.run.Task
   dacapo.experiments.run.Architecture
   dacapo.experiments.run.Trainer
   dacapo.experiments.run.TrainingStats
   dacapo.experiments.run.ValidationScores
   dacapo.experiments.run.Start
   dacapo.experiments.run.Model
   dacapo.experiments.run.Run


Module Contents
---------------

.. py:class:: DataSplit



   A class for creating a simple train dataset and no validation dataset. It is derived from `DataSplit` class.
   It is used to split the data into training and validation datasets. The training and validation datasets are
   used to train and validate the model respectively.

   .. attribute:: train

      list
      The list containing training datasets. In this class, it contains only one dataset for training.

   .. attribute:: validate

      list
      The list containing validation datasets. In this class, it is an empty list as no validation dataset is set.

   .. method:: __init__(self, datasplit_config)

      
      The constructor for DummyDataSplit class. It initialises a list with training datasets according to the input configuration.

   .. rubric:: Notes

   This class is used to split the data into training and validation datasets.


   .. py:attribute:: train
      :type:  List[dacapo.experiments.datasplits.datasets.Dataset]


   .. py:attribute:: validate
      :type:  Optional[List[dacapo.experiments.datasplits.datasets.Dataset]]


.. py:class:: Task



   Helper class that provides a standard way to create an ABC using
   inheritance.


   .. py:attribute:: predictor
      :type:  dacapo.experiments.tasks.predictors.Predictor


   .. py:attribute:: loss
      :type:  dacapo.experiments.tasks.losses.Loss


   .. py:attribute:: evaluator
      :type:  dacapo.experiments.tasks.evaluators.Evaluator


   .. py:attribute:: post_processor
      :type:  dacapo.experiments.tasks.post_processors.PostProcessor


   .. py:property:: parameters
      :type: Iterable[dacapo.experiments.tasks.post_processors.PostProcessorParameters]



   .. py:property:: evaluation_scores
      :type: dacapo.experiments.tasks.evaluators.EvaluationScores



   .. py:method:: create_model(architecture)


.. py:class:: Architecture(*args, **kwargs)



   An abstract base class for defining the architecture of a neural network model.
   It is inherited from PyTorch's Module and built-in class `ABC` (Abstract Base Classes).
   Other classes can inherit this class to define their own specific variations of architecture.
   It requires to implement several property methods, and also includes additional methods related to the architecture design.

   .. attribute:: input_shape

      The spatial input shape for the neural network architecture.

      :type: Coordinate

   .. attribute:: eval_shape_increase

      The amount to increase the input shape during prediction.

      :type: Coordinate

   .. attribute:: num_in_channels

      The number of input channels required by the architecture.

      :type: int

   .. attribute:: num_out_channels

      The number of output channels provided by the architecture.

      :type: int

   .. method:: dims

      Returns the number of dimensions of the input shape.

   .. method:: scale

      Scales the input voxel size as required by the architecture.

   .. note:: The class is abstract and requires to implement the abstract methods.


   .. py:property:: input_shape
      :type: funlib.geometry.Coordinate

      :abstractmethod:

      Abstract method to define the spatial input shape for the neural network architecture.
      The shape should not account for the channels and batch dimensions.

      :returns: The spatial input shape.
      :rtype: Coordinate

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> input_shape = Coordinate((128, 128, 128))
      >>> model = MyModel(input_shape)

      .. note:: The method should be implemented in the derived class.


   .. py:property:: eval_shape_increase
      :type: funlib.geometry.Coordinate

      Provides information about how much to increase the input shape during prediction.

      :returns: An instance representing the amount to increase in each dimension of the input shape.
      :rtype: Coordinate

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> eval_shape_increase = Coordinate((0, 0, 0))
      >>> model = MyModel(input_shape, eval_shape_increase)

      .. note:: The method is optional and can be overridden in the derived class.


   .. py:property:: num_in_channels
      :type: int

      :abstractmethod:

      Abstract method to return number of input channels required by the architecture.

      :returns: Required number of input channels.
      :rtype: int

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> num_in_channels = 1
      >>> model = MyModel(input_shape, num_in_channels)

      .. note:: The method should be implemented in the derived class.


   .. py:property:: num_out_channels
      :type: int

      :abstractmethod:

      Abstract method to return the number of output channels provided by the architecture.

      :returns: Number of output channels.
      :rtype: int

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> num_out_channels = 1
      >>> model = MyModel(input_shape, num_out_channels)

      .. note:: The method should be implemented in the derived class.


   .. py:property:: dims
      :type: int

      Returns the number of dimensions of the input shape.

      :returns: The number of dimensions.
      :rtype: int

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> input_shape = Coordinate((128, 128, 128))
      >>> model = MyModel(input_shape)
      >>> model.dims
      3

      .. note:: The method is optional and can be overridden in the derived class.


   .. py:method:: scale(input_voxel_size: funlib.geometry.Coordinate) -> funlib.geometry.Coordinate

      Method to scale the input voxel size as required by the architecture.

      :param input_voxel_size: The original size of the input voxel.
      :type input_voxel_size: Coordinate

      :returns: The scaled voxel size.
      :rtype: Coordinate

      :raises NotImplementedError: If the method is not implemented in the derived class.

      .. rubric:: Examples

      >>> input_voxel_size = Coordinate((1, 1, 1))
      >>> model = MyModel(input_shape)
      >>> model.scale(input_voxel_size)
      Coordinate((1, 1, 1))

      .. note:: The method is optional and can be overridden in the derived class.



.. py:class:: Trainer



   Trainer Abstract Base Class

   This serves as the blueprint for any trainer classes in the dacapo library.
   It defines essential methods that every subclass must implement for effective
   training of a neural network model.

   .. attribute:: iteration

      The number of training iterations.

      :type: int

   .. attribute:: batch_size

      The size of the training batch.

      :type: int

   .. attribute:: learning_rate

      The learning rate for the optimizer.

      :type: float

   .. method:: create_optimizer(model

      Model) -> torch.optim.Optimizer:
      Creates an optimizer for the model.

   .. method:: iterate(num_iterations

      int, model: Model, optimizer: torch.optim.Optimizer, device: torch.device) -> Iterator[TrainingIterationStats]:
      Performs a number of training iterations.

   .. method:: can_train(datasets

      List[Dataset]) -> bool:
      Checks if the trainer can train with a specific set of datasets.

   .. method:: build_batch_provider(datasets

      List[Dataset], model: Model, task: Task, snapshot_container: LocalContainerIdentifier) -> None:
      Initializes the training pipeline using various components.

   .. note:: The Trainer class is an abstract class that cannot be instantiated directly. It is meant to be subclassed.


   .. py:attribute:: iteration
      :type:  int


   .. py:attribute:: batch_size
      :type:  int


   .. py:attribute:: learning_rate
      :type:  float


   .. py:method:: create_optimizer(model: dacapo.experiments.model.Model) -> torch.optim.Optimizer
      :abstractmethod:


      Creates an optimizer for the model.

      :param model: The model for which the optimizer will be created.
      :type model: Model

      :returns: The optimizer created for the model.
      :rtype: torch.optim.Optimizer

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> optimizer = trainer.create_optimizer(model)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: iterate(num_iterations: int, model: dacapo.experiments.model.Model, optimizer: torch.optim.Optimizer, device: torch.device) -> Iterator[dacapo.experiments.training_iteration_stats.TrainingIterationStats]
      :abstractmethod:


      Performs a number of training iterations.

      :param num_iterations: Number of training iterations.
      :type num_iterations: int
      :param model: The model to be trained.
      :type model: Model
      :param optimizer: The optimizer for the model.
      :type optimizer: torch.optim.Optimizer
      :param device: The device (GPU/CPU) where the model will be trained.
      :type device: torch.device

      :returns: An iterator of the training statistics.
      :rtype: Iterator[TrainingIterationStats]

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> for iteration_stats in trainer.iterate(num_iterations, model, optimizer, device):
      >>>     print(iteration_stats)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: can_train(datasets: List[dacapo.experiments.datasplits.datasets.Dataset]) -> bool
      :abstractmethod:


      Checks if the trainer can train with a specific set of datasets.

      Some trainers may have specific requirements for their training datasets.

      :param datasets: The training datasets.
      :type datasets: List[Dataset]

      :returns: True if the trainer can train on the given datasets, False otherwise.
      :rtype: bool

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> can_train = trainer.can_train(datasets)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: build_batch_provider(datasets: List[dacapo.experiments.datasplits.datasets.Dataset], model: dacapo.experiments.model.Model, task: dacapo.experiments.tasks.task.Task, snapshot_container: dacapo.store.array_store.LocalContainerIdentifier) -> None
      :abstractmethod:


      Initializes the training pipeline using various components.

      This method uses the datasets, model, task, and snapshot_container to set up the
      training pipeline.

      :param datasets: The datasets to pull data from.
      :type datasets: List[Dataset]
      :param model: The model to inform the pipeline of required input/output sizes.
      :type model: Model
      :param task: The task to transform ground truth into target.
      :type task: Task
      :param snapshot_container: Defines where snapshots will be saved.
      :type snapshot_container: LocalContainerIdentifier

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> trainer.build_batch_provider(datasets, model, task, snapshot_container)

      .. note:: This method must be implemented by the subclass.



.. py:class:: TrainingStats

   A class used to represent Training Statistics. It contains a list of training
   iteration statistics. It also provides methods to add new iteration stats,
   delete stats after a specified iteration, get the number of iterations trained
   for, and convert the stats to a xarray data array.

   .. attribute:: iteration_stats

      List[TrainingIterationStats]
      an ordered list of training stats.

   .. method:: add_iteration_stats(iteration_stats

      TrainingIterationStats) -> None:
      Add a new set of iterations stats to the existing list of iteration
      stats.

   .. method:: delete_after(iteration

      int) -> None:
      Deletes training stats after a specified iteration number.

   .. method:: trained_until() -> int

      
      Gets the number of iterations that the model has been trained for.

   .. method:: to_xarray() -> xr.DataArray

      
      Converts the iteration statistics to a xarray data array.

   .. note::

      The iteration stats list is structured as follows:
      - The outer list contains the stats for each iteration.
      - The inner list contains the stats for each training iteration.


   .. py:attribute:: iteration_stats
      :type:  List[dacapo.experiments.training_iteration_stats.TrainingIterationStats]


   .. py:method:: add_iteration_stats(iteration_stats: dacapo.experiments.training_iteration_stats.TrainingIterationStats) -> None

      Add a new iteration stats to the current iteration stats.

      :param iteration_stats: a new iteration stats object.
      :type iteration_stats: TrainingIterationStats

      :raises assert: if the new iteration stats do not follow the order of existing iteration stats.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.iteration_stats
      [TrainingIterationStats(iteration=0, loss=0.1),
       TrainingIterationStats(iteration=1, loss=0.2),
       TrainingIterationStats(iteration=2, loss=0.3)]

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: delete_after(iteration: int) -> None

      Deletes training stats after a specified iteration.

      :param iteration: the iteration after which the stats are to be deleted.
      :type iteration: int

      :raises assert: if the iteration number is less than the maximum iteration number.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.delete_after(1)
      >>> training_stats.iteration_stats
      [TrainingIterationStats(iteration=0, loss=0.1)]

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: trained_until() -> int

      The number of iterations trained for (the maximum iteration plus one).
      Returns zero if no iterations trained yet.

      :returns: number of iterations that the model has been trained for.
      :rtype: int

      :raises assert: if the iteration stats list is empty.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.trained_until()
      3

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



   .. py:method:: to_xarray() -> xarray.DataArray

      Converts the iteration stats to a data array format easily manipulatable.

      :returns: xarray DataArray of iteration losses.
      :rtype: xr.DataArray

      :raises assert: if the iteration stats list is empty.

      .. rubric:: Examples

      >>> training_stats = TrainingStats()
      >>> training_stats.add_iteration_stats(TrainingIterationStats(0, 0.1))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(1, 0.2))
      >>> training_stats.add_iteration_stats(TrainingIterationStats(2, 0.3))
      >>> training_stats.to_xarray()
      <xarray.DataArray (iterations: 3)>
      array([0.1, 0.2, 0.3])
      Coordinates:
        * iterations  (iterations) int64 0 1 2

      .. note::

         The iteration stats list is structured as follows:
         - The outer list contains the stats for each iteration.
         - The inner list contains the stats for each training iteration.



.. py:class:: ValidationScores

   Class representing the validation scores for a set of parameters and datasets.

   .. attribute:: parameters

      The list of parameters that are being evaluated.

      :type: List[PostProcessorParameters]

   .. attribute:: datasets

      The datasets that will be evaluated at each iteration.

      :type: List[Dataset]

   .. attribute:: evaluation_scores

      The scores that are collected on each iteration per
      `PostProcessorParameters` and `Dataset`.

      :type: EvaluationScores

   .. attribute:: scores

      A list of evaluation scores and their associated
      post-processing parameters.

      :type: List[ValidationIterationScores]

   .. method:: subscores(iteration_scores)

      Create a new ValidationScores object with a subset of the iteration scores.

   .. method:: add_iteration_scores(iteration_scores)

      Add iteration scores to the list of scores.

   .. method:: delete_after(iteration)

      Delete scores after a specified iteration.

   .. method:: validated_until()

      Get the number of iterations validated for (the maximum iteration plus one).

   .. method:: compare(existing_iteration_scores)

      Compare iteration stats provided from elsewhere to scores we have saved locally.

   .. method:: criteria()

      Get the list of evaluation criteria.

   .. method:: parameter_names()

      Get the list of parameter names.

   .. method:: to_xarray()

      Convert the validation scores to an xarray DataArray.

   .. method:: get_best(data, dim)

      Compute the Best scores along dimension "dim" per criterion.

   .. rubric:: Notes

   The `scores` attribute is a list of `ValidationIterationScores` objects, each of which
   contains the scores for a single iteration.


   .. py:attribute:: parameters
      :type:  List[dacapo.experiments.tasks.post_processors.PostProcessorParameters]


   .. py:attribute:: datasets
      :type:  List[dacapo.experiments.datasplits.datasets.Dataset]


   .. py:attribute:: evaluation_scores
      :type:  dacapo.experiments.tasks.evaluators.EvaluationScores


   .. py:attribute:: scores
      :type:  List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]


   .. py:method:: subscores(iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> ValidationScores

      Create a new ValidationScores object with a subset of the iteration scores.

      :param iteration_scores: The iteration scores to include in the new ValidationScores object.

      :returns: A new ValidationScores object with the specified iteration scores.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.subscores([validation_scores.scores[0]])

      .. note::

         This method is used to create a new ValidationScores object with a subset of the
         iteration scores. This is useful when you want to create a new ValidationScores object
         that only contains the scores up to a certain iteration.



   .. py:method:: add_iteration_scores(iteration_scores: dacapo.experiments.validation_iteration_scores.ValidationIterationScores) -> None

      Add iteration scores to the list of scores.

      :param iteration_scores: The iteration scores to add.

      :raises ValueError: If the iteration scores are already in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.add_iteration_scores(validation_scores.scores[0])

      .. note::

         This method is used to add iteration scores to the list of scores. This is useful when
         you want to add scores for a new iteration to the ValidationScores object.



   .. py:method:: delete_after(iteration: int) -> None

      Delete scores after a specified iteration.

      :param iteration: The iteration after which to delete the scores.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.delete_after(0)

      .. note::

         This method is used to delete scores after a specified iteration. This is useful when
         you want to delete scores after a certain iteration.



   .. py:method:: validated_until() -> int

      Get the number of iterations validated for (the maximum iteration plus one).

      :returns: The number of iterations validated for.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.validated_until()

      .. note::

         This method is used to get the number of iterations validated for (the maximum iteration
         plus one). This is useful when you want to know how many iterations have been validated.



   .. py:method:: compare(existing_iteration_scores: List[dacapo.experiments.validation_iteration_scores.ValidationIterationScores]) -> Tuple[bool, int]

      Compare iteration stats provided from elsewhere to scores we have saved locally.
      Local scores take priority. If local scores are at a lower iteration than the
      existing ones, delete the existing ones and replace with local.
      If local iteration > existing iteration, just update existing scores with the last
      overhanging local scores.

      :param existing_iteration_scores: The existing iteration scores to compare with.

      :returns: A tuple indicating whether the local scores should replace the existing ones
                and the existing iteration number.

      :raises ValueError: If the iteration scores are not in the list of scores.

      .. rubric:: Examples

      >>> validation_scores.compare([validation_scores.scores[0]])

      .. note::

         This method is used to compare iteration stats provided from elsewhere to scores we have
         saved locally. Local scores take priority. If local scores are at a lower iteration than
         the existing ones, delete the existing ones and replace with local. If local iteration >
         existing iteration, just update existing scores with the last overhanging local scores.



   .. py:property:: criteria
      :type: List[str]

      Get the list of evaluation criteria.

      :returns: The list of evaluation criteria.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.criteria

      .. note::

         This property is used to get the list of evaluation criteria. This is useful when you
         want to know what criteria are being used to evaluate the scores.


   .. py:property:: parameter_names
      :type: List[str]

      Get the list of parameter names.

      :returns: The list of parameter names.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.parameter_names

      .. note::

         This property is used to get the list of parameter names. This is useful when you want
         to know what parameters are being used to evaluate the scores.


   .. py:method:: to_xarray() -> xarray.DataArray

      Convert the validation scores to an xarray DataArray.

      :returns: An xarray DataArray representing the validation scores.

      :raises ValueError: If there are no scores.

      .. rubric:: Examples

      >>> validation_scores.to_xarray()

      .. note::

         This method is used to convert the validation scores to an xarray DataArray. This is
         useful when you want to work with the validation scores as an xarray DataArray.



   .. py:method:: get_best(data: xarray.DataArray, dim: str) -> Tuple[xarray.DataArray, xarray.DataArray]

      Compute the Best scores along dimension "dim" per criterion.
      Returns both the index associated with the best value, and the
      best value in two separate arrays.

      :param data: The data array to compute the best scores from.
      :param dim: The dimension along which to compute the best scores.

      :returns: A tuple containing the index associated with the best value and the best value
                in two separate arrays.

      :raises ValueError: If the criteria are not in the data array.

      .. rubric:: Examples

      >>> validation_scores.get_best(data, "iterations")

      .. note::

         This method is used to compute the Best scores along dimension "dim" per criterion. It
         returns both the index associated with the best value and the best value in two separate
         arrays. This is useful when you want to know the best scores for a given data array.
         Fix: The method is currently not able to handle the case where the criteria are not in the data array.
         To fix this, we need to add a check to see if the criteria are in the data array and raise an error if they are not.



.. py:class:: Start(start_config)



   This class interfaces with the dacapo store to retrieve and load the
   weights of the starter model used for finetuning.

   .. attribute:: run

      str
      The specified run to retrieve weights for the model.

   .. attribute:: criterion

      str
      The policy that was used to decide when to store the weights.

   .. attribute:: channels

      int
      The number of channels in the input data.

   .. method:: __init__(start_config)

      
      Initializes the Start class with specified config to run the
      initialization of weights for a model associated with a specific
      criterion.

   .. method:: initialize_weights(model, new_head=None)

      
      Retrieves the weights from the dacapo store and load them into
      the model.

   .. rubric:: Notes

   This class is used to retrieve and load the weights of the starter
   model used for finetuning from the dacapo store.


   .. py:method:: initialize_weights(model, new_head=None)

      Retrieves the weights from the dacapo store and load them into
      the model.

      :param model: obj
                    The model to which the weights are to be loaded.
      :param new_head: list
                       The labels of the new head.

      :returns:

                obj
                    The model with the weights loaded from the dacapo store.
      :rtype: model

      :raises RuntimeError: If weights of a non-existing or mismatched layer are being
          loaded, a RuntimeError exception is thrown which is logged
          and handled by loading only the common layers from weights.

      .. rubric:: Examples

      >>> model = start.initialize_weights(model, new_head)

      .. rubric:: Notes

      This function is called by the Start class to retrieve the weights
      from the dacapo store and load them into the model.



.. py:class:: Model(architecture: dacapo.experiments.architectures.architecture.Architecture, prediction_head: torch.nn.Module, eval_activation: torch.nn.Module | None = None)



   A trainable DaCapo model. Consists of an ``Architecture`` and a
   prediction head. Models are generated by ``Predictor``s.

   May include an optional eval_activation that is only executed when the model
   is in eval mode. This is particularly useful if you want to train with something
   like BCELossWithLogits, since you want to avoid applying softmax while training,
   but apply it during evaluation.

   .. attribute:: architecture

      The architecture of the model.

      :type: Architecture

   .. attribute:: prediction_head

      The prediction head of the model.

      :type: torch.nn.Module

   .. attribute:: chain

      The architecture followed by the prediction head.

      :type: torch.nn.Sequential

   .. attribute:: num_in_channels

      The number of input channels.

      :type: int

   .. attribute:: input_shape

      The shape of the input tensor.

      :type: Coordinate

   .. attribute:: eval_input_shape

      The shape of the input tensor during evaluation.

      :type: Coordinate

   .. attribute:: num_out_channels

      The number of output channels.

      :type: int

   .. attribute:: output_shape

      The shape of the output

      :type: Coordinate

   .. attribute:: eval_activation

      The activation function to apply during evaluation.

      :type: torch.nn.Module | None

   .. method:: forward(x

      torch.Tensor) -> torch.Tensor:
      Forward pass of the model.

   .. method:: compute_output_shape(input_shape

      Coordinate) -> Tuple[int, Coordinate]:
      Compute the spatial shape of this model, when fed a tensor of the given spatial shape as input.

   .. method:: scale(voxel_size

      Coordinate) -> Coordinate:
      Scale the model by the given voxel size.

   .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.


   .. py:attribute:: num_out_channels
      :type:  int


   .. py:attribute:: num_in_channels
      :type:  int


   .. py:method:: forward(x)

      Forward pass of the model.

      :param x: The input tensor.
      :type x: torch.Tensor

      :returns: The output tensor.
      :rtype: torch.Tensor

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.forward(x)
      torch.Tensor

      .. note:: The eval_activation is only applied during evaluation. This is particularly useful if you want to train with something like BCELossWithLogits, since you want to avoid applying softmax while training, but apply it during evaluation.



   .. py:method:: compute_output_shape(input_shape: funlib.geometry.Coordinate) -> Tuple[int, funlib.geometry.Coordinate]

      Compute the spatial shape (i.e., not accounting for channels and
      batch dimensions) of this model, when fed a tensor of the given spatial
      shape as input.

      :param input_shape: The shape of the input tensor.
      :type input_shape: Coordinate

      :returns: The number of output channels and the spatial shape of the output.
      :rtype: Tuple[int, Coordinate]

      :raises AssertionError: If the input_shape is not a Coordinate.

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.compute_output_shape(input_shape)
      (1, Coordinate(1, 1, 1))

      .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.



   .. py:method:: scale(voxel_size: funlib.geometry.Coordinate) -> funlib.geometry.Coordinate

      Scale the model by the given voxel size.

      :param voxel_size: The voxel size to scale the model by.
      :type voxel_size: Coordinate

      :returns: The scaled model.
      :rtype: Coordinate

      :raises AssertionError: If the voxel_size is not a Coordinate.

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.scale(voxel_size)
      Coordinate(1, 1, 1)

      .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.



.. py:class:: Run(run_config, load_starter_model: bool = True)

   Class representing a run in the experiment. A run is a combination of a task, architecture, trainer, datasplit,
   model, optimizer, training stats, and validation scores. It also contains the name of the run, the number of
   iterations to train for, and the interval at which to validate. It also contains a start object that can be used to
   initialize the model with preloaded weights. The run object can be used to move the optimizer to a specified device.

   .. attribute:: name

      The name of the run.

      :type: str

   .. attribute:: train_until

      The number of iterations to train for.

      :type: int

   .. attribute:: validation_interval

      The interval at which to validate.

      :type: int

   .. attribute:: task

      The task object.

      :type: Task

   .. attribute:: architecture

      The architecture object.

      :type: Architecture

   .. attribute:: trainer

      The trainer object.

      :type: Trainer

   .. attribute:: datasplit

      The datasplit object.

      :type: DataSplit

   .. attribute:: model

      The model object.

      :type: Model

   .. attribute:: optimizer

      The optimizer object.

      :type: torch.optim.Optimizer

   .. attribute:: training_stats

      The training stats object.

      :type: TrainingStats

   .. attribute:: validation_scores

      The validation scores object.

      :type: ValidationScores

   .. attribute:: start

      The start object.

      :type: Start

   .. method:: move_optimizer(device

      torch.device, empty_cuda_cache: bool) -> None:
      Moves the optimizer to the specified device.

   .. method:: get_validation_scores(run_config) -> ValidationScores

      
      Static method to get the validation scores without initializing model, optimizer, trainer, etc.

   .. note::

      The iteration stats list is structured as follows:
      - The outer list contains the stats for each iteration.
      - The inner list contains the stats for each training iteration.


   .. py:attribute:: name
      :type:  str


   .. py:attribute:: train_until
      :type:  int


   .. py:attribute:: validation_interval
      :type:  int


   .. py:attribute:: task
      :type:  dacapo.experiments.tasks.task.Task


   .. py:attribute:: architecture
      :type:  dacapo.experiments.architectures.Architecture


   .. py:attribute:: trainer
      :type:  dacapo.experiments.trainers.Trainer


   .. py:attribute:: model
      :type:  dacapo.experiments.model.Model


   .. py:attribute:: optimizer
      :type:  torch.optim.Optimizer


   .. py:attribute:: training_stats
      :type:  dacapo.experiments.training_stats.TrainingStats


   .. py:property:: datasplit


   .. py:property:: validation_scores


   .. py:method:: get_validation_scores(run_config) -> dacapo.experiments.validation_scores.ValidationScores
      :staticmethod:


      Static method to get the validation scores without initializing model, optimizer, trainer, etc.

      :param run_config: The configuration for the run.

      :returns: The validation scores.

      :raises AssertionError: If the task or datasplit types are not specified in the run_config.

      .. rubric:: Examples

      >>> validation_scores = Run.get_validation_scores(run_config)
      >>> validation_scores
      ValidationScores object



   .. py:method:: move_optimizer(device: torch.device, empty_cuda_cache: bool = False) -> None

      Moves the optimizer to the specified device.

      :param device: The device to move the optimizer to.
      :param empty_cuda_cache: Whether to empty the CUDA cache after moving the optimizer.

      :raises AssertionError: If the optimizer state is not a dictionary.

      .. rubric:: Examples

      >>> run.move_optimizer(device)
      >>> run.optimizer
      Optimizer object



