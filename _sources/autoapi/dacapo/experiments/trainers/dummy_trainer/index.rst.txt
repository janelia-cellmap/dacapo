dacapo.experiments.trainers.dummy_trainer
=========================================

.. py:module:: dacapo.experiments.trainers.dummy_trainer


Classes
-------

.. autoapisummary::

   dacapo.experiments.trainers.dummy_trainer.TrainingIterationStats
   dacapo.experiments.trainers.dummy_trainer.Trainer
   dacapo.experiments.trainers.dummy_trainer.Model
   dacapo.experiments.trainers.dummy_trainer.DummyTrainer


Module Contents
---------------

.. py:class:: TrainingIterationStats

   A class to represent the training iteration statistics. It contains the loss and time taken for each iteration.

   .. attribute:: iteration

      The iteration that produced these stats.

      :type: int

   .. attribute:: loss

      The loss value of this iteration.

      :type: float

   .. attribute:: time

      The time it took to process this iteration.

      :type: float

   .. note::

      The iteration stats list is structured as follows:
      - The outer list contains the stats for each iteration.
      - The inner list contains the stats for each training iteration.


   .. py:attribute:: iteration
      :type:  int


   .. py:attribute:: loss
      :type:  float


   .. py:attribute:: time
      :type:  float


.. py:class:: Trainer



   Trainer Abstract Base Class

   This serves as the blueprint for any trainer classes in the dacapo library.
   It defines essential methods that every subclass must implement for effective
   training of a neural network model.

   .. attribute:: iteration

      The number of training iterations.

      :type: int

   .. attribute:: batch_size

      The size of the training batch.

      :type: int

   .. attribute:: learning_rate

      The learning rate for the optimizer.

      :type: float

   .. method:: create_optimizer(model

      Model) -> torch.optim.Optimizer:
      Creates an optimizer for the model.

   .. method:: iterate(num_iterations

      int, model: Model, optimizer: torch.optim.Optimizer, device: torch.device) -> Iterator[TrainingIterationStats]:
      Performs a number of training iterations.

   .. method:: can_train(datasets

      List[Dataset]) -> bool:
      Checks if the trainer can train with a specific set of datasets.

   .. method:: build_batch_provider(datasets

      List[Dataset], model: Model, task: Task, snapshot_container: LocalContainerIdentifier) -> None:
      Initializes the training pipeline using various components.

   .. note:: The Trainer class is an abstract class that cannot be instantiated directly. It is meant to be subclassed.


   .. py:attribute:: iteration
      :type:  int


   .. py:attribute:: batch_size
      :type:  int


   .. py:attribute:: learning_rate
      :type:  float


   .. py:method:: create_optimizer(model: dacapo.experiments.model.Model) -> torch.optim.Optimizer
      :abstractmethod:


      Creates an optimizer for the model.

      :param model: The model for which the optimizer will be created.
      :type model: Model

      :returns: The optimizer created for the model.
      :rtype: torch.optim.Optimizer

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> optimizer = trainer.create_optimizer(model)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: iterate(num_iterations: int, model: dacapo.experiments.model.Model, optimizer: torch.optim.Optimizer, device: torch.device) -> Iterator[dacapo.experiments.training_iteration_stats.TrainingIterationStats]
      :abstractmethod:


      Performs a number of training iterations.

      :param num_iterations: Number of training iterations.
      :type num_iterations: int
      :param model: The model to be trained.
      :type model: Model
      :param optimizer: The optimizer for the model.
      :type optimizer: torch.optim.Optimizer
      :param device: The device (GPU/CPU) where the model will be trained.
      :type device: torch.device

      :returns: An iterator of the training statistics.
      :rtype: Iterator[TrainingIterationStats]

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> for iteration_stats in trainer.iterate(num_iterations, model, optimizer, device):
      >>>     print(iteration_stats)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: can_train(datasets: List[dacapo.experiments.datasplits.datasets.Dataset]) -> bool
      :abstractmethod:


      Checks if the trainer can train with a specific set of datasets.

      Some trainers may have specific requirements for their training datasets.

      :param datasets: The training datasets.
      :type datasets: List[Dataset]

      :returns: True if the trainer can train on the given datasets, False otherwise.
      :rtype: bool

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> can_train = trainer.can_train(datasets)

      .. note:: This method must be implemented by the subclass.



   .. py:method:: build_batch_provider(datasets: List[dacapo.experiments.datasplits.datasets.Dataset], model: dacapo.experiments.model.Model, task: dacapo.experiments.tasks.task.Task, snapshot_container: dacapo.store.array_store.LocalContainerIdentifier) -> None
      :abstractmethod:


      Initializes the training pipeline using various components.

      This method uses the datasets, model, task, and snapshot_container to set up the
      training pipeline.

      :param datasets: The datasets to pull data from.
      :type datasets: List[Dataset]
      :param model: The model to inform the pipeline of required input/output sizes.
      :type model: Model
      :param task: The task to transform ground truth into target.
      :type task: Task
      :param snapshot_container: Defines where snapshots will be saved.
      :type snapshot_container: LocalContainerIdentifier

      :raises NotImplementedError: If the method is not implemented by the subclass.

      .. rubric:: Examples

      >>> trainer.build_batch_provider(datasets, model, task, snapshot_container)

      .. note:: This method must be implemented by the subclass.



.. py:class:: Model(architecture: dacapo.experiments.architectures.architecture.Architecture, prediction_head: torch.nn.Module, eval_activation: torch.nn.Module | None = None)



   A trainable DaCapo model. Consists of an ``Architecture`` and a
   prediction head. Models are generated by ``Predictor``s.

   May include an optional eval_activation that is only executed when the model
   is in eval mode. This is particularly useful if you want to train with something
   like BCELossWithLogits, since you want to avoid applying softmax while training,
   but apply it during evaluation.

   .. attribute:: architecture

      The architecture of the model.

      :type: Architecture

   .. attribute:: prediction_head

      The prediction head of the model.

      :type: torch.nn.Module

   .. attribute:: chain

      The architecture followed by the prediction head.

      :type: torch.nn.Sequential

   .. attribute:: num_in_channels

      The number of input channels.

      :type: int

   .. attribute:: input_shape

      The shape of the input tensor.

      :type: Coordinate

   .. attribute:: eval_input_shape

      The shape of the input tensor during evaluation.

      :type: Coordinate

   .. attribute:: num_out_channels

      The number of output channels.

      :type: int

   .. attribute:: output_shape

      The shape of the output

      :type: Coordinate

   .. attribute:: eval_activation

      The activation function to apply during evaluation.

      :type: torch.nn.Module | None

   .. method:: forward(x

      torch.Tensor) -> torch.Tensor:
      Forward pass of the model.

   .. method:: compute_output_shape(input_shape

      Coordinate) -> Tuple[int, Coordinate]:
      Compute the spatial shape of this model, when fed a tensor of the given spatial shape as input.

   .. method:: scale(voxel_size

      Coordinate) -> Coordinate:
      Scale the model by the given voxel size.

   .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.


   .. py:attribute:: num_out_channels
      :type:  int


   .. py:attribute:: num_in_channels
      :type:  int


   .. py:method:: forward(x)

      Forward pass of the model.

      :param x: The input tensor.
      :type x: torch.Tensor

      :returns: The output tensor.
      :rtype: torch.Tensor

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.forward(x)
      torch.Tensor

      .. note:: The eval_activation is only applied during evaluation. This is particularly useful if you want to train with something like BCELossWithLogits, since you want to avoid applying softmax while training, but apply it during evaluation.



   .. py:method:: compute_output_shape(input_shape: funlib.geometry.Coordinate) -> Tuple[int, funlib.geometry.Coordinate]

      Compute the spatial shape (i.e., not accounting for channels and
      batch dimensions) of this model, when fed a tensor of the given spatial
      shape as input.

      :param input_shape: The shape of the input tensor.
      :type input_shape: Coordinate

      :returns: The number of output channels and the spatial shape of the output.
      :rtype: Tuple[int, Coordinate]

      :raises AssertionError: If the input_shape is not a Coordinate.

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.compute_output_shape(input_shape)
      (1, Coordinate(1, 1, 1))

      .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.



   .. py:method:: scale(voxel_size: funlib.geometry.Coordinate) -> funlib.geometry.Coordinate

      Scale the model by the given voxel size.

      :param voxel_size: The voxel size to scale the model by.
      :type voxel_size: Coordinate

      :returns: The scaled model.
      :rtype: Coordinate

      :raises AssertionError: If the voxel_size is not a Coordinate.

      .. rubric:: Examples

      >>> model = Model(architecture, prediction_head)
      >>> model.scale(voxel_size)
      Coordinate(1, 1, 1)

      .. note:: The output shape is the spatial shape of the model, i.e., not accounting for channels and batch dimensions.



.. py:class:: DummyTrainer(trainer_config)



   This class is used to train a model using dummy data and is used for testing purposes. It contains attributes
   related to learning rate, batch size, and mirror augment. It also contains methods to create an optimizer, iterate
   over the training data, build a batch provider, and check if the trainer can train on the given data split. This class
   contains methods to enter and exit the context manager. The iterate method yields training iteration statistics.

   .. attribute:: learning_rate

      The learning rate to use.

      :type: float

   .. attribute:: batch_size

      The batch size to use.

      :type: int

   .. attribute:: mirror_augment

      A boolean value indicating whether to use mirror augmentation or not.

      :type: bool

   .. method:: __init__(self, trainer_config)

      This method initializes the DummyTrainer object.

   .. method:: create_optimizer(self, model)

      This method creates an optimizer for the given model.

   .. method:: iterate(self, num_iterations

      int, model, optimizer, device): This method iterates over the training data for the specified number of iterations.

   .. method:: build_batch_provider(self, datasplit, architecture, task, snapshot_container)

      This method builds a batch provider for the given data split, architecture, task, and snapshot container.

   .. method:: can_train(self, datasplit)

      This method checks if the trainer can train on the given data split.

   .. method:: __enter__(self)

      This method enters the context manager.

   .. method:: __exit__(self, exc_type, exc_val, exc_tb)

      This method exits the context manager.

   .. note:: The iterate method yields TrainingIterationStats.


   .. py:attribute:: iteration
      :value: 0



   .. py:method:: create_optimizer(model)

      Create an optimizer for the given model.

      :param model: The model to optimize.
      :type model: Model

      :returns: The optimizer object.
      :rtype: torch.optim.Optimizer

      .. rubric:: Examples

      >>> optimizer = create_optimizer(model)



   .. py:method:: iterate(num_iterations: int, model: dacapo.experiments.model.Model, optimizer, device)

      Iterate over the training data for the specified number of iterations.

      :param num_iterations: The number of iterations to perform.
      :type num_iterations: int
      :param model: The model to train.
      :type model: Model
      :param optimizer: The optimizer to use.
      :type optimizer: torch.optim.Optimizer
      :param device: The device to perform the computations on.
      :type device: torch.device

      :Yields: *TrainingIterationStats* -- The training iteration statistics.

      :raises ValueError: If the number of iterations is less than or equal to zero.

      .. rubric:: Examples

      >>> for stats in iterate(num_iterations, model, optimizer, device):
      >>>     print(stats)



   .. py:method:: build_batch_provider(datasplit, architecture, task, snapshot_container)

      Build a batch provider for the given data split, architecture, task, and snapshot container.

      :param datasplit: The data split to use.
      :type datasplit: DataSplit
      :param architecture: The architecture to use.
      :type architecture: Architecture
      :param task: The task to perform.
      :type task: Task
      :param snapshot_container: The snapshot container to use.
      :type snapshot_container: SnapshotContainer

      :returns: The batch provider object.
      :rtype: BatchProvider

      :raises ValueError: If the task loss is not set.

      .. rubric:: Examples

      >>> batch_provider = build_batch_provider(datasplit, architecture, task, snapshot_container)



   .. py:method:: can_train(datasplit)

      Check if the trainer can train on the given data split.

      :param datasplit: The data split to check.
      :type datasplit: DataSplit

      :returns: True if the trainer can train on the data split, False otherwise.
      :rtype: bool

      :raises NotImplementedError: If the method is not implemented.

      .. rubric:: Examples

      >>> can_train(datasplit)



