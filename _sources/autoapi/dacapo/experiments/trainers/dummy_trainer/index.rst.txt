dacapo.experiments.trainers.dummy_trainer
=========================================

.. py:module:: dacapo.experiments.trainers.dummy_trainer


Classes
-------

.. autoapisummary::

   dacapo.experiments.trainers.dummy_trainer.DummyTrainer


Module Contents
---------------

.. py:class:: DummyTrainer(trainer_config)



   This class is used to train a model using dummy data and is used for testing purposes. It contains attributes
   related to learning rate, batch size, and mirror augment. It also contains methods to create an optimizer, iterate
   over the training data, build a batch provider, and check if the trainer can train on the given data split. This class
   contains methods to enter and exit the context manager. The iterate method yields training iteration statistics.

   .. attribute:: learning_rate

      The learning rate to use.

      :type: float

   .. attribute:: batch_size

      The batch size to use.

      :type: int

   .. attribute:: mirror_augment

      A boolean value indicating whether to use mirror augmentation or not.

      :type: bool

   .. method:: __init__(self, trainer_config)

      This method initializes the DummyTrainer object.

   .. method:: create_optimizer(self, model)

      This method creates an optimizer for the given model.

   .. method:: iterate(self, num_iterations

      int, model, optimizer, device): This method iterates over the training data for the specified number of iterations.

   .. method:: build_batch_provider(self, datasplit, architecture, task, snapshot_container)

      This method builds a batch provider for the given data split, architecture, task, and snapshot container.

   .. method:: can_train(self, datasplit)

      This method checks if the trainer can train on the given data split.

   .. method:: __enter__(self)

      This method enters the context manager.

   .. method:: __exit__(self, exc_type, exc_val, exc_tb)

      This method exits the context manager.

   .. note:: The iterate method yields TrainingIterationStats.


   .. py:attribute:: iteration
      :value: 0



   .. py:attribute:: learning_rate


   .. py:attribute:: batch_size


   .. py:attribute:: mirror_augment


   .. py:method:: create_optimizer(model)

      Create an optimizer for the given model.

      :param model: The model to optimize.
      :type model: Model

      :returns: The optimizer object.
      :rtype: torch.optim.Optimizer

      .. rubric:: Examples

      >>> optimizer = create_optimizer(model)



   .. py:method:: iterate(num_iterations: int, model: dacapo.experiments.model.Model, optimizer, device)

      Iterate over the training data for the specified number of iterations.

      :param num_iterations: The number of iterations to perform.
      :type num_iterations: int
      :param model: The model to train.
      :type model: Model
      :param optimizer: The optimizer to use.
      :type optimizer: torch.optim.Optimizer
      :param device: The device to perform the computations on.
      :type device: torch.device

      :Yields: *TrainingIterationStats* -- The training iteration statistics.

      :raises ValueError: If the number of iterations is less than or equal to zero.

      .. rubric:: Examples

      >>> for stats in iterate(num_iterations, model, optimizer, device):
      >>>     print(stats)



   .. py:method:: build_batch_provider(datasplit, architecture, task, snapshot_container)

      Build a batch provider for the given data split, architecture, task, and snapshot container.

      :param datasplit: The data split to use.
      :type datasplit: DataSplit
      :param architecture: The architecture to use.
      :type architecture: Architecture
      :param task: The task to perform.
      :type task: Task
      :param snapshot_container: The snapshot container to use.
      :type snapshot_container: SnapshotContainer

      :returns: The batch provider object.
      :rtype: BatchProvider

      :raises ValueError: If the task loss is not set.

      .. rubric:: Examples

      >>> batch_provider = build_batch_provider(datasplit, architecture, task, snapshot_container)



   .. py:method:: can_train(datasplit)

      Check if the trainer can train on the given data split.

      :param datasplit: The data split to check.
      :type datasplit: DataSplit

      :returns: True if the trainer can train on the data split, False otherwise.
      :rtype: bool

      :raises NotImplementedError: If the method is not implemented.

      .. rubric:: Examples

      >>> can_train(datasplit)



