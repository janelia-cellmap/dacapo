dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores
=========================================================================

.. py:module:: dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores


Classes
-------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores.EvaluationScores
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores.BinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluation_scores.MultiChannelBinarySegmentationEvaluationScores


Module Contents
---------------

.. py:class:: EvaluationScores

   Base class for evaluation scores. This class is used to store the evaluation scores for a task.
   The scores include the evaluation criteria. The class also provides methods to determine whether higher is better for a given criterion,
   the bounds for a given criterion, and whether to store the best score for a given criterion.

   .. attribute:: criteria

      List[str]
      the evaluation criteria

   .. method:: higher_is_better(criterion)

      
      Return whether higher is better for the given criterion.

   .. method:: bounds(criterion)

      
      Return the bounds for the given criterion.

   .. method:: store_best(criterion)

      
      Return whether to store the best score for the given criterion.

   .. note:: The EvaluationScores class is used to store the evaluation scores for a task. All evaluation scores should inherit from this class.


   .. py:property:: criteria
      :type: List[str]

      :abstractmethod:

      The evaluation criteria.

      :returns:

                List[str]
                    the evaluation criteria

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluation_scores = EvaluationScores()
      >>> evaluation_scores.criteria
      ["criterion1", "criterion2"]

      .. note:: This function is used to return the evaluation criteria.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:

      :abstractmethod:


      Wether or not higher is better for this criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                bool
                    whether higher is better for this criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluation_scores = EvaluationScores()
      >>> criterion = "criterion1"
      >>> evaluation_scores.higher_is_better(criterion)
      True

      .. note:: This function is used to determine whether higher is better for a given criterion.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:

      :abstractmethod:


      The bounds for this criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                Tuple[Union[int, float, None], Union[int, float, None]]
                    the bounds for this criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluation_scores = EvaluationScores()
      >>> criterion = "criterion1"
      >>> evaluation_scores.bounds(criterion)
      (0, 1)

      .. note:: This function is used to return the bounds for the given criterion.



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:

      :abstractmethod:


      Whether or not to save the best validation block and model
      weights for this criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                bool
                    whether to store the best score for this criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluation_scores = EvaluationScores()
      >>> criterion = "criterion1"
      >>> evaluation_scores.store_best(criterion)
      True

      .. note:: This function is used to return whether to store the best score for the given criterion.



.. py:class:: BinarySegmentationEvaluationScores



   Class representing evaluation scores for binary segmentation tasks.

   The metrics include:
   - Dice coefficient: 2 * |A ∩ B| / |A| + |B| ; where A and B are the binary segmentations
   - Jaccard coefficient: |A ∩ B| / |A ∪ B| ; where A and B are the binary segmentations
   - Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
   - False negative rate: |A - B| / |A| ; where A and B are the binary segmentations
   - False positive rate: |B - A| / |B| ; where A and B are the binary segmentations
   - False discovery rate: |B - A| / |A| ; where A and B are the binary segmentations
   - VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
   - Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
   - Mean false negative distance: mean distance of false negatives
   - Mean false positive distance: mean distance of false positives
   - Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
   - Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
   - Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
   - Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
   - Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
   - F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
   - Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
   - Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
   - F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives

   .. attribute:: dice

      The Dice coefficient.

      :type: float

   .. attribute:: jaccard

      The Jaccard index.

      :type: float

   .. attribute:: hausdorff

      The Hausdorff distance.

      :type: float

   .. attribute:: false_negative_rate

      The false negative rate.

      :type: float

   .. attribute:: false_negative_rate_with_tolerance

      The false negative rate with tolerance.

      :type: float

   .. attribute:: false_positive_rate

      The false positive rate.

      :type: float

   .. attribute:: false_discovery_rate

      The false discovery rate.

      :type: float

   .. attribute:: false_positive_rate_with_tolerance

      The false positive rate with tolerance.

      :type: float

   .. attribute:: voi

      The variation of information.

      :type: float

   .. attribute:: mean_false_distance

      The mean false distance.

      :type: float

   .. attribute:: mean_false_negative_distance

      The mean false negative distance.

      :type: float

   .. attribute:: mean_false_positive_distance

      The mean false positive distance.

      :type: float

   .. attribute:: mean_false_distance_clipped

      The mean false distance clipped.

      :type: float

   .. attribute:: mean_false_negative_distance_clipped

      The mean false negative distance clipped.

      :type: float

   .. attribute:: mean_false_positive_distance_clipped

      The mean false positive distance clipped.

      :type: float

   .. attribute:: precision_with_tolerance

      The precision with tolerance.

      :type: float

   .. attribute:: recall_with_tolerance

      The recall with tolerance.

      :type: float

   .. attribute:: f1_score_with_tolerance

      The F1 score with tolerance.

      :type: float

   .. attribute:: precision

      The precision.

      :type: float

   .. attribute:: recall

      The recall.

      :type: float

   .. attribute:: f1_score

      The F1 score.

      :type: float

   .. method:: store_best(criterion

      str) -> bool: Whether or not to store the best weights/validation blocks for this criterion.

   .. method:: higher_is_better(criterion

      str) -> bool: Determines whether a higher value is better for a given criterion.

   .. method:: bounds(criterion

      str) -> Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.

   .. rubric:: Notes

   The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.


   .. py:attribute:: dice
      :type:  float


   .. py:attribute:: jaccard
      :type:  float


   .. py:attribute:: hausdorff
      :type:  float


   .. py:attribute:: false_negative_rate
      :type:  float


   .. py:attribute:: false_negative_rate_with_tolerance
      :type:  float


   .. py:attribute:: false_positive_rate
      :type:  float


   .. py:attribute:: false_discovery_rate
      :type:  float


   .. py:attribute:: false_positive_rate_with_tolerance
      :type:  float


   .. py:attribute:: voi
      :type:  float


   .. py:attribute:: mean_false_distance
      :type:  float


   .. py:attribute:: mean_false_negative_distance
      :type:  float


   .. py:attribute:: mean_false_positive_distance
      :type:  float


   .. py:attribute:: mean_false_distance_clipped
      :type:  float


   .. py:attribute:: mean_false_negative_distance_clipped
      :type:  float


   .. py:attribute:: mean_false_positive_distance_clipped
      :type:  float


   .. py:attribute:: precision_with_tolerance
      :type:  float


   .. py:attribute:: recall_with_tolerance
      :type:  float


   .. py:attribute:: f1_score_with_tolerance
      :type:  float


   .. py:attribute:: precision
      :type:  float


   .. py:attribute:: recall
      :type:  float


   .. py:attribute:: f1_score
      :type:  float


   .. py:attribute:: criteria
      :value: ['dice', 'jaccard', 'hausdorff', 'false_negative_rate', 'false_negative_rate_with_tolerance',...



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:


      Determines whether or not to store the best weights/validation blocks for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if the best weights/validation blocks should be stored, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.store_best("dice")
      False
      >>> BinarySegmentationEvaluationScores.store_best("f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.



   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:


      Determines whether a higher value is better for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if a higher value is better, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.higher_is_better("dice")
      True
      >>> BinarySegmentationEvaluationScores.higher_is_better("f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:


      Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: The lower and upper bounds for the criterion.
      :rtype: Tuple[Union[int, float, None], Union[int, float, None]]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.bounds("dice")
      (0, 1)
      >>> BinarySegmentationEvaluationScores.bounds("hausdorff")
      (0, nan)

      .. rubric:: Notes

      The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.



.. py:class:: MultiChannelBinarySegmentationEvaluationScores



   Class representing evaluation scores for multi-channel binary segmentation tasks.

   .. attribute:: channel_scores

      The list of channel scores.

      :type: List[Tuple[str, BinarySegmentationEvaluationScores]]

   .. method:: higher_is_better(criterion

      str) -> bool: Determines whether a higher value is better for a given criterion.

   .. method:: store_best(criterion

      str) -> bool: Whether or not to store the best weights/validation blocks for this criterion.

   .. method:: bounds(criterion

      str) -> Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.

   .. rubric:: Notes

   The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.


   .. py:attribute:: channel_scores
      :type:  List[Tuple[str, BinarySegmentationEvaluationScores]]


   .. py:property:: criteria
      Returns a list of all criteria for all channels.

      :returns: The list of criteria.
      :rtype: List[str]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> channel_scores = [("channel1", BinarySegmentationEvaluationScores()), ("channel2", BinarySegmentationEvaluationScores())]
      >>> MultiChannelBinarySegmentationEvaluationScores(channel_scores).criteria

      .. rubric:: Notes

      The method returns a list of all criteria for all channels. The criteria are stored as attributes of the class.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:


      Determines whether a higher value is better for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if a higher value is better, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.higher_is_better("channel1__dice")
      True
      >>> MultiChannelBinarySegmentationEvaluationScores.higher_is_better("channel1__f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:


      Determines whether or not to store the best weights/validation blocks for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if the best weights/validation blocks should be stored, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.store_best("channel1__dice")
      False
      >>> MultiChannelBinarySegmentationEvaluationScores.store_best("channel1__f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:


      Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: The lower and upper bounds for the criterion.
      :rtype: Tuple[Union[int, float, None], Union[int, float, None]]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.bounds("channel1__dice")
      (0, 1)
      >>> MultiChannelBinarySegmentationEvaluationScores.bounds("channel1__hausdorff")
      (0, nan)

      .. rubric:: Notes

      The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.



