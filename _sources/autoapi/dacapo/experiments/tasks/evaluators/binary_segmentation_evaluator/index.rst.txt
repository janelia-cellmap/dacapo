dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator
=================================================================

.. py:module:: dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator


Attributes
----------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.logger
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BG


Classes
-------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.Evaluator
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.MultiChannelBinarySegmentationEvaluationScores
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ZarrArray
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.BinarySegmentationEvaluator
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.ArrayEvaluator
   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.CremiEvaluator


Functions
---------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.binary_segmentation_evaluator.voi


Module Contents
---------------

.. py:function:: voi(reconstruction, groundtruth, ignore_reconstruction=[], ignore_groundtruth=[0])

   Return the conditional entropies of the variation of information metric. [1]

   Let X be a reconstruction, and Y a ground truth labelling. The variation of
   information between the two is the sum of two conditional entropies:

       VI(X, Y) = H(X|Y) + H(Y|X).

   The first one, H(X|Y), is a measure of oversegmentation, the second one,
   H(Y|X), a measure of undersegmentation. These measures are referred to as
   the variation of information split or merge error, respectively.

   :param seg: A candidate segmentation.
   :type seg: np.ndarray, int type, arbitrary shape
   :param gt: The ground truth segmentation.
   :type gt: np.ndarray, int type, same shape as `seg`
   :param ignore_seg: Any points having a label in this list are ignored in the evaluation.
                      By default, only the label 0 in the ground truth will be ignored.
   :type ignore_seg: list of int, optional
   :param ignore_gt: Any points having a label in this list are ignored in the evaluation.
                     By default, only the label 0 in the ground truth will be ignored.
   :type ignore_gt: list of int, optional

   :returns: **(split, merge)** -- The variation of information split and merge error, i.e., H(X|Y) and H(Y|X)
   :rtype: float

   :raises ValueError: If `reconstruction` and `groundtruth` have different shapes.

   .. rubric:: References

   [1] Meila, M. (2007). Comparing clusterings - an information based
   distance. Journal of Multivariate Analysis 98, 873-895.


.. py:class:: Evaluator



   Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.

   An evaluator takes a post-processor's output and compares it against
   ground-truth. It then returns a set of scores that can be used to
   determine the quality of the post-processor's output.

   .. attribute:: best_scores

      Dict[OutputIdentifier, BestScore]
      the best scores for each dataset/post-processing parameter/criterion combination

   .. method:: evaluate(output_array_identifier, evaluation_array)

      
      Compare and evaluate the output array against the evaluation array.

   .. method:: is_best(dataset, parameter, criterion, score)

      
      Check if the provided score is the best for this dataset/parameter/criterion combo.

   .. method:: get_overall_best(dataset, criterion)

      
      Return the best score for the given dataset and criterion.

   .. method:: get_overall_best_parameters(dataset, criterion)

      
      Return the best parameters for the given dataset and criterion.

   .. method:: compare(score_1, score_2, criterion)

      
      Compare two scores for the given criterion.

   .. method:: set_best(validation_scores)

      
      Find the best iteration for each dataset/post_processing_parameter/criterion.

   .. method:: higher_is_better(criterion)

      
      Return whether higher is better for the given criterion.

   .. method:: bounds(criterion)

      
      Return the bounds for the given criterion.

   .. method:: store_best(criterion)

      
      Return whether to store the best score for the given criterion.

   .. note:: The Evaluator class is used to compare and evaluate the output array against the evaluation array.


   .. py:method:: evaluate(output_array_identifier: dacapo.store.local_array_store.LocalArrayIdentifier, evaluation_array: dacapo.experiments.datasplits.datasets.arrays.Array) -> dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:


      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: LocalArrayIdentifier
                                      The identifier of the output array.
      :param evaluation_array: Array
                               The evaluation array.

      :returns:

                EvaluationScores
                    The evaluation scores.

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> output_array_identifier = LocalArrayIdentifier("output_array")
      >>> evaluation_array = Array()
      >>> evaluator.evaluate(output_array_identifier, evaluation_array)
      EvaluationScores()

      .. note:: This function is used to compare and evaluate the output array against the evaluation array.



   .. py:property:: best_scores
      :type: Dict[OutputIdentifier, BestScore]

      The best scores for each dataset/post-processing parameter/criterion combination.

      :returns:

                Dict[OutputIdentifier, BestScore]
                    the best scores for each dataset/post-processing parameter/criterion combination

      :raises AttributeError: if the best scores are not set

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.best_scores
      {}

      .. note:: This function is used to return the best scores for each dataset/post-processing parameter/criterion combination.


   .. py:method:: is_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, parameter: dacapo.experiments.tasks.post_processors.PostProcessorParameters, criterion: str, score: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores) -> bool

      Check if the provided score is the best for this dataset/parameter/criterion combo.

      :param dataset: Dataset
                      the dataset
      :param parameter: PostProcessorParameters
                        the post-processor parameters
      :param criterion: str
                        the criterion
      :param score: EvaluationScores
                    the evaluation scores

      :returns:

                bool
                    whether the provided score is the best for this dataset/parameter/criterion combo

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> parameter = PostProcessorParameters()
      >>> criterion = "criterion"
      >>> score = EvaluationScores()
      >>> evaluator.is_best(dataset, parameter, criterion, score)
      False

      .. note:: This function is used to check if the provided score is the best for this dataset/parameter/criterion combo.



   .. py:method:: get_overall_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)

      Return the best score for the given dataset and criterion.

      :param dataset: Dataset
                      the dataset
      :param criterion: str
                        the criterion

      :returns:

                Optional[float]
                    the best score for the given dataset and criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> criterion = "criterion"
      >>> evaluator.get_overall_best(dataset, criterion)
      None

      .. note:: This function is used to return the best score for the given dataset and criterion.



   .. py:method:: get_overall_best_parameters(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)

      Return the best parameters for the given dataset and criterion.

      :param dataset: Dataset
                      the dataset
      :param criterion: str
                        the criterion

      :returns:

                Optional[PostProcessorParameters]
                    the best parameters for the given dataset and criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> criterion = "criterion"
      >>> evaluator.get_overall_best_parameters(dataset, criterion)
      None

      .. note:: This function is used to return the best parameters for the given dataset and criterion.



   .. py:method:: compare(score_1, score_2, criterion)

      Compare two scores for the given criterion.

      :param score_1: float
                      the first score
      :param score_2: float
                      the second score
      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether the first score is better than the second score for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> score_1 = 0.0
      >>> score_2 = 0.0
      >>> criterion = "criterion"
      >>> evaluator.compare(score_1, score_2, criterion)
      False

      .. note:: This function is used to compare two scores for the given criterion.



   .. py:method:: set_best(validation_scores: dacapo.experiments.validation_scores.ValidationScores) -> None

      Find the best iteration for each dataset/post_processing_parameter/criterion.

      :param validation_scores: ValidationScores
                                the validation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> validation_scores = ValidationScores()
      >>> evaluator.set_best(validation_scores)
      None

      .. note::

         This function is used to find the best iteration for each dataset/post_processing_parameter/criterion.
         Typically, this function is called after the validation scores have been computed.



   .. py:property:: criteria
      :type: List[str]

      :abstractmethod:

      A list of all criteria for which a model might be "best". i.e. your
      criteria might be "precision", "recall", and "jaccard". It is unlikely
      that the best iteration/post processing parameters will be the same
      for all 3 of these criteria

      :returns:

                List[str]
                    the evaluation criteria

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.criteria
      []

      .. note:: This function is used to return the evaluation criteria.


   .. py:method:: higher_is_better(criterion: str) -> bool

      Wether or not higher is better for this criterion.

      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether higher is better for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.higher_is_better(criterion)
      False

      .. note:: This function is used to determine whether higher is better for the given criterion.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]

      The bounds for this criterion

      :param criterion: str
                        the criterion

      :returns:

                Tuple[Union[int, float, None], Union[int, float, None]]
                    the bounds for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.bounds(criterion)
      (0, 1)

      .. note:: This function is used to return the bounds for the given criterion.



   .. py:method:: store_best(criterion: str) -> bool

      The bounds for this criterion

      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether to store the best score for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.store_best(criterion)
      False

      .. note:: This function is used to return whether to store the best score for the given criterion.



   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores

      :abstractmethod:

      The evaluation scores.

      :returns:

                EvaluationScores
                    the evaluation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.score
      EvaluationScores()

      .. note:: This function is used to return the evaluation scores.


.. py:class:: BinarySegmentationEvaluationScores



   Class representing evaluation scores for binary segmentation tasks.

   The metrics include:
   - Dice coefficient: 2 * |A ∩ B| / |A| + |B| ; where A and B are the binary segmentations
   - Jaccard coefficient: |A ∩ B| / |A ∪ B| ; where A and B are the binary segmentations
   - Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
   - False negative rate: |A - B| / |A| ; where A and B are the binary segmentations
   - False positive rate: |B - A| / |B| ; where A and B are the binary segmentations
   - False discovery rate: |B - A| / |A| ; where A and B are the binary segmentations
   - VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
   - Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
   - Mean false negative distance: mean distance of false negatives
   - Mean false positive distance: mean distance of false positives
   - Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
   - Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
   - Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
   - Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
   - Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
   - F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
   - Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
   - Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
   - F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives

   .. attribute:: dice

      The Dice coefficient.

      :type: float

   .. attribute:: jaccard

      The Jaccard index.

      :type: float

   .. attribute:: hausdorff

      The Hausdorff distance.

      :type: float

   .. attribute:: false_negative_rate

      The false negative rate.

      :type: float

   .. attribute:: false_negative_rate_with_tolerance

      The false negative rate with tolerance.

      :type: float

   .. attribute:: false_positive_rate

      The false positive rate.

      :type: float

   .. attribute:: false_discovery_rate

      The false discovery rate.

      :type: float

   .. attribute:: false_positive_rate_with_tolerance

      The false positive rate with tolerance.

      :type: float

   .. attribute:: voi

      The variation of information.

      :type: float

   .. attribute:: mean_false_distance

      The mean false distance.

      :type: float

   .. attribute:: mean_false_negative_distance

      The mean false negative distance.

      :type: float

   .. attribute:: mean_false_positive_distance

      The mean false positive distance.

      :type: float

   .. attribute:: mean_false_distance_clipped

      The mean false distance clipped.

      :type: float

   .. attribute:: mean_false_negative_distance_clipped

      The mean false negative distance clipped.

      :type: float

   .. attribute:: mean_false_positive_distance_clipped

      The mean false positive distance clipped.

      :type: float

   .. attribute:: precision_with_tolerance

      The precision with tolerance.

      :type: float

   .. attribute:: recall_with_tolerance

      The recall with tolerance.

      :type: float

   .. attribute:: f1_score_with_tolerance

      The F1 score with tolerance.

      :type: float

   .. attribute:: precision

      The precision.

      :type: float

   .. attribute:: recall

      The recall.

      :type: float

   .. attribute:: f1_score

      The F1 score.

      :type: float

   .. method:: store_best(criterion

      str) -> bool: Whether or not to store the best weights/validation blocks for this criterion.

   .. method:: higher_is_better(criterion

      str) -> bool: Determines whether a higher value is better for a given criterion.

   .. method:: bounds(criterion

      str) -> Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.

   .. rubric:: Notes

   The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.


   .. py:attribute:: dice
      :type:  float


   .. py:attribute:: jaccard
      :type:  float


   .. py:attribute:: hausdorff
      :type:  float


   .. py:attribute:: false_negative_rate
      :type:  float


   .. py:attribute:: false_negative_rate_with_tolerance
      :type:  float


   .. py:attribute:: false_positive_rate
      :type:  float


   .. py:attribute:: false_discovery_rate
      :type:  float


   .. py:attribute:: false_positive_rate_with_tolerance
      :type:  float


   .. py:attribute:: voi
      :type:  float


   .. py:attribute:: mean_false_distance
      :type:  float


   .. py:attribute:: mean_false_negative_distance
      :type:  float


   .. py:attribute:: mean_false_positive_distance
      :type:  float


   .. py:attribute:: mean_false_distance_clipped
      :type:  float


   .. py:attribute:: mean_false_negative_distance_clipped
      :type:  float


   .. py:attribute:: mean_false_positive_distance_clipped
      :type:  float


   .. py:attribute:: precision_with_tolerance
      :type:  float


   .. py:attribute:: recall_with_tolerance
      :type:  float


   .. py:attribute:: f1_score_with_tolerance
      :type:  float


   .. py:attribute:: precision
      :type:  float


   .. py:attribute:: recall
      :type:  float


   .. py:attribute:: f1_score
      :type:  float


   .. py:attribute:: criteria
      :value: ['dice', 'jaccard', 'hausdorff', 'false_negative_rate', 'false_negative_rate_with_tolerance',...



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:


      Determines whether or not to store the best weights/validation blocks for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if the best weights/validation blocks should be stored, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.store_best("dice")
      False
      >>> BinarySegmentationEvaluationScores.store_best("f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.



   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:


      Determines whether a higher value is better for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if a higher value is better, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.higher_is_better("dice")
      True
      >>> BinarySegmentationEvaluationScores.higher_is_better("f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:


      Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: The lower and upper bounds for the criterion.
      :rtype: Tuple[Union[int, float, None], Union[int, float, None]]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> BinarySegmentationEvaluationScores.bounds("dice")
      (0, 1)
      >>> BinarySegmentationEvaluationScores.bounds("hausdorff")
      (0, nan)

      .. rubric:: Notes

      The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.



.. py:class:: MultiChannelBinarySegmentationEvaluationScores



   Class representing evaluation scores for multi-channel binary segmentation tasks.

   .. attribute:: channel_scores

      The list of channel scores.

      :type: List[Tuple[str, BinarySegmentationEvaluationScores]]

   .. method:: higher_is_better(criterion

      str) -> bool: Determines whether a higher value is better for a given criterion.

   .. method:: store_best(criterion

      str) -> bool: Whether or not to store the best weights/validation blocks for this criterion.

   .. method:: bounds(criterion

      str) -> Tuple[Union[int, float, None], Union[int, float, None]]: Determines the bounds for a given criterion.

   .. rubric:: Notes

   The evaluation scores are stored as attributes of the class. The class also contains methods to determine whether a higher value is better for a given criterion, whether or not to store the best weights/validation blocks for a given criterion, and the bounds for a given criterion.


   .. py:attribute:: channel_scores
      :type:  List[Tuple[str, BinarySegmentationEvaluationScores]]


   .. py:property:: criteria
      Returns a list of all criteria for all channels.

      :returns: The list of criteria.
      :rtype: List[str]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> channel_scores = [("channel1", BinarySegmentationEvaluationScores()), ("channel2", BinarySegmentationEvaluationScores())]
      >>> MultiChannelBinarySegmentationEvaluationScores(channel_scores).criteria

      .. rubric:: Notes

      The method returns a list of all criteria for all channels. The criteria are stored as attributes of the class.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:


      Determines whether a higher value is better for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if a higher value is better, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.higher_is_better("channel1__dice")
      True
      >>> MultiChannelBinarySegmentationEvaluationScores.higher_is_better("channel1__f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether a higher value is better for a given criterion is determined by the mapping dictionary.



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:


      Determines whether or not to store the best weights/validation blocks for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: True if the best weights/validation blocks should be stored, False otherwise.
      :rtype: bool

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.store_best("channel1__dice")
      False
      >>> MultiChannelBinarySegmentationEvaluationScores.store_best("channel1__f1_score")
      True

      .. rubric:: Notes

      The method returns True if the criterion is recognized and False otherwise. Whether or not to store the best weights/validation blocks for a given criterion is determined by the mapping dictionary.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:


      Determines the bounds for a given criterion. The bounds are used to determine the best value for a given criterion.

      :param criterion: The evaluation criterion.
      :type criterion: str

      :returns: The lower and upper bounds for the criterion.
      :rtype: Tuple[Union[int, float, None], Union[int, float, None]]

      :raises ValueError: If the criterion is not recognized.

      .. rubric:: Examples

      >>> MultiChannelBinarySegmentationEvaluationScores.bounds("channel1__dice")
      (0, 1)
      >>> MultiChannelBinarySegmentationEvaluationScores.bounds("channel1__hausdorff")
      (0, nan)

      .. rubric:: Notes

      The method returns the lower and upper bounds for the criterion. The bounds are determined by the mapping dictionary.



.. py:class:: ZarrArray(array_config)



   This is a zarr array.

   .. attribute:: name

      The name of the array.

      :type: str

   .. attribute:: file_name

      The file name of the array.

      :type: Path

   .. attribute:: dataset

      The dataset name.

      :type: str

   .. attribute:: _axes

      The axes of the array.

      :type: Optional[List[str]]

   .. attribute:: snap_to_grid

      The snap to grid.

      :type: Optional[Coordinate]

   .. method:: __init__(array_config)

      
      Initializes the array type 'raw' and name for the DummyDataset instance.

   .. method:: __str__()

      
      Returns the string representation of the ZarrArray.

   .. method:: __repr__()

      
      Returns the string representation of the ZarrArray.

   .. method:: attrs()

      
      Returns the attributes of the array.

   .. method:: axes()

      
      Returns the axes of the array.

   .. method:: dims()

      
      Returns the dimensions of the array.

   .. method:: _daisy_array()

      
      Returns the daisy array.

   .. method:: voxel_size()

      
      Returns the voxel size of the array.

   .. method:: roi()

      
      Returns the region of interest of the array.

   .. method:: writable()

      
      Returns the boolean value of the array.

   .. method:: dtype()

      
      Returns the data type of the array.

   .. method:: num_channels()

      
      Returns the number of channels of the array.

   .. method:: spatial_axes()

      
      Returns the spatial axes of the array.

   .. method:: data()

      
      Returns the data of the array.

   .. method:: __getitem__(roi)

      
      Returns the data of the array for the given region of interest.

   .. method:: __setitem__(roi, value)

      
      Sets the data of the array for the given region of interest.

   .. method:: create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, write_size=None, name=None, overwrite=False)

      
      Creates a new ZarrArray given an array identifier.

   .. method:: open_from_array_identifier(array_identifier, name="")

      
      Opens a new ZarrArray given an array identifier.

   .. method:: _can_neuroglance()

      
      Returns the boolean value of the array.

   .. method:: _neuroglancer_source()

      
      Returns the neuroglancer source of the array.

   .. method:: _neuroglancer_layer()

      
      Returns the neuroglancer layer of the array.

   .. method:: _transform_matrix()

      
      Returns the transform matrix of the array.

   .. method:: _output_dimensions()

      
      Returns the output dimensions of the array.

   .. method:: _source_name()

      
      Returns the source name of the array.

   .. method:: add_metadata(metadata)

      
      Adds metadata to the array.

   .. rubric:: Notes

   This class is used to create a zarr array.


   .. py:property:: mode


   .. py:property:: attrs
      Returns the attributes of the array.

      :param attrs: The attributes of the array.
      :type attrs: Any

      :returns: The attributes of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> attrs()

      .. rubric:: Notes

      This method is used to return the attributes of the array.


   .. py:property:: axes
      Returns the axes of the array.

      :param axes: The axes of the array.
      :type axes: List[str]

      :returns: The axes of the array.
      :rtype: List[str]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> axes()

      .. rubric:: Notes

      This method is used to return the axes of the array.


   .. py:property:: dims
      :type: int

      Returns the dimensions of the array.

      :param dims: The dimensions of the array.
      :type dims: int

      :returns: The dimensions of the array.
      :rtype: int

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> dims()

      .. rubric:: Notes

      This method is used to return the dimensions of the array.


   .. py:method:: voxel_size() -> funlib.geometry.Coordinate

      Returns the voxel size of the array.

      :param voxel_size: The voxel size.
      :type voxel_size: Coordinate

      :returns: The voxel size of the array.
      :rtype: Coordinate

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> voxel_size()

      .. rubric:: Notes

      This method is used to return the voxel size of the array.



   .. py:method:: roi() -> funlib.geometry.Roi

      Returns the region of interest of the array.

      :param roi: The region of interest.
      :type roi: Roi

      :returns: The region of interest of the array.
      :rtype: Roi

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> roi()

      .. rubric:: Notes

      This method is used to return the region of interest of the array.



   .. py:property:: writable
      :type: bool

      Returns the boolean value of the array.

      :param writable: The boolean value of the array.
      :type writable: bool

      :returns: The boolean value of the array.
      :rtype: bool

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> writable()

      .. rubric:: Notes

      This method is used to return the boolean value of the array.


   .. py:property:: dtype
      :type: Any

      Returns the data type of the array.

      :param dtype: The data type of the array.
      :type dtype: Any

      :returns: The data type of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> dtype()

      .. rubric:: Notes

      This method is used to return the data type of the array.


   .. py:property:: num_channels
      :type: Optional[int]

      Returns the number of channels of the array.

      :param num_channels: The number of channels of the array.
      :type num_channels: Optional[int]

      :returns: The number of channels of the array.
      :rtype: Optional[int]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> num_channels()

      .. rubric:: Notes

      This method is used to return the number of channels of the array.


   .. py:property:: spatial_axes
      :type: List[str]

      Returns the spatial axes of the array.

      :param spatial_axes: The spatial axes of the array.
      :type spatial_axes: List[str]

      :returns: The spatial axes of the array.
      :rtype: List[str]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> spatial_axes()

      .. rubric:: Notes

      This method is used to return the spatial axes of the array.


   .. py:property:: data
      :type: Any

      Returns the data of the array.

      :param data: The data of the array.
      :type data: Any

      :returns: The data of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> data()

      .. rubric:: Notes

      This method is used to return the data of the array.


   .. py:method:: create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, mode='a', write_size=None, name=None, overwrite=False)
      :classmethod:


      Create a new ZarrArray given an array identifier. It is assumed that
      this array_identifier points to a dataset that does not yet exist.

      :param array_identifier: The array identifier.
      :type array_identifier: ArrayIdentifier
      :param axes: The axes of the array.
      :type axes: List[str]
      :param roi: The region of interest.
      :type roi: Roi
      :param num_channels: The number of channels.
      :type num_channels: int
      :param voxel_size: The voxel size.
      :type voxel_size: Coordinate
      :param dtype: The data type.
      :type dtype: Any
      :param write_size: The write size.
      :type write_size: Optional[Coordinate]
      :param name: The name of the array.
      :type name: Optional[str]
      :param overwrite: The boolean value to overwrite the array.
      :type overwrite: bool

      :returns: The ZarrArray.
      :rtype: ZarrArray

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, write_size=None, name=None, overwrite=False)

      .. rubric:: Notes

      This method is used to create a new ZarrArray given an array identifier.



   .. py:method:: open_from_array_identifier(array_identifier, name='')
      :classmethod:


      Opens a new ZarrArray given an array identifier.

      :param array_identifier: The array identifier.
      :type array_identifier: ArrayIdentifier
      :param name: The name of the array.
      :type name: str

      :returns: The ZarrArray.
      :rtype: ZarrArray

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> open_from_array_identifier(array_identifier, name="")

      .. rubric:: Notes

      This method is used to open a new ZarrArray given an array identifier.



   .. py:method:: add_metadata(metadata: Dict[str, Any]) -> None

      Adds metadata to the array.

      :param metadata: The metadata to add to the array.
      :type metadata: Dict[str, Any]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> add_metadata(metadata)

      .. rubric:: Notes

      This method is used to add metadata to the array.



.. py:data:: logger

.. py:data:: BG
   :value: 0


.. py:class:: BinarySegmentationEvaluator(clip_distance: float, tol_distance: float, channels: List[str])



   Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:
   - Dice coefficient: 2 * |A ∩ B| / |A| + |B| ; where A and B are the binary segmentations
   - Jaccard coefficient: |A ∩ B| / |A ∪ B| ; where A and B are the binary segmentations
   - Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
   - False negative rate: |A - B| / |A| ; where A and B are the binary segmentations
   - False positive rate: |B - A| / |B| ; where A and B are the binary segmentations
   - False discovery rate: |B - A| / |A| ; where A and B are the binary segmentations
   - VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
   - Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
   - Mean false negative distance: mean distance of false negatives
   - Mean false positive distance: mean distance of false positives
   - Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
   - Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
   - Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
   - Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
   - Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
   - F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
   - Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
   - Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
   - F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives

   .. attribute:: clip_distance

      float
      the clip distance

   .. attribute:: tol_distance

      float
      the tolerance distance

   .. attribute:: channels

      List[str]
      the channels

   .. attribute:: criteria

      List[str]
      the evaluation criteria

   .. method:: evaluate(output_array_identifier, evaluation_array)

      
      Evaluate the output array against the evaluation array.

   .. method:: score

      
      Return the evaluation scores.

   .. note::

      The BinarySegmentationEvaluator class is used to evaluate the performance of a binary segmentation task.
      The class provides methods to evaluate the output array against the evaluation array and return the evaluation scores.
      All evaluation scores should inherit from this class.
      
      Clip distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a false positive.
      Tolerance distance is the maximum distance between the ground truth and the predicted segmentation for a pixel to be considered a true positive.
      Channels are the channels of the binary segmentation.
      Criteria are the evaluation criteria.


   .. py:attribute:: criteria
      :value: ['jaccard', 'voi']



   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Evaluate the output array against the evaluation array.

      :param output_array_identifier: str
                                      the identifier of the output array
      :param evaluation_array: ZarrArray
                               the evaluation array

      :returns:

                BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores
                    the evaluation scores

      :raises ValueError: if the output array identifier is not valid

      .. rubric:: Examples

      >>> binary_segmentation_evaluator = BinarySegmentationEvaluator(clip_distance=200, tol_distance=40, channels=["channel1", "channel2"])
      >>> output_array_identifier = "output_array"
      >>> evaluation_array = ZarrArray.open_from_array_identifier("evaluation_array")
      >>> binary_segmentation_evaluator.evaluate(output_array_identifier, evaluation_array)
      BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)

      .. note:: This function is used to evaluate the output array against the evaluation array.



   .. py:property:: score
      Return the evaluation scores.

      :returns:

                BinarySegmentationEvaluationScores or MultiChannelBinarySegmentationEvaluationScores
                    the evaluation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> binary_segmentation_evaluator = BinarySegmentationEvaluator(clip_distance=200, tol_distance=40, channels=["channel1", "channel2"])
      >>> binary_segmentation_evaluator.score
      BinarySegmentationEvaluationScores(dice=0.0, jaccard=0.0, hausdorff=0.0, false_negative_rate=0.0, false_positive_rate=0.0, false_discovery_rate=0.0, voi=0.0, mean_false_distance=0.0, mean_false_negative_distance=0.0, mean_false_positive_distance=0.0, mean_false_distance_clipped=0.0, mean_false_negative_distance_clipped=0.0, mean_false_positive_distance_clipped=0.0, precision_with_tolerance=0.0, recall_with_tolerance=0.0, f1_score_with_tolerance=0.0, precision=0.0, recall=0.0, f1_score=0.0)

      .. note:: This function is used to return the evaluation scores.


.. py:class:: ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)

   Given a binary segmentation, compute various metrics to determine their similarity. The metrics include:
   - Dice coefficient: 2 * |A ∩ B| / |A| + |B| ; where A and B are the binary segmentations
   - Jaccard coefficient: |A ∩ B| / |A ∪ B| ; where A and B are the binary segmentations
   - Hausdorff distance: max(h(A, B), h(B, A)) ; where h(A, B) is the Hausdorff distance between A and B
   - False negative rate: |A - B| / |A| ; where A and B are the binary segmentations
   - False positive rate: |B - A| / |B| ; where A and B are the binary segmentations
   - False discovery rate: |B - A| / |A| ; where A and B are the binary segmentations
   - VOI: Variation of Information; split and merge errors combined into a single measure of segmentation quality
   - Mean false distance: 0.5 * (mean false positive distance + mean false negative distance)
   - Mean false negative distance: mean distance of false negatives
   - Mean false positive distance: mean distance of false positives
   - Mean false distance clipped: 0.5 * (mean false positive distance clipped + mean false negative distance clipped) ; clipped to a maximum distance
   - Mean false negative distance clipped: mean distance of false negatives clipped ; clipped to a maximum distance
   - Mean false positive distance clipped: mean distance of false positives clipped ; clipped to a maximum distance
   - Precision with tolerance: TP / (TP + FP) ; where TP and FP are the true and false positives within a tolerance distance
   - Recall with tolerance: TP / (TP + FN) ; where TP and FN are the true and false positives within a tolerance distance
   - F1 score with tolerance: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives within a tolerance distance
   - Precision: TP / (TP + FP) ; where TP and FP are the true and false positives
   - Recall: TP / (TP + FN) ; where TP and FN are the true and false positives
   - F1 score: 2 * (Recall * Precision) / (Recall + Precision) ; where Recall and Precision are the true and false positives

   .. attribute:: truth

      np.ndarray
      the truth binary segmentation

   .. attribute:: test

      np.ndarray
      the test binary segmentation

   .. attribute:: truth_empty

      bool
      whether the truth binary segmentation is empty

   .. attribute:: test_empty

      bool
      whether the test binary segmentation is empty

   .. attribute:: cremieval

      CremiEvaluator
      the cremi evaluator

   .. attribute:: resolution

      Tuple[float, float, float]
      the resolution

   .. method:: dice

      
      Return the Dice coefficient.

   .. method:: jaccard

      
      Return the Jaccard coefficient.

   .. method:: hausdorff

      
      Return the Hausdorff distance.

   .. method:: false_negative_rate

      
      Return the false negative rate.

   .. method:: false_positive_rate

      
      Return the false positive rate.

   .. method:: false_discovery_rate

      
      Return the false discovery rate.

   .. method:: precision

      
      Return the precision.

   .. method:: recall

      
      Return the recall.

   .. method:: f1_score

      
      Return the F1 score.

   .. method:: voi

      
      Return the VOI.

   .. method:: mean_false_distance

      
      Return the mean false distance.

   .. method:: mean_false_negative_distance

      
      Return the mean false negative distance.

   .. method:: mean_false_positive_distance

      
      Return the mean false positive distance.

   .. method:: mean_false_distance_clipped

      
      Return the mean false distance clipped.

   .. method:: mean_false_negative_distance_clipped

      
      Return the mean false negative distance clipped.

   .. method:: mean_false_positive_distance_clipped

      
      Return the mean false positive distance clipped.

   .. method:: false_positive_rate_with_tolerance

      
      Return the false positive rate with tolerance.

   .. method:: false_negative_rate_with_tolerance

      
      Return the false negative rate with tolerance.

   .. method:: precision_with_tolerance

      
      Return the precision with tolerance.

   .. method:: recall_with_tolerance

      
      Return the recall with tolerance.

   .. method:: f1_score_with_tolerance

      
      Return the F1 score with tolerance.

   .. note::

      The ArrayEvaluator class is used to evaluate the performance of a binary segmentation task.
      The class provides methods to evaluate the truth binary segmentation against the test binary segmentation.
      All evaluation scores should inherit from this class.


   .. py:method:: truth_itk()

      A SimpleITK image of the truth binary segmentation.

      :returns:

                sitk.Image
                    the truth binary segmentation as a SimpleITK image

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.truth_itk
      <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'std::vector< itk::simple::Image >::value_type *' at 0x7f8b1c0b3f30> >

      .. note:: This function is used to return the truth binary segmentation as a SimpleITK image.



   .. py:method:: test_itk()

      A SimpleITK image of the test binary segmentation.

      :param test: np.ndarray
                   the test binary segmentation
      :param resolution: Tuple[float, float, float]
                         the resolution

      :returns:

                sitk.Image
                    the test binary segmentation as a SimpleITK image

      :raises ValueError: if the test binary segmentation is not valid

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.test_itk
      <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'std::vector< itk::simple::Image >::value_type *' at 0x7f8b1c0b3f30> >

      .. note:: This function is used to return the test binary segmentation as a SimpleITK image.



   .. py:method:: overlap_measures_filter()

      A SimpleITK filter to compute overlap measures.

      :param truth_itk: sitk.Image
                        the truth binary segmentation as a SimpleITK image
      :param test_itk: sitk.Image
                       the test binary segmentation as a SimpleITK image

      :returns:

                sitk.LabelOverlapMeasuresImageFilter
                    the overlap measures filter

      :raises ValueError: if the truth binary segmentation or the test binary segmentation is not valid

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.overlap_measures_filter
      <SimpleITK.SimpleITK.LabelOverlapMeasuresImageFilter; proxy of <Swig Object of type 'itk::simple::LabelOverlapMeasuresImageFilter *' at 0x7f8b1c0b3f30> >

      .. note:: This function is used to return the overlap measures filter.



   .. py:method:: dice()

      The Dice coefficient.

      :param truth_itk: sitk.Image
                        the truth binary segmentation as a SimpleITK image
      :param test_itk: sitk.Image
                       the test binary segmentation as a SimpleITK image

      :returns:

                float
                    the Dice coefficient

      :raises ValueError: if the truth binary segmentation or the test binary segmentation is not valid

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.dice()
      0.0

      .. note:: This function is used to return the Dice coefficient.



   .. py:method:: jaccard()

      The Jaccard coefficient.

      :param truth_itk: sitk.Image
                        the truth binary segmentation as a SimpleITK image
      :param test_itk: sitk.Image
                       the test binary segmentation as a SimpleITK image

      :returns:

                float
                    the Jaccard coefficient

      :raises ValueError: if the truth binary segmentation or the test binary segmentation is not valid

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.jaccard()
      0.0

      .. note:: This function is used to return the Jaccard coefficient.



   .. py:method:: hausdorff()

      The Hausdorff distance.

      :param None:

      :returns: the Hausdorff distance
      :rtype: float

      :raises None:

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.hausdorff()
      0.0

      .. note::

         This function is used to return the Hausdorff distance between the truth binary segmentation and the test binary segmentation.
         
         If either the truth or test binary segmentation is empty, the function returns 0.
         Otherwise, it calculates the Hausdorff distance using the HausdorffDistanceImageFilter from the SimpleITK library.



   .. py:method:: false_negative_rate()

      The false negative rate.

      :returns:

                float
                    the false negative rate

      :returns: if the truth binary segmentation or the test binary segmentation is not valid
      :rtype: ValueError

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.false_negative_rate()
      0.0

      .. note:: This function is used to return the false negative rate.



   .. py:method:: false_positive_rate()

      The false positive rate.

      :param truth_itk: sitk.Image
                        the truth binary segmentation as a SimpleITK image
      :param test_itk: sitk.Image
                       the test binary segmentation as a SimpleITK image

      :returns:

                float
                    the false positive rate

      :raises ValueError: if the truth binary segmentation or the test binary segmentation is not valid

      .. rubric:: Examples

      >>> array_evaluator = ArrayEvaluator(truth_binary, test_binary, truth_empty, test_empty, metric_params, resolution)
      >>> array_evaluator.false_positive_rate()
      0.0

      .. note:: This function is used to return the false positive rate.



   .. py:method:: false_discovery_rate()

      Calculate the false discovery rate (FDR) for the binary segmentation evaluation.

      :returns: The false discovery rate.
      :rtype: float

      :raises None:

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_discovery_rate()
      0.25

      .. note::

         The false discovery rate is a measure of the proportion of false positives among the predicted positive samples.
         It is calculated as the ratio of false positives to the sum of true positives and false positives.
         If either the ground truth or the predicted segmentation is empty, the FDR is set to NaN.



   .. py:method:: precision()

      Calculate the precision of the binary segmentation evaluation.

      :returns: The precision value.
      :rtype: float

      :raises None.:

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.precision()
      0.75

      .. note::

         Precision is a measure of the accuracy of the positive predictions made by the model.
         It is calculated as the ratio of true positives to the total number of positive predictions.
         If either the ground truth or the predicted values are empty, the precision value will be NaN.



   .. py:method:: recall()

      Calculate the recall metric for binary segmentation evaluation.

      :returns: The recall value.
      :rtype: float

      :raises None:

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.recall()
      0.75

      .. note::

         Recall is a measure of the ability of a binary classifier to identify all positive samples.
         It is calculated as the ratio of true positives to the total number of actual positives.



   .. py:method:: f1_score()

      Calculate the F1 score for binary segmentation evaluation.

      :returns: The F1 score value.
      :rtype: float

      :raises None.:

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.f1_score()
      0.75

      .. note::

         The F1 score is the harmonic mean of precision and recall.
         It is a measure of the balance between precision and recall, providing a single metric to evaluate the model's performance.
         
         If either the ground truth or the predicted values are empty, the F1 score will be NaN.



   .. py:method:: voi()

      Calculate the Variation of Information (VOI) for binary segmentation evaluation.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The VOI value.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.voi()
      0.75

      .. note::

         The VOI is a measure of the similarity between two segmentations.
         It combines the split and merge errors into a single measure of segmentation quality.
         If either the ground truth or the predicted values are empty, the VOI will be NaN.



   .. py:method:: mean_false_distance()

      Calculate the mean false distance between the ground truth and the test results.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false distance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_distance()
      0.25

      .. note::

         - This method returns np.nan if either the ground truth or the test results are empty.
         - The mean false distance is a measure of the average distance between the false positive pixels in the test results and the nearest true positive pixels in the ground truth.



   .. py:method:: mean_false_negative_distance()

      Calculate the mean false negative distance between the ground truth and the test results.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false negative distance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_negative_distance()
      0.25

      .. note:: This method returns np.nan if either the ground truth or the test results are empty.



   .. py:method:: mean_false_positive_distance()

      Calculate the mean false positive distance.

      This method calculates the mean false positive distance between the ground truth and the test results.
      If either the ground truth or the test results are empty, the method returns NaN.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false positive distance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_positive_distance()
      0.5

      .. note::

         The mean false positive distance is a measure of the average distance between false positive pixels in the
         test results and the corresponding ground truth pixels. It is commonly used to evaluate the performance of
         binary segmentation algorithms.



   .. py:method:: mean_false_distance_clipped()

      Calculate the mean false distance (clipped) between the ground truth and the test results.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false distance (clipped) value.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_distance_clipped()
      0.123

      .. note:: This method returns np.nan if either the ground truth or the test results are empty.



   .. py:method:: mean_false_negative_distance_clipped()

      Calculate the mean false negative distance, with clipping.

      This method calculates the mean false negative distance between the ground truth and the test results.
      The distance is clipped to avoid extreme values.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false negative distance with clipping.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_negative_distance_clipped()
      0.123

      .. note::

         - The mean false negative distance is a measure of the average distance between the false negative pixels in the ground truth and the test results.
         - Clipping the distance helps to avoid extreme values that may skew the overall evaluation.



   .. py:method:: mean_false_positive_distance_clipped()

      Calculate the mean false positive distance, with clipping.

      This method calculates the mean false positive distance between the ground truth and the test results,
      taking into account any clipping that may have been applied.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The mean false positive distance with clipping.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_positive_distance_clipped()
      0.25

      .. note::

         - The mean false positive distance is a measure of the average distance between false positive pixels
           in the test results and the corresponding ground truth pixels.
         - If either the ground truth or the test results are empty, the method returns NaN.



   .. py:method:: false_positive_rate_with_tolerance()

      Calculate the false positive rate with tolerance.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The false positive rate with tolerance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_positive_rate_with_tolerance()
      0.25

      .. note::

         This method calculates the false positive rate with tolerance by comparing the truth and test data.
         If either the truth or test data is empty, it returns NaN.



   .. py:method:: false_negative_rate_with_tolerance()

      Calculate the false negative rate with tolerance.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The false negative rate with tolerance as a floating-point number.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_negative_rate_with_tolerance()
      0.25

      .. note:: This method calculates the false negative rate with tolerance, which is a measure of the proportion of false negatives in a binary segmentation evaluation. If either the ground truth or the test data is empty, the method returns NaN.



   .. py:method:: precision_with_tolerance()

      Calculate the precision with tolerance.

      This method calculates the precision with tolerance by comparing the truth and test data.
      Precision is the ratio of true positives to the sum of true positives and false positives.
      Tolerance is a distance threshold within which two pixels are considered to be a match.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The precision with tolerance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.precision_with_tolerance()
      0.75

      .. note::

         - Precision is a measure of the accuracy of the positive predictions.
         - If either the ground truth or the test data is empty, the method returns NaN.



   .. py:method:: recall_with_tolerance()

      Calculate the recall with tolerance for the binary segmentation evaluator.

      :returns: The recall with tolerance value.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.recall_with_tolerance()
      0.75

      .. note:: This method calculates the recall with tolerance, which is a measure of how well the binary segmentation evaluator performs. It returns the recall with tolerance value as a float. If either the truth or test data is empty, it returns NaN.



   .. py:method:: f1_score_with_tolerance()

      Calculate the F1 score with tolerance.

      This method calculates the F1 score with tolerance between the ground truth and the test results.
      If either the ground truth or the test results are empty, the function returns NaN.

      :param truth: np.ndarray
                    the truth binary segmentation
      :param test: np.ndarray
                   the test binary segmentation

      :returns: The F1 score with tolerance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.f1_score_with_tolerance()
      0.85

      .. note::

         The F1 score is a measure of a test's accuracy. It considers both the precision and recall of the test to compute the score.
         The tolerance parameter allows for a certain degree of variation between the ground truth and the test results.



.. py:class:: CremiEvaluator(truth, test, sampling=(1, 1, 1), clip_distance=200, tol_distance=40)

   Evaluate the performance of a binary segmentation task using the CREMI score.
   The CREMI score is a measure of the similarity between two binary segmentations.

   .. attribute:: truth

      np.ndarray
      the truth binary segmentation

   .. attribute:: test

      np.ndarray
      the test binary segmentation

   .. attribute:: sampling

      Tuple[float, float, float]
      the sampling resolution

   .. attribute:: clip_distance

      float
      the maximum distance to clip

   .. attribute:: tol_distance

      float
      the tolerance distance

   .. method:: false_positive_distances

      
      Return the false positive distances.

   .. method:: false_positives_with_tolerance

      
      Return the false positives with tolerance.

   .. method:: false_positive_rate_with_tolerance

      
      Return the false positive rate with tolerance.

   .. method:: false_negatives_with_tolerance

      
      Return the false negatives with tolerance.

   .. method:: false_negative_rate_with_tolerance

      
      Return the false negative rate with tolerance.

   .. method:: true_positives_with_tolerance

      
      Return the true positives with tolerance.

   .. method:: precision_with_tolerance

      
      Return the precision with tolerance.

   .. method:: recall_with_tolerance

      
      Return the recall with tolerance.

   .. method:: f1_score_with_tolerance

      
      Return the F1 score with tolerance.

   .. method:: mean_false_positive_distances_clipped

      
      Return the mean false positive distances clipped.

   .. method:: mean_false_negative_distances_clipped

      
      Return the mean false negative distances clipped.

   .. method:: mean_false_positive_distance

      
      Return the mean false positive distance.

   .. method:: false_negative_distances

      
      Return the false negative distances.

   .. method:: mean_false_negative_distance

      
      Return the mean false negative distance.

   .. method:: mean_false_distance

      
      Return the mean false distance.

   .. method:: mean_false_distance_clipped

      
      Return the mean false distance clipped.

   .. note::

      - The CremiEvaluator class is used to evaluate the performance of a binary segmentation task using the CREMI score.
      - True and test binary segmentations are compared to calculate various evaluation metrics.
      - The class provides methods to evaluate the performance of the binary segmentation task.
      - Toleration distance is used to determine the tolerance level for the evaluation.
      - Clip distance is used to clip the distance values to avoid extreme values.
      - All evaluation scores should inherit from this class.


   .. py:method:: test_mask()

      Generate a binary mask for the test data.

      :param test: np.ndarray
                   the test binary segmentation

      :returns: A binary mask indicating the regions of interest in the test data.
      :rtype: test_mask (ndarray)

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.test = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])
      >>> evaluator.test_mask()
      array([[False,  True, False],
              [ True,  True,  True],
              [False,  True, False]])

      .. note:: This method assumes that the background class is represented by the constant `BG`.



   .. py:method:: truth_mask()

      Returns a binary mask indicating the truth values.

      :param truth: np.ndarray
                    the truth binary segmentation

      :returns: A binary mask where True indicates the truth values and False indicates other values.
      :rtype: truth_mask (ndarray)

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> mask = evaluator.truth_mask()
      >>> print(mask)
      [[ True  True False]
          [False  True False]
          [ True False False]]

      .. note:: The truth mask is computed by comparing the truth values with a predefined background value (BG).



   .. py:method:: test_edt()

      Calculate the Euclidean Distance Transform (EDT) of the test mask.

      :param self.test_mask: The binary test mask.
      :type self.test_mask: ndarray
      :param self.sampling: The pixel spacing or sampling along each dimension.
      :type self.sampling: float or sequence of floats

      :returns: The Euclidean Distance Transform of the test mask.
      :rtype: ndarray

      .. rubric:: Examples

      # Example 1:
      test_mask = np.array([[0, 0, 1],
                            [1, 1, 1],
                            [0, 0, 0]])
      sampling = 1.0
      result = test_edt(test_mask, sampling)
      # Output: array([[1.        , 1.        , 0.        ],
      #                [0.        , 0.        , 0.        ],
      #                [1.        , 1.        , 1.41421356]])

      # Example 2:
      test_mask = np.array([[0, 1, 0],
                            [1, 0, 1],
                            [0, 1, 0]])
      sampling = 0.5
      result = test_edt(test_mask, sampling)
      # Output: array([[0.5       , 0.        , 0.5       ],
      #                [0.        , 0.70710678, 0.        ],
      #                [0.5       , 0.        , 0.5       ]])

      .. note:: The Euclidean Distance Transform (EDT) calculates the distance from each pixel in the binary mask to the nearest boundary pixel. It is commonly used in image processing and computer vision tasks, such as edge detection and shape analysis.



   .. py:method:: truth_edt()

      Calculate the Euclidean Distance Transform (EDT) of the ground truth mask.

      :param self.truth_mask: The binary ground truth mask.
      :type self.truth_mask: ndarray
      :param self.sampling: The pixel spacing or sampling along each dimension.
      :type self.sampling: float or sequence of floats

      :returns: The Euclidean Distance Transform of the ground truth mask.
      :rtype: ndarray

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> edt = evaluator.truth_edt()

      .. note:: The Euclidean Distance Transform (EDT) calculates the distance from each pixel in the binary mask to the nearest boundary pixel. It is commonly used in image processing and computer vision tasks.



   .. py:method:: false_positive_distances()

      Calculate the distances of false positive pixels from the ground truth segmentation.

      :param self.test_mask: The binary test mask.
      :type self.test_mask: ndarray
      :param self.truth_edt: The Euclidean Distance Transform of the ground truth segmentation.
      :type self.truth_edt: ndarray

      :returns: An array containing the distances of false positive pixels from the ground truth segmentation.
      :rtype: numpy.ndarray

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> distances = evaluator.false_positive_distances()
      >>> print(distances)
      [1.2, 0.8, 2.5, 1.0]

      .. note::

         This method assumes that the ground truth segmentation and the test mask have been initialized.
         The ground truth segmentation is stored in the `truth_edt` attribute, and the test mask is obtained by inverting the `test_mask` attribute.



   .. py:method:: false_positives_with_tolerance()

      Calculate the number of false positives with a given tolerance distance.

      :param self.false_positive_distances: The distances of false positive pixels from the ground truth segmentation.
      :type self.false_positive_distances: ndarray
      :param self.tol_distance: The tolerance distance.
      :type self.tol_distance: float

      :returns: The number of false positives with a distance greater than the tolerance distance.
      :rtype: int

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_positive_distances = [1, 2, 3]
      >>> evaluator.tol_distance = 2
      >>> false_positives = evaluator.false_positives_with_tolerance()
      >>> print(false_positives)
      1

      .. note:: The `false_positive_distances` attribute should be initialized before calling this method.



   .. py:method:: false_positive_rate_with_tolerance()

      Calculate the false positive rate with tolerance.

      This method calculates the false positive rate by dividing the number of false positives with tolerance
      by the number of condition negatives.

      :param self.false_positives_with_tolerance: The number of false positives with tolerance.
      :type self.false_positives_with_tolerance: int
      :param self.truth_mask: The binary ground truth mask.
      :type self.truth_mask: ndarray

      :returns: The false positive rate with tolerance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_positives_with_tolerance = 10
      >>> evaluator.truth_mask = np.array([0, 1, 0, 1, 0])
      >>> evaluator.false_positive_rate_with_tolerance()
      0.5

      .. note::

         The false positive rate with tolerance is a measure of the proportion of false positive predictions
         with respect to the total number of condition negatives. It is commonly used in binary segmentation tasks.



   .. py:method:: false_negatives_with_tolerance()

      Calculate the number of false negatives with tolerance.

      :param self.false_negative_distances: The distances of false negative pixels from the ground truth segmentation.
      :type self.false_negative_distances: ndarray
      :param self.tol_distance: The tolerance distance.
      :type self.tol_distance: float

      :returns: The number of false negatives with tolerance.
      :rtype: int

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_negative_distances = [1, 2, 3]
      >>> evaluator.tol_distance = 2
      >>> false_negatives = evaluator.false_negatives_with_tolerance()
      >>> print(false_negatives)
      1

      .. note::

         False negatives are cases where the model incorrectly predicts the absence of a positive class.
         The tolerance distance is used to determine whether a false negative is within an acceptable range.



   .. py:method:: false_negative_rate_with_tolerance()

      Calculate the false negative rate with tolerance.

      This method calculates the false negative rate by dividing the number of false negatives
      with tolerance by the number of condition positives.

      :param self.false_negatives_with_tolerance: The number of false negatives with tolerance.
      :type self.false_negatives_with_tolerance: int
      :param self.false_negative_distances: The distances of false negative pixels from the ground truth segmentation.
      :type self.false_negative_distances: ndarray

      :returns: The false negative rate with tolerance.
      :rtype: float

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_negative_distances = [1, 2, 3]
      >>> evaluator.false_negatives_with_tolerance = 2
      >>> evaluator.false_negative_rate_with_tolerance()
      0.6666666666666666

      .. note::

         The false negative rate with tolerance is a measure of the proportion of condition positives
         that are incorrectly classified as negatives, considering a certain tolerance level.



   .. py:method:: true_positives_with_tolerance()

      Calculate the number of true positives with tolerance.

      :param self.test_mask: The test binary segmentation mask.
      :type self.test_mask: ndarray
      :param self.truth_mask: The ground truth binary segmentation mask.
      :type self.truth_mask: ndarray
      :param self.false_negatives_with_tolerance: The number of false negatives with tolerance.
      :type self.false_negatives_with_tolerance: int
      :param self.false_positives_with_tolerance: The number of false positives with tolerance.
      :type self.false_positives_with_tolerance: int

      :returns: The number of true positives with tolerance.
      :rtype: int

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.test_mask = np.array([[0, 1], [1, 0]])
      >>> evaluator.truth_mask = np.array([[0, 1], [1, 0]])
      >>> evaluator.false_negatives_with_tolerance = 1
      >>> evaluator.false_positives_with_tolerance = 1
      >>> true_positives = evaluator.true_positives_with_tolerance()
      >>> print(true_positives)
      2

      .. note::

         True positives are cases where the model correctly predicts the presence of a positive class.
         The tolerance distance is used to determine whether a true positive is within an acceptable range.



   .. py:method:: precision_with_tolerance()

      Calculate the precision with tolerance.

      This method calculates the precision with tolerance by dividing the number of true positives
      with tolerance by the sum of true positives with tolerance and false positives with tolerance.

      :param self.true_positives_with_tolerance: The number of true positives with tolerance.
      :type self.true_positives_with_tolerance: int
      :param self.false_positives_with_tolerance: The number of false positives with tolerance.
      :type self.false_positives_with_tolerance: int

      :returns: The precision with tolerance.
      :rtype: float

      :raises ZeroDivisionError: If the sum of true positives with tolerance and false positives with tolerance is zero.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.true_positives_with_tolerance = 10
      >>> evaluator.false_positives_with_tolerance = 5
      >>> evaluator.precision_with_tolerance()
      0.6666666666666666

      .. note::

         The precision with tolerance is a measure of the proportion of true positives with tolerance
         out of the total number of predicted positives with tolerance.
         It indicates how well the binary segmentation evaluator performs in terms of correctly identifying positive samples.
         If the sum of true positives with tolerance and false positives with tolerance is zero, the precision with tolerance is undefined and a ZeroDivisionError is raised.



   .. py:method:: recall_with_tolerance()

      A measure of the ability of a binary classifier to identify all positive samples.

      :param self.true_positives_with_tolerance: The number of true positives with tolerance.
      :type self.true_positives_with_tolerance: int
      :param self.false_negatives_with_tolerance: The number of false negatives with tolerance.
      :type self.false_negatives_with_tolerance: int

      :returns: The recall with tolerance value.
      :rtype: float

      :raises ZeroDivisionError: If the sum of true positives with tolerance and false negatives with tolerance is zero.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.recall_with_tolerance()
      0.75

      .. note:: This method calculates the recall with tolerance, which is a measure of how well the binary segmentation evaluator performs. It returns the recall with tolerance value as a float. If either the truth or test data is empty, it returns NaN.



   .. py:method:: f1_score_with_tolerance()

      Calculate the F1 score with tolerance.

      :param self.recall_with_tolerance: The recall with tolerance value.
      :type self.recall_with_tolerance: float
      :param self.precision_with_tolerance: The precision with tolerance value.
      :type self.precision_with_tolerance: float

      :returns: The F1 score with tolerance.
      :rtype: float

      :raises ZeroDivisionError: If both the recall with tolerance and precision with tolerance are zero.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.recall_with_tolerance = 0.8
      >>> evaluator.precision_with_tolerance = 0.9
      >>> evaluator.f1_score_with_tolerance()
      0.8571428571428571

      .. note::

         The F1 score is a measure of a test's accuracy. It considers both the precision and recall of the test to compute the score.
         The F1 score with tolerance is calculated using the formula:
         F1 = 2 * (recall_with_tolerance * precision_with_tolerance) / (recall_with_tolerance + precision_with_tolerance)
         If both recall_with_tolerance and precision_with_tolerance are 0, the F1 score with tolerance will be NaN.



   .. py:method:: mean_false_positive_distances_clipped()

      Calculate the mean of the false positive distances, clipped to a maximum distance.

      :param self.false_positive_distances: The distances of false positive pixels from the ground truth segmentation.
      :type self.false_positive_distances: ndarray
      :param self.clip_distance: The maximum distance to clip.
      :type self.clip_distance: float

      :returns: The mean of the false positive distances, clipped to a maximum distance.
      :rtype: float

      :raises ValueError: If the clip distance is not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_positive_distances = [1, 2, 3, 4, 5]
      >>> evaluator.clip_distance = 3
      >>> evaluator.mean_false_positive_distances_clipped()
      2.5

      .. note:: This method calculates the mean of the false positive distances, where the distances are clipped to a maximum distance. The `false_positive_distances` attribute should be set before calling this method. The `clip_distance` attribute determines the maximum distance to which the distances are clipped.



   .. py:method:: mean_false_negative_distances_clipped()

      Calculate the mean of the false negative distances, clipped to a maximum distance.

      :param self.false_negative_distances: The distances of false negative pixels from the ground truth segmentation.
      :type self.false_negative_distances: ndarray
      :param self.clip_distance: The maximum distance to clip.
      :type self.clip_distance: float

      :returns: The mean of the false negative distances, clipped to a maximum distance.
      :rtype: float

      :raises ValueError: If the clip distance is not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_negative_distances = [1, 2, 3, 4, 5]
      >>> evaluator.clip_distance = 3
      >>> evaluator.mean_false_negative_distances_clipped()
      2.5

      .. note:: This method calculates the mean of the false negative distances, where the distances are clipped to a maximum distance. The `false_negative_distances` attribute should be set before calling this method. The `clip_distance` attribute determines the maximum distance to which the distances are clipped.



   .. py:method:: mean_false_positive_distance()

      Calculate the mean false positive distance.

      This method calculates the mean distance between the false positive points and the ground truth points.

      :param self.false_positive_distances: The distances of false positive pixels from the ground truth mask.
      :type self.false_positive_distances: ndarray

      :returns: The mean false positive distance.
      :rtype: float

      :raises ValueError: If the false positive distances are not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_positive_distances = [1.2, 3.4, 2.1]
      >>> evaluator.mean_false_positive_distance()
      2.2333333333333334

      .. note:: The false positive distances should be set before calling this method using the `false_positive_distances` attribute.



   .. py:method:: false_negative_distances()

      Calculate the distances of false negative pixels from the ground truth mask.

      :param self.truth_mask: The binary ground truth mask.
      :type self.truth_mask: ndarray

      :returns: An array containing the distances of false negative pixels.
      :rtype: numpy.ndarray

      :raises ValueError: If the truth mask is not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> distances = evaluator.false_negative_distances()
      >>> print(distances)
      [0.5, 1.0, 1.5, 2.0]

      .. note:: This method assumes that the ground truth mask and the test mask have already been set.



   .. py:method:: mean_false_negative_distance()

      Calculate the mean false negative distance.

      :param self.false_negative_distances: The distances of false negative pixels from the ground truth mask.
      :type self.false_negative_distances: ndarray

      :returns: The mean false negative distance.
      :rtype: float

      :raises ValueError: If the false negative distances are not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.false_negative_distances = [1.2, 3.4, 2.1]
      >>> evaluator.mean_false_negative_distance()
      2.2333333333333334

      .. note:: The mean false negative distance is calculated as the average of all false negative distances.



   .. py:method:: mean_false_distance()

      Calculate the mean false distance.

      This method calculates the mean false distance by taking the average of the mean false positive distance
      and the mean false negative distance.

      :param self.mean_false_positive_distance: The mean false positive distance.
      :type self.mean_false_positive_distance: float
      :param self.mean_false_negative_distance: The mean false negative distance.
      :type self.mean_false_negative_distance: float

      :returns: The calculated mean false distance.
      :rtype: float

      :raises ValueError: If the mean false positive distance or the mean false negative distance is not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_distance()
      5.0

      .. note::

         The mean false distance is a metric used to evaluate the performance of a binary segmentation model.
         It provides a measure of the average distance between false positive and false negative predictions.



   .. py:method:: mean_false_distance_clipped()

      Calculates the mean false distance clipped.

      This method calculates the mean false distance clipped by taking the average of the mean false positive distances
      clipped and the mean false negative distances clipped.

      :param self.mean_false_positive_distances_clipped: The mean false positive distances clipped.
      :type self.mean_false_positive_distances_clipped: float
      :param self.mean_false_negative_distances_clipped: The mean false negative distances clipped.
      :type self.mean_false_negative_distances_clipped: float

      :returns: The calculated mean false distance clipped.
      :rtype: float

      :raises ValueError: If the mean false positive distances clipped or the mean false negative distances clipped are not set.

      .. rubric:: Examples

      >>> evaluator = BinarySegmentationEvaluator()
      >>> evaluator.mean_false_distance_clipped()
      2.5

      .. note::

         The mean false distance clipped is calculated as 0.5 * (mean_false_positive_distances_clipped +
         mean_false_negative_distances_clipped).



