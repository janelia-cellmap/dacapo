dacapo.experiments.tasks.evaluators.instance_evaluator
======================================================

.. py:module:: dacapo.experiments.tasks.evaluators.instance_evaluator


Attributes
----------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.instance_evaluator.logger


Classes
-------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.instance_evaluator.ZarrArray
   dacapo.experiments.tasks.evaluators.instance_evaluator.Evaluator
   dacapo.experiments.tasks.evaluators.instance_evaluator.InstanceEvaluationScores
   dacapo.experiments.tasks.evaluators.instance_evaluator.InstanceEvaluator


Functions
---------

.. autoapisummary::

   dacapo.experiments.tasks.evaluators.instance_evaluator.relabel
   dacapo.experiments.tasks.evaluators.instance_evaluator.voi


Module Contents
---------------

.. py:class:: ZarrArray(array_config)



   This is a zarr array.

   .. attribute:: name

      The name of the array.

      :type: str

   .. attribute:: file_name

      The file name of the array.

      :type: Path

   .. attribute:: dataset

      The dataset name.

      :type: str

   .. attribute:: _axes

      The axes of the array.

      :type: Optional[List[str]]

   .. attribute:: snap_to_grid

      The snap to grid.

      :type: Optional[Coordinate]

   .. method:: __init__(array_config)

      
      Initializes the array type 'raw' and name for the DummyDataset instance.

   .. method:: __str__()

      
      Returns the string representation of the ZarrArray.

   .. method:: __repr__()

      
      Returns the string representation of the ZarrArray.

   .. method:: attrs()

      
      Returns the attributes of the array.

   .. method:: axes()

      
      Returns the axes of the array.

   .. method:: dims()

      
      Returns the dimensions of the array.

   .. method:: _daisy_array()

      
      Returns the daisy array.

   .. method:: voxel_size()

      
      Returns the voxel size of the array.

   .. method:: roi()

      
      Returns the region of interest of the array.

   .. method:: writable()

      
      Returns the boolean value of the array.

   .. method:: dtype()

      
      Returns the data type of the array.

   .. method:: num_channels()

      
      Returns the number of channels of the array.

   .. method:: spatial_axes()

      
      Returns the spatial axes of the array.

   .. method:: data()

      
      Returns the data of the array.

   .. method:: __getitem__(roi)

      
      Returns the data of the array for the given region of interest.

   .. method:: __setitem__(roi, value)

      
      Sets the data of the array for the given region of interest.

   .. method:: create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, write_size=None, name=None, overwrite=False)

      
      Creates a new ZarrArray given an array identifier.

   .. method:: open_from_array_identifier(array_identifier, name="")

      
      Opens a new ZarrArray given an array identifier.

   .. method:: _can_neuroglance()

      
      Returns the boolean value of the array.

   .. method:: _neuroglancer_source()

      
      Returns the neuroglancer source of the array.

   .. method:: _neuroglancer_layer()

      
      Returns the neuroglancer layer of the array.

   .. method:: _transform_matrix()

      
      Returns the transform matrix of the array.

   .. method:: _output_dimensions()

      
      Returns the output dimensions of the array.

   .. method:: _source_name()

      
      Returns the source name of the array.

   .. method:: add_metadata(metadata)

      
      Adds metadata to the array.

   .. rubric:: Notes

   This class is used to create a zarr array.


   .. py:property:: mode


   .. py:property:: attrs
      Returns the attributes of the array.

      :param attrs: The attributes of the array.
      :type attrs: Any

      :returns: The attributes of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> attrs()

      .. rubric:: Notes

      This method is used to return the attributes of the array.


   .. py:property:: axes
      Returns the axes of the array.

      :param axes: The axes of the array.
      :type axes: List[str]

      :returns: The axes of the array.
      :rtype: List[str]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> axes()

      .. rubric:: Notes

      This method is used to return the axes of the array.


   .. py:property:: dims
      :type: int

      Returns the dimensions of the array.

      :param dims: The dimensions of the array.
      :type dims: int

      :returns: The dimensions of the array.
      :rtype: int

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> dims()

      .. rubric:: Notes

      This method is used to return the dimensions of the array.


   .. py:method:: voxel_size() -> funlib.geometry.Coordinate

      Returns the voxel size of the array.

      :param voxel_size: The voxel size.
      :type voxel_size: Coordinate

      :returns: The voxel size of the array.
      :rtype: Coordinate

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> voxel_size()

      .. rubric:: Notes

      This method is used to return the voxel size of the array.



   .. py:method:: roi() -> funlib.geometry.Roi

      Returns the region of interest of the array.

      :param roi: The region of interest.
      :type roi: Roi

      :returns: The region of interest of the array.
      :rtype: Roi

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> roi()

      .. rubric:: Notes

      This method is used to return the region of interest of the array.



   .. py:property:: writable
      :type: bool

      Returns the boolean value of the array.

      :param writable: The boolean value of the array.
      :type writable: bool

      :returns: The boolean value of the array.
      :rtype: bool

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> writable()

      .. rubric:: Notes

      This method is used to return the boolean value of the array.


   .. py:property:: dtype
      :type: Any

      Returns the data type of the array.

      :param dtype: The data type of the array.
      :type dtype: Any

      :returns: The data type of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> dtype()

      .. rubric:: Notes

      This method is used to return the data type of the array.


   .. py:property:: num_channels
      :type: Optional[int]

      Returns the number of channels of the array.

      :param num_channels: The number of channels of the array.
      :type num_channels: Optional[int]

      :returns: The number of channels of the array.
      :rtype: Optional[int]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> num_channels()

      .. rubric:: Notes

      This method is used to return the number of channels of the array.


   .. py:property:: spatial_axes
      :type: List[str]

      Returns the spatial axes of the array.

      :param spatial_axes: The spatial axes of the array.
      :type spatial_axes: List[str]

      :returns: The spatial axes of the array.
      :rtype: List[str]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> spatial_axes()

      .. rubric:: Notes

      This method is used to return the spatial axes of the array.


   .. py:property:: data
      :type: Any

      Returns the data of the array.

      :param data: The data of the array.
      :type data: Any

      :returns: The data of the array.
      :rtype: Any

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> data()

      .. rubric:: Notes

      This method is used to return the data of the array.


   .. py:method:: create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, mode='a', write_size=None, name=None, overwrite=False)
      :classmethod:


      Create a new ZarrArray given an array identifier. It is assumed that
      this array_identifier points to a dataset that does not yet exist.

      :param array_identifier: The array identifier.
      :type array_identifier: ArrayIdentifier
      :param axes: The axes of the array.
      :type axes: List[str]
      :param roi: The region of interest.
      :type roi: Roi
      :param num_channels: The number of channels.
      :type num_channels: int
      :param voxel_size: The voxel size.
      :type voxel_size: Coordinate
      :param dtype: The data type.
      :type dtype: Any
      :param write_size: The write size.
      :type write_size: Optional[Coordinate]
      :param name: The name of the array.
      :type name: Optional[str]
      :param overwrite: The boolean value to overwrite the array.
      :type overwrite: bool

      :returns: The ZarrArray.
      :rtype: ZarrArray

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> create_from_array_identifier(array_identifier, axes, roi, num_channels, voxel_size, dtype, write_size=None, name=None, overwrite=False)

      .. rubric:: Notes

      This method is used to create a new ZarrArray given an array identifier.



   .. py:method:: open_from_array_identifier(array_identifier, name='')
      :classmethod:


      Opens a new ZarrArray given an array identifier.

      :param array_identifier: The array identifier.
      :type array_identifier: ArrayIdentifier
      :param name: The name of the array.
      :type name: str

      :returns: The ZarrArray.
      :rtype: ZarrArray

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> open_from_array_identifier(array_identifier, name="")

      .. rubric:: Notes

      This method is used to open a new ZarrArray given an array identifier.



   .. py:method:: add_metadata(metadata: Dict[str, Any]) -> None

      Adds metadata to the array.

      :param metadata: The metadata to add to the array.
      :type metadata: Dict[str, Any]

      :raises NotImplementedError:

      .. rubric:: Examples

      >>> add_metadata(metadata)

      .. rubric:: Notes

      This method is used to add metadata to the array.



.. py:class:: Evaluator



   Base class of all evaluators: An abstract class representing an evaluator that compares and evaluates the output array against the evaluation array.

   An evaluator takes a post-processor's output and compares it against
   ground-truth. It then returns a set of scores that can be used to
   determine the quality of the post-processor's output.

   .. attribute:: best_scores

      Dict[OutputIdentifier, BestScore]
      the best scores for each dataset/post-processing parameter/criterion combination

   .. method:: evaluate(output_array_identifier, evaluation_array)

      
      Compare and evaluate the output array against the evaluation array.

   .. method:: is_best(dataset, parameter, criterion, score)

      
      Check if the provided score is the best for this dataset/parameter/criterion combo.

   .. method:: get_overall_best(dataset, criterion)

      
      Return the best score for the given dataset and criterion.

   .. method:: get_overall_best_parameters(dataset, criterion)

      
      Return the best parameters for the given dataset and criterion.

   .. method:: compare(score_1, score_2, criterion)

      
      Compare two scores for the given criterion.

   .. method:: set_best(validation_scores)

      
      Find the best iteration for each dataset/post_processing_parameter/criterion.

   .. method:: higher_is_better(criterion)

      
      Return whether higher is better for the given criterion.

   .. method:: bounds(criterion)

      
      Return the bounds for the given criterion.

   .. method:: store_best(criterion)

      
      Return whether to store the best score for the given criterion.

   .. note:: The Evaluator class is used to compare and evaluate the output array against the evaluation array.


   .. py:method:: evaluate(output_array_identifier: dacapo.store.local_array_store.LocalArrayIdentifier, evaluation_array: dacapo.experiments.datasplits.datasets.arrays.Array) -> dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores
      :abstractmethod:


      Compares and evaluates the output array against the evaluation array.

      :param output_array_identifier: LocalArrayIdentifier
                                      The identifier of the output array.
      :param evaluation_array: Array
                               The evaluation array.

      :returns:

                EvaluationScores
                    The evaluation scores.

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> output_array_identifier = LocalArrayIdentifier("output_array")
      >>> evaluation_array = Array()
      >>> evaluator.evaluate(output_array_identifier, evaluation_array)
      EvaluationScores()

      .. note:: This function is used to compare and evaluate the output array against the evaluation array.



   .. py:property:: best_scores
      :type: Dict[OutputIdentifier, BestScore]

      The best scores for each dataset/post-processing parameter/criterion combination.

      :returns:

                Dict[OutputIdentifier, BestScore]
                    the best scores for each dataset/post-processing parameter/criterion combination

      :raises AttributeError: if the best scores are not set

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.best_scores
      {}

      .. note:: This function is used to return the best scores for each dataset/post-processing parameter/criterion combination.


   .. py:method:: is_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, parameter: dacapo.experiments.tasks.post_processors.PostProcessorParameters, criterion: str, score: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores) -> bool

      Check if the provided score is the best for this dataset/parameter/criterion combo.

      :param dataset: Dataset
                      the dataset
      :param parameter: PostProcessorParameters
                        the post-processor parameters
      :param criterion: str
                        the criterion
      :param score: EvaluationScores
                    the evaluation scores

      :returns:

                bool
                    whether the provided score is the best for this dataset/parameter/criterion combo

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> parameter = PostProcessorParameters()
      >>> criterion = "criterion"
      >>> score = EvaluationScores()
      >>> evaluator.is_best(dataset, parameter, criterion, score)
      False

      .. note:: This function is used to check if the provided score is the best for this dataset/parameter/criterion combo.



   .. py:method:: get_overall_best(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)

      Return the best score for the given dataset and criterion.

      :param dataset: Dataset
                      the dataset
      :param criterion: str
                        the criterion

      :returns:

                Optional[float]
                    the best score for the given dataset and criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> criterion = "criterion"
      >>> evaluator.get_overall_best(dataset, criterion)
      None

      .. note:: This function is used to return the best score for the given dataset and criterion.



   .. py:method:: get_overall_best_parameters(dataset: dacapo.experiments.datasplits.datasets.Dataset, criterion: str)

      Return the best parameters for the given dataset and criterion.

      :param dataset: Dataset
                      the dataset
      :param criterion: str
                        the criterion

      :returns:

                Optional[PostProcessorParameters]
                    the best parameters for the given dataset and criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> dataset = Dataset()
      >>> criterion = "criterion"
      >>> evaluator.get_overall_best_parameters(dataset, criterion)
      None

      .. note:: This function is used to return the best parameters for the given dataset and criterion.



   .. py:method:: compare(score_1, score_2, criterion)

      Compare two scores for the given criterion.

      :param score_1: float
                      the first score
      :param score_2: float
                      the second score
      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether the first score is better than the second score for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> score_1 = 0.0
      >>> score_2 = 0.0
      >>> criterion = "criterion"
      >>> evaluator.compare(score_1, score_2, criterion)
      False

      .. note:: This function is used to compare two scores for the given criterion.



   .. py:method:: set_best(validation_scores: dacapo.experiments.validation_scores.ValidationScores) -> None

      Find the best iteration for each dataset/post_processing_parameter/criterion.

      :param validation_scores: ValidationScores
                                the validation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> validation_scores = ValidationScores()
      >>> evaluator.set_best(validation_scores)
      None

      .. note::

         This function is used to find the best iteration for each dataset/post_processing_parameter/criterion.
         Typically, this function is called after the validation scores have been computed.



   .. py:property:: criteria
      :type: List[str]

      :abstractmethod:

      A list of all criteria for which a model might be "best". i.e. your
      criteria might be "precision", "recall", and "jaccard". It is unlikely
      that the best iteration/post processing parameters will be the same
      for all 3 of these criteria

      :returns:

                List[str]
                    the evaluation criteria

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.criteria
      []

      .. note:: This function is used to return the evaluation criteria.


   .. py:method:: higher_is_better(criterion: str) -> bool

      Wether or not higher is better for this criterion.

      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether higher is better for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.higher_is_better(criterion)
      False

      .. note:: This function is used to determine whether higher is better for the given criterion.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]

      The bounds for this criterion

      :param criterion: str
                        the criterion

      :returns:

                Tuple[Union[int, float, None], Union[int, float, None]]
                    the bounds for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.bounds(criterion)
      (0, 1)

      .. note:: This function is used to return the bounds for the given criterion.



   .. py:method:: store_best(criterion: str) -> bool

      The bounds for this criterion

      :param criterion: str
                        the criterion

      :returns:

                bool
                    whether to store the best score for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> criterion = "criterion"
      >>> evaluator.store_best(criterion)
      False

      .. note:: This function is used to return whether to store the best score for the given criterion.



   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.evaluation_scores.EvaluationScores

      :abstractmethod:

      The evaluation scores.

      :returns:

                EvaluationScores
                    the evaluation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> evaluator = Evaluator()
      >>> evaluator.score
      EvaluationScores()

      .. note:: This function is used to return the evaluation scores.


.. py:class:: InstanceEvaluationScores



   The evaluation scores for the instance segmentation task. The scores include the variation of information (VOI) split, VOI merge, and VOI.

   .. attribute:: voi_split

      float
      the variation of information (VOI) split

   .. attribute:: voi_merge

      float
      the variation of information (VOI) merge

   .. attribute:: voi

      float
      the variation of information (VOI)

   .. method:: higher_is_better(criterion)

      
      Return whether higher is better for the given criterion.

   .. method:: bounds(criterion)

      
      Return the bounds for the given criterion.

   .. method:: store_best(criterion)

      
      Return whether to store the best score for the given criterion.

   .. note:: The InstanceEvaluationScores class is used to store the evaluation scores for the instance segmentation task.


   .. py:attribute:: criteria
      :value: ['voi_split', 'voi_merge', 'voi']



   .. py:attribute:: voi_split
      :type:  float


   .. py:attribute:: voi_merge
      :type:  float


   .. py:property:: voi
      Return the average of the VOI split and VOI merge.

      :returns:

                float
                    the average of the VOI split and VOI merge

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> instance_evaluation_scores = InstanceEvaluationScores(voi_split=0.1, voi_merge=0.2)
      >>> instance_evaluation_scores.voi
      0.15

      .. note:: This function is used to calculate the average of the VOI split and VOI merge.


   .. py:method:: higher_is_better(criterion: str) -> bool
      :staticmethod:


      Return whether higher is better for the given criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                bool
                    whether higher is better for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> InstanceEvaluationScores.higher_is_better("voi_split")
      False

      .. note:: This function is used to determine whether higher is better for the given criterion.



   .. py:method:: bounds(criterion: str) -> Tuple[Union[int, float, None], Union[int, float, None]]
      :staticmethod:


      Return the bounds for the given criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                Tuple[Union[int, float, None], Union[int, float, None]]
                    the bounds for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> InstanceEvaluationScores.bounds("voi_split")
      (0, 1)

      .. note:: This function is used to return the bounds for the given criterion.



   .. py:method:: store_best(criterion: str) -> bool
      :staticmethod:


      Return whether to store the best score for the given criterion.

      :param criterion: str
                        the evaluation criterion

      :returns:

                bool
                    whether to store the best score for the given criterion

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> InstanceEvaluationScores.store_best("voi_split")
      True

      .. note:: This function is used to determine whether to store the best score for the given criterion.



.. py:data:: logger

.. py:function:: relabel(array, return_backwards_map=False, inplace=False)

   Relabel array, such that IDs are consecutive. Excludes 0.

   :param array: The array to relabel.
   :type array: ndarray
   :param return_backwards_map: If ``True``, return an ndarray that maps new labels (indices in
                                the array) to old labels.
   :type return_backwards_map: ``bool``, optional
   :param inplace: Perform the replacement in-place on ``array``.
   :type inplace: ``bool``, optional

   :returns: A tuple ``(relabelled, n)``, where ``relabelled`` is the relabelled
             array and ``n`` the number of unique labels found.
             If ``return_backwards_map`` is ``True``, returns ``(relabelled, n,
             backwards_map)``.

   :raises ValueError: If ``array`` is not of type ``np.ndarray``.

   .. rubric:: Examples

   >>> array = np.array([[1, 2, 0], [0, 2, 1]])
   >>> relabel(array)
   (array([[1, 2, 0], [0, 2, 1]]), 2)
   >>> relabel(array, return_backwards_map=True)
   (array([[1, 2, 0], [0, 2, 1]]), 2, [0, 1, 2])

   .. note:: This function is used to relabel an array, such that IDs are consecutive. Excludes 0.


.. py:class:: InstanceEvaluator



   A class representing an evaluator for instance segmentation tasks.

   .. attribute:: criteria

      List[str]
      the evaluation criteria

   .. method:: evaluate(output_array_identifier, evaluation_array)

      
      Evaluate the output array against the evaluation array.

   .. method:: score

      
      Return the evaluation scores.

   .. note:: The InstanceEvaluator class is used to evaluate the performance of an instance segmentation task.


   .. py:attribute:: criteria
      :type:  List[str]
      :value: ['voi_merge', 'voi_split', 'voi']



   .. py:method:: evaluate(output_array_identifier, evaluation_array)

      Evaluate the output array against the evaluation array.

      :param output_array_identifier: str
                                      the identifier of the output array
      :param evaluation_array: ZarrArray
                               the evaluation array

      :returns:

                InstanceEvaluationScores
                    the evaluation scores

      :raises ValueError: if the output array identifier is not valid

      .. rubric:: Examples

      >>> instance_evaluator = InstanceEvaluator()
      >>> output_array_identifier = "output_array"
      >>> evaluation_array = ZarrArray.open_from_array_identifier("evaluation_array")
      >>> instance_evaluator.evaluate(output_array_identifier, evaluation_array)
      InstanceEvaluationScores(voi_merge=0.0, voi_split=0.0)

      .. note:: This function is used to evaluate the output array against the evaluation array.



   .. py:property:: score
      :type: dacapo.experiments.tasks.evaluators.instance_evaluation_scores.InstanceEvaluationScores

      Return the evaluation scores.

      :returns:

                InstanceEvaluationScores
                    the evaluation scores

      :raises NotImplementedError: if the function is not implemented

      .. rubric:: Examples

      >>> instance_evaluator = InstanceEvaluator()
      >>> instance_evaluator.score
      InstanceEvaluationScores(voi_merge=0.0, voi_split=0.0)

      .. note:: This function is used to return the evaluation scores.


.. py:function:: voi(truth, test)

   Calculate the variation of information (VOI) between two segmentations.

   :param truth: ndarray
                 the ground truth segmentation
   :param test: ndarray
                the test segmentation

   :returns:

             dict
                 the variation of information (VOI) scores

   :raises ValueError: if the truth and test arrays are not of type np.ndarray

   .. rubric:: Examples

   >>> truth = np.array([[1, 1, 0], [0, 2, 2]])
   >>> test = np.array([[1, 1, 0], [0, 2, 2]])
   >>> voi(truth, test)
   {'voi_split': 0.0, 'voi_merge': 0.0}

   .. note:: This function is used to calculate the variation of information (VOI) between two segmentations.


